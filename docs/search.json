[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is a website for the exercises on the course Risk Assessment in Environment and Public Health MVEN10 Fall 2023."
  },
  {
    "objectID": "index.html#august-30",
    "href": "index.html#august-30",
    "title": "Home",
    "section": "August 30",
    "text": "August 30\nEx 1. Expressions of uncertainty - How probable is probable?\nEx 2. Chance, belief and frequency\nEx 3. Introduction to Quarto and R"
  },
  {
    "objectID": "index.html#sept-4",
    "href": "index.html#sept-4",
    "title": "Home",
    "section": "Sept 4",
    "text": "Sept 4\nEx 4. Risk classification\nEx 5. Expert judgement\nTest of Wisdom of Crowds\nEx 6. Introduction to useful functions in Microsoft Excel"
  },
  {
    "objectID": "index.html#sept-6",
    "href": "index.html#sept-6",
    "title": "Home",
    "section": "Sept 6",
    "text": "Sept 6\nUseful functions reproduced using R\nEx 7. Observe and summarise\nMake sure you finalise all exercises we have had up to this point."
  },
  {
    "objectID": "index.html#sept-11",
    "href": "index.html#sept-11",
    "title": "Home",
    "section": "Sept 11",
    "text": "Sept 11\nEx 8. Observe, fit and simulate\nEx 9. Environmental exposure assessment"
  },
  {
    "objectID": "index.html#sept-13",
    "href": "index.html#sept-13",
    "title": "Home",
    "section": "Sept 13",
    "text": "Sept 13\nEx 10. Exposure assessment from databases\nEx 11. Hazard assessment using Benchmark Dose Modelling\nEx 12. Interval arithmetic and Monte Carlo simulation on the daily intake equation"
  },
  {
    "objectID": "index.html#sept-18",
    "href": "index.html#sept-18",
    "title": "Home",
    "section": "Sept 18",
    "text": "Sept 18\nEx 13. Daily intake equation - Solutions for the Monte Carlo simulation\nEx 14. Population Viability Analysis - Baltic Cod"
  },
  {
    "objectID": "index.html#sept-19",
    "href": "index.html#sept-19",
    "title": "Home",
    "section": "Sept 19",
    "text": "Sept 19\nEx 15. Hazard assessment using Species Sensitivity Distributions\nEx 16. Species Distribution Modelling - climate matching\nEx 17. Assessment of overall uncertainty - video"
  },
  {
    "objectID": "index.html#oct-10",
    "href": "index.html#oct-10",
    "title": "Home",
    "section": "Oct 10",
    "text": "Oct 10\nCancelled"
  },
  {
    "objectID": "index.html#oct-17",
    "href": "index.html#oct-17",
    "title": "Home",
    "section": "Oct 17",
    "text": "Oct 17\nEx 18. Environmental health risk assessment (on canvas)"
  },
  {
    "objectID": "index.html#oct-18",
    "href": "index.html#oct-18",
    "title": "Home",
    "section": "Oct 18",
    "text": "Oct 18\nEx 19. Food safety assessment with uncertainty analysis\nEx 20. Exposure assessment using a model for bioaccumulation"
  },
  {
    "objectID": "ex/useful_functions_with_R.html",
    "href": "ex/useful_functions_with_R.html",
    "title": "The R-version of what we did in Introduction to useful functions in Excel",
    "section": "",
    "text": "Load packages\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nLoad data and save the variable to an object called x\n\ndf = as_tibble(read_csv(\"../data/breast-cancer.csv\"))%&gt;% select(radius_mean)"
  },
  {
    "objectID": "ex/useful_functions_with_R.html#preparations",
    "href": "ex/useful_functions_with_R.html#preparations",
    "title": "The R-version of what we did in Introduction to useful functions in Excel",
    "section": "",
    "text": "Load packages\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nLoad data and save the variable to an object called x\n\ndf = as_tibble(read_csv(\"../data/breast-cancer.csv\"))%&gt;% select(radius_mean)"
  },
  {
    "objectID": "ex/useful_functions_with_R.html#descriptive-statistics",
    "href": "ex/useful_functions_with_R.html#descriptive-statistics",
    "title": "The R-version of what we did in Introduction to useful functions in Excel",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\nsummary(df)\n\n  radius_mean    \n Min.   : 6.981  \n 1st Qu.:11.700  \n Median :13.370  \n Mean   :14.127  \n 3rd Qu.:15.780  \n Max.   :28.110  \n\n\n\nx = df$radius_mean\nquantile(x,probs=0.25)\n\n 25% \n11.7 \n\nmedian(x)\n\n[1] 13.37\n\nquantile(x,probs=0.95)\n\n   95% \n20.576 \n\nmean(x)\n\n[1] 14.12729\n\nsd(x)\n\n[1] 3.524049\n\nmin(x)\n\n[1] 6.981\n\nmax(x)\n\n[1] 28.11\n\nlength(x)\n\n[1] 569\n\n\nCalculate the three summary statistics described in the green area of the sheet.\n\nThe third quartile in the sample, P75\n\n\nquantile(x,probs=0.75)\n\n  75% \n15.78 \n\n\n\nThe 5% quantile (or 5th percentile), P05\n\n\nquantile(x,probs=0.05)\n\n    5% \n9.5292 \n\n\n\nThe coefficient of variation is the ratio between the sample standard deviation and the sample mean\n\n\nsd(x)/mean(x)*100\n\n[1] 24.94497"
  },
  {
    "objectID": "ex/useful_functions_with_R.html#histogram",
    "href": "ex/useful_functions_with_R.html#histogram",
    "title": "The R-version of what we did in Introduction to useful functions in Excel",
    "section": "Histogram",
    "text": "Histogram\n\nhist(x)\n\n\n\n\n\ndf %&gt;% \n  ggplot(aes(x=x))+\n  geom_histogram()\n\n\n\n\n\ndf %&gt;% \n  ggplot(aes(x=x))+\n  geom_histogram(binwidth = 2.5)\n\n\n\n\n\ndf %&gt;% \n  ggplot(aes(x=x))+\n  geom_density()"
  },
  {
    "objectID": "ex/useful_functions_with_R.html#probability-functions",
    "href": "ex/useful_functions_with_R.html#probability-functions",
    "title": "The R-version of what we did in Introduction to useful functions in Excel",
    "section": "Probability functions",
    "text": "Probability functions\nThe probability functions follow the principles of combining p, d, q and r with the name (or short name) of the probability distributions.\n\nFunctions for the normal distribution\n\n\nWhat to calculate\nR-function\n\n\n\n\nCDF\npnorm\n\n\nPDF\ndnorm\n\n\nquantile\nqnorm\n\n\nrandom draw\nrnorm\n\n\n\nCalculate the probability that a normally distributed variable with mean 14 and standard deviation 3.5 is less than 10\n\npnorm(10,mean=14,sd=3.5)\n\n[1] 0.126549\n\n\nFind the 95% quantile in the same distribution\n\nqnorm(0.95,mean=14,sd=3.5)\n\n[1] 19.75699\n\n\nCalculate the probability that an exponentially distributed variable with mean 14 is less than 10\n\npexp(10,rate=1/14)\n\n[1] 0.5104583\n\n\n\n\n\n\n\n\nTip\n\n\n\nType a question mark before the function to see the help text ?pexp"
  },
  {
    "objectID": "ex/useful_functions_with_R.html#plot-probability-distributions",
    "href": "ex/useful_functions_with_R.html#plot-probability-distributions",
    "title": "The R-version of what we did in Introduction to useful functions in Excel",
    "section": "Plot probability distributions",
    "text": "Plot probability distributions\n\nm=14\ns=3.5\ndata.frame(pp=ppoints(100)) %&gt;%\n  mutate(x=qnorm(pp,m,s)) %&gt;%\n  mutate(d=dnorm(x,m,s)) %&gt;%\n  ggplot(aes(x=x,y=d))+\n  geom_line()+\n  xlab('value')+\n  ylab('density')\n\n\n\n\n\n\n\n\n\n\nExtra\n\n\n\nIf you feel you have the time or do another time:\nCopy the sheet and refine the grid by using pp-values from 0.001 to 0.999.\n\n\n\nm=14\ns=3.5\ndata.frame(pp=ppoints(1000)) %&gt;%\n  mutate(x=qnorm(pp,m,s)) %&gt;%\n  mutate(d=dnorm(x,m,s)) %&gt;%\n  ggplot(aes(x=x,y=d))+\n  geom_line()+\n  xlab('value')+\n  ylab('density')"
  },
  {
    "objectID": "ex/useful_functions_with_R.html#random-sampling",
    "href": "ex/useful_functions_with_R.html#random-sampling",
    "title": "The R-version of what we did in Introduction to useful functions in Excel",
    "section": "Random sampling",
    "text": "Random sampling\nAll sample generators start with a random number between 0 and 1. This is also a sample from a uniform distribution.\n\nrunif(1)\n\n[1] 0.9704786\n\n\nType a function that generates a uniform random number in the interval 1 to 6.\n\nrunif(1,min=1,max=6)\n\n[1] 5.595887\n\n\nA random draw from a probability distribution can be generated by the inverse method. - Draws pp-values from a uniform distribution between 0 and 1 - Transform them into quantiles of the target distribution\nGenerates random draws from a normal distribution using the inverse method\n\nqnorm(runif(1),m,s)\n\n[1] 13.14799\n\n\nThis is already implemented as a function\n\nrnorm(1,m,s)\n\n[1] 18.37606\n\n\nDraw from a beta distribution with parameters \\(\\alpha=2\\) and \\(\\beta=8\\)\n\nrbeta(1,2,8)\n\n[1] 0.2157914"
  },
  {
    "objectID": "ex/useful_functions_with_R.html#compare-descriptive-statistics-against-theoretical-values",
    "href": "ex/useful_functions_with_R.html#compare-descriptive-statistics-against-theoretical-values",
    "title": "The R-version of what we did in Introduction to useful functions in Excel",
    "section": "Compare descriptive statistics against theoretical values",
    "text": "Compare descriptive statistics against theoretical values\nWow - now we can generate data where we know the true value on parameters and all theoretical probabilities and quantiles, and compare with what we get when deriving descriptive statistics from the random sample.\nThis sheet generates a random sample of size 20 from a beta distribution.\n\nrbeta(n=20,2,8)\n\n [1] 0.21436125 0.24865058 0.07527012 0.05475960 0.17566076 0.05934510\n [7] 0.25065048 0.05348027 0.33044344 0.23641477 0.08971576 0.17476356\n[13] 0.19743103 0.15513189 0.45292185 0.03770521 0.27509703 0.04703781\n[19] 0.19567211 0.18022758\n\n\nA beta distribution has two parameters \\(\\alpha\\) and \\(\\beta\\)\nThe expected value of a beta distributed variable is \\(\\frac{\\alpha}{\\alpha+\\beta}\\)\nCompare the calculated sample average to the theoretical expected value\n\nalpha=2\nbeta=8\nalpha/(alpha+beta)\n\n[1] 0.2\n\nmean(rbeta(n=20,alpha,beta))\n\n[1] 0.1860472\n\n\nWe can also derive the theoretical quantile, let us say the P95.\nCompare the quantile from the sample with the quantile calculated from the inverse probability distribution function\n\nqbeta(0.95,alpha,beta)\n\n[1] 0.4291355\n\nquantile(rbeta(n=20,alpha,beta),probs=0.95)\n\n      95% \n0.4128793 \n\n\n\nWhich of them has the smallest difference? Why do you think it is like that?\n\n\n\n\n\n\n\nExtra\n\n\n\nIf you feel you have the time or do another time:\nExplore what happens with the difference between theoretical and statistical values when you increase sample size from 20 to a high number (close to 1000)?\n\n\nBelow I wrte a script where sample size is controlled at one place. The P95 is approximated fairly well by the sampling when I use \\(n=10 000\\).\n\nalpha=2\nbeta=8\nn=10000\nalpha/(alpha+beta)\n\n[1] 0.2\n\nmean(rbeta(n=n,alpha,beta))\n\n[1] 0.200594\n\nqbeta(0.95,alpha,beta)\n\n[1] 0.4291355\n\nquantile(rbeta(n=n,alpha,beta),probs=0.95)\n\n      95% \n0.4327183 \n\n\nLet us visualise the convergence of the approximation of the mean and 95th percentile of the beta distribution using Monte Carlo simulation.\nThe code below defines a function which calculates the mean and P95 after every increase of the sample size and plots the convergence.\n\nplot_conv &lt;- function(n){\nsample_mean=cummean(rbeta(n=n,alpha,beta))\nsample_P95=unlist(lapply(1:n,function(iter){\n  quantile(rbeta(n=iter,alpha,beta),probs=0.95)}))\ndata.frame(values=c(sample_mean,sample_P95),n=rep(1:n,2), statistic=rep(c(\"mean\",\"P95\"),each=n))  %&gt;%\n  ggplot(aes(x=n,y=values,color=statistic))+\n  geom_line()+\n  geom_hline(yintercept = alpha/(alpha+beta)) +\n  geom_hline(yintercept = qbeta(0.95,alpha,beta))\n}\n\nWe start with \\(n=10\\)\n\nplot_conv(n=10)\n\n\n\n\n..increase to \\(n=100\\)\n\nplot_conv(n=100)\n\n\n\n\n..increase to \\(n=1000\\)\n\nplot_conv(n=10^3)\n\n\n\n\n..and finally \\(n=10000\\)\n\nplot_conv(n=10^4)"
  },
  {
    "objectID": "ex/the_bus_stop.html",
    "href": "ex/the_bus_stop.html",
    "title": "Expert judgement",
    "section": "",
    "text": "Work in pairs or alone.\n\n\n\n\n\n\n\nTo\n\n\n\n\n\n\n\n\n\n\nminutes\n\n\n\nBe prepared to report back at the end of the exercise. Write a report using the template and upload it on the assignment in canvas.\n\n\n\nhttps://shelf.sites.sheffield.ac.uk/software#h.n8yy6jrhk7"
  },
  {
    "objectID": "ex/the_bus_stop.html#exercise-overview",
    "href": "ex/the_bus_stop.html#exercise-overview",
    "title": "Expert judgement",
    "section": "",
    "text": "Work in pairs or alone.\n\n\n\n\n\n\n\nTo\n\n\n\n\n\n\n\n\n\n\nminutes\n\n\n\nBe prepared to report back at the end of the exercise. Write a report using the template and upload it on the assignment in canvas.\n\n\n\nhttps://shelf.sites.sheffield.ac.uk/software#h.n8yy6jrhk7"
  },
  {
    "objectID": "ex/the_bus_stop.html#content-1",
    "href": "ex/the_bus_stop.html#content-1",
    "title": "Expert judgement",
    "section": "Content",
    "text": "Content\nMake judgements\nDemonstrate the “Wisdom of crowds” by considering the aggregated judgement"
  },
  {
    "objectID": "ex/observe_summarise.html",
    "href": "ex/observe_summarise.html",
    "title": "Observe and Summarise",
    "section": "",
    "text": "Work in groups of 2-4\n\n\n\nRisk assessments are made using available data (+ evidence and expert knowledge).\nObservations of the real world are useful to inform (or with a fancy word - calibrate) assessment models. It is important to consider how data have been collected and it is related to the system being studied.\n\n\n\n\nTo get a feeling for collecting and managing data for further analysis\nTo practice summarising data by its descriptive statistics\n\n\n\n\n\nmake observations and save them in a spreadsheet\nload data to R\nsummarise by statistics and suitable graph\nmake a report\n\n\n\n\n70 minutes\n\n\n\nWrite a report using a qmd document and upload it on the discussion forum in canvas.\nThe report should contain the name of the authors, a boxplot, a histogram, the size, mean, median, min, max, and the quantiles: P5, P25, P75 and P95, of the data sample.\nThere is no need to make the report look fancy, the important thing is that you produce a report."
  },
  {
    "objectID": "ex/observe_summarise.html#exercise-overview",
    "href": "ex/observe_summarise.html#exercise-overview",
    "title": "Observe and Summarise",
    "section": "",
    "text": "Work in groups of 2-4\n\n\n\nRisk assessments are made using available data (+ evidence and expert knowledge).\nObservations of the real world are useful to inform (or with a fancy word - calibrate) assessment models. It is important to consider how data have been collected and it is related to the system being studied.\n\n\n\n\nTo get a feeling for collecting and managing data for further analysis\nTo practice summarising data by its descriptive statistics\n\n\n\n\n\nmake observations and save them in a spreadsheet\nload data to R\nsummarise by statistics and suitable graph\nmake a report\n\n\n\n\n70 minutes\n\n\n\nWrite a report using a qmd document and upload it on the discussion forum in canvas.\nThe report should contain the name of the authors, a boxplot, a histogram, the size, mean, median, min, max, and the quantiles: P5, P25, P75 and P95, of the data sample.\nThere is no need to make the report look fancy, the important thing is that you produce a report."
  },
  {
    "objectID": "ex/observe_summarise.html#collect-observations",
    "href": "ex/observe_summarise.html#collect-observations",
    "title": "Observe and Summarise",
    "section": "Collect observations",
    "text": "Collect observations\nCollect 20 observations from one of these alternatives:\n\nMeasure the length of wooden bricks in the box placed in the lecture room.\nWeight fallen apples. There are lots around the Ecology building.\nCount the number of cars passing in one direction during 30 seconds intervals. Do it on Tunavägen by the bridge over the tram.\nSame as above, but measure instead the time between cars passing.\nSomething you suggest yourself, as long as it is discrete or continuous data. Check if OK with Ullrika first. Send her an email.\n\nOpen a new spreadsheet and call it “ex7_yournames.xlsx”, where “yournames” is altered to your names.\nCall the first sheet metadata and the second sheet one data.\nFeed in the observations in the data sheet. Give them a name on the top of the column. I call mine “obs”.\nIn the metadata sheet, add information on how and when data was collected and by whom. If the data is discrete or continuous. If applicable, what unit used. Any other relevant information.\nPost the spreadsheet on the discussion forum for exercises."
  },
  {
    "objectID": "ex/observe_summarise.html#descriptive-statistics",
    "href": "ex/observe_summarise.html#descriptive-statistics",
    "title": "Observe and Summarise",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\nLoad your data into R.\nThe code below loads a package for reading Excel files. If you don’t have this, you will be asked to install it.\nThe function read_excel directs to the Excel file and which sheet to read from.\n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.3.1\n\ndf = read_excel(path=\"../files/ex7_yournames.xlsx\",sheet=\"data\")\ndf\n\n# A tibble: 19 × 1\n     obs\n   &lt;dbl&gt;\n 1   4  \n 2   2.2\n 3   8.1\n 4   9.2\n 5   9.9\n 6  10  \n 7  14  \n 8   1  \n 9   3  \n10   3.3\n11   4.1\n12   6  \n13   3.6\n14   5.7\n15   8.9\n16   2.2\n17   3.6\n18   8  \n19   5.1\n\n\n\nsummary(df)\n\n      obs        \n Min.   : 1.000  \n 1st Qu.: 3.450  \n Median : 5.100  \n Mean   : 5.889  \n 3rd Qu.: 8.500  \n Max.   :14.000  \n\n\nSample size\n\nnrow(df)\n\n[1] 19"
  },
  {
    "objectID": "ex/observe_summarise.html#visualise-your-data",
    "href": "ex/observe_summarise.html#visualise-your-data",
    "title": "Observe and Summarise",
    "section": "Visualise your data",
    "text": "Visualise your data\n\nlibrary(ggplot2)\n\n\nBoxplot\n\nggplot(df,aes(y=obs))+\n  geom_boxplot()\n\n\n\n\n\n\nHistogram\n\nggplot(df,aes(x=obs))+\n  geom_histogram(binwidth=2)\n\n\n\n\n\n\nDensity plot\n\nggplot(df,aes(x=obs))+\n  geom_density()"
  },
  {
    "objectID": "ex/observe_summarise.html#report",
    "href": "ex/observe_summarise.html#report",
    "title": "Observe and Summarise",
    "section": "Report",
    "text": "Report\nUpload the report into the same discussion on canvas where you also provided the data."
  },
  {
    "objectID": "ex/introduction_to_quarto_and_R.html",
    "href": "ex/introduction_to_quarto_and_R.html",
    "title": "Introduction to Quarto and R",
    "section": "",
    "text": "Work in pairs or alone.\n\n\n\nCalculations, simulations, data analysis and statistical analysis are common elements in risk assessments.\nBeing able to produce or read code supporting an assessment is a valuable skill when working as an expert or assessor.\nOpen source systems for coding and reporting are useful for collaborative work, reproducibility and external evaluation.\n\n\n\nTo learn how to create a presentation in html format using Quarto and how to combine text, figures, equations and results from analysis into a report.\n\n\n\n\nCreate a Quarto presentation in html format from R Studio cloud.\nUse basic commands in R run from R Studio cloud.\nCreate a quarto report integrating text and results from running commands i R from R Studio cloud.\n\n\n\n\n45 minutes\n\n\n\nNo reporting required\n\n\n\nhttps://quarto.org/"
  },
  {
    "objectID": "ex/introduction_to_quarto_and_R.html#exercise-overview",
    "href": "ex/introduction_to_quarto_and_R.html#exercise-overview",
    "title": "Introduction to Quarto and R",
    "section": "",
    "text": "Work in pairs or alone.\n\n\n\nCalculations, simulations, data analysis and statistical analysis are common elements in risk assessments.\nBeing able to produce or read code supporting an assessment is a valuable skill when working as an expert or assessor.\nOpen source systems for coding and reporting are useful for collaborative work, reproducibility and external evaluation.\n\n\n\nTo learn how to create a presentation in html format using Quarto and how to combine text, figures, equations and results from analysis into a report.\n\n\n\n\nCreate a Quarto presentation in html format from R Studio cloud.\nUse basic commands in R run from R Studio cloud.\nCreate a quarto report integrating text and results from running commands i R from R Studio cloud.\n\n\n\n\n45 minutes\n\n\n\nNo reporting required\n\n\n\nhttps://quarto.org/"
  },
  {
    "objectID": "ex/introduction_to_quarto_and_R.html#quarto-presentation",
    "href": "ex/introduction_to_quarto_and_R.html#quarto-presentation",
    "title": "Introduction to Quarto and R",
    "section": "Quarto presentation",
    "text": "Quarto presentation\n\nGo to posit.cloud and register an account.\nOpen a new project and call it “intro”.\nClick on the new file button (up to the left) and open a new Quarto Presentation.\nAssign a title, e.g. My report\nAssign yourself as author\nClick on Create\n\nIf you get a yellow ribbon asking you to install rmarkdown - Click install and wait. When finished, your window should look like this:\n\n\nSave the file as “testpresentation.qmd”\nPress Render\n\nThe program is running the qmd-file creating a html-file that is automatically opened in your browser.\nUse the page down button or left/right arrow to change slide.\n\nGo back to the page with Your workspace intro\n\nThe code in testpresentation.qmd is currently shown as Visual.\n\nChange to Source.\n\nYou can enlarge the window by reducing the console window.\n\nRemove all text from line 8 and downwards\nAdd the following text\n\n## Slide 1\n\nA probability is always between 0 and 1\n\n## Slide 2\n\nProbability can be interpreted as a\n\n- theoretical probability\n\n- frequency\n\n- subjective probability\n\n## Slide 3\n\nToday we have learnt about \n\n### Uncertainty\n\nIt was *fun*\n\n### Probability\n\nIt was even more **fun**\n\n## Slide 4\n\nNow I practise writing a math expression \n\n$\\frac{m}{n}$\n\n$\\alpha$\n\nI use two dollar signs to center it\n\n$$X\\cdot Y$$\n\nPress Render and look at the html-document for the presentation.\n\nIf you did not close it before, it should be in the browser.\nNow you know how to create a presentation with headings, subheadings and bulletpoints."
  },
  {
    "objectID": "ex/introduction_to_quarto_and_R.html#basic-commands-in-r",
    "href": "ex/introduction_to_quarto_and_R.html#basic-commands-in-r",
    "title": "Introduction to Quarto and R",
    "section": "Basic commands in R",
    "text": "Basic commands in R\n\nSimple calculations\n\nClick on new file and open an R Script\n\nR can work as a calculator\n\nOn line 1, type 1 + 2 and press Run\n\nYou should see the result 3 in the Console\n\n1 + 2\n\n[1] 3\n\n\nYou can save the result from a calculation as an object\n\nChange the code on line 1 to be y = 1 + 2 and press Run\n\nTo see the value of y, you have to type it as well in the code an rerun or in the Console\n\ny = 1 + 2\ny\n\n[1] 3\n\n\n\n\nSimple plotting\nNow let us look at how to create a plot\n\nCreate a data frame consisting of two variables X and Y with 10 values each, where Y is positively associated with X.\n\n\n\n\n\n\n\nNote\n\n\n\nIt is useful to denote variables with capital letters and use lower case letters for observations of this variable.\nFor example, x is an sample from X.\n\n\n\nPlot the sample from Y against the sample from X\n\n\nx = runif(10)\ny = 2*x + rnorm(10)\nplot(x,y)\n\n\n\n\nNice visualisations of data makes a huge difference to a report. We will therefore demonstrate a way to generate the same plot using ggplot2.\n\nInstall ggplot2 by typing install.packages(“tidyverse”) in the R Console. You might have to rewrite the quotation marks.\n\nThis installs several packages including ggplot2 that you will use later on. R-packages are libraries with functions and data sets designed for a specific purpose. Note that you will only have to do this one time in your work space.\n\nLoad the library\n\n\nlibrary(\"ggplot2\")\n\n\nCreate a data frame with the observations and redo the plotting using ggplot2\n\n\ndf = data.frame(x=x,y=y)\n\nggplot(df,aes(x=x,y=y))+\n  geom_point()\n\n\n\n\nThere will be time to explore ggplot later on in the course, but let us add a line fitted to the data.\n\nggplot(df,aes(x=x,y=y))+\n  geom_point()+\n  geom_smooth(method=lm,formula = y ~ x)\n\n\n\n\nNow you can make a plot using R! More things will be introduced during exercises.\nThere are lot of resources for self studies on R. We recommend you to have a look at the W3schools’ tutorial on R Statistics after the exercise."
  },
  {
    "objectID": "ex/introduction_to_quarto_and_R.html#quarto-report",
    "href": "ex/introduction_to_quarto_and_R.html#quarto-report",
    "title": "Introduction to Quarto and R",
    "section": "Quarto report",
    "text": "Quarto report\nNow you will create a report using Quarto in which you combine text and outputs from code running in R.\n\nCreate a new Quarto Document with the title “My Report”, you as the author and save it as “testreport.qmd”\nIn the configuration section (also known as the YAML), add the text “date: today” as shown in the figure below.\n\n\n\nPress Render and view the generated html-file that appears in the browser.\nGo back and look at the testreport.qmd file. R-code is added as gray chunks. One can use to show or hide the code.\n\nLet us now add our own information into the report.\n\nRemove everything from line 9 and downwards.\nAdd the following text\n\n## Summary\n\n### Lessons learnt\n\nProbability can be interpreted as a\n\n- theoretical probability\n\n- frequency\n\n- subjective probability\n\n### Skills gained\n\n(@) Generate presentations and reports using Quarto\n\n(@) Create and plot data from R, such as this\n\nPress Render to see what the report looks like\n\nLet us now add the figure by adding the following code\n\nlibrary(\"ggplot2\")\nx = runif(10)\ny = 2*x + rnorm(10)\ndf = data.frame(x=x,y=y)\nggplot(df,aes(x=x,y=y))+\n  geom_point()+\n  geom_smooth(method=lm,formula = y ~ x)\n\n\n\n\n\nPress Render and look at the report\n\nLet us now hide the code by adding #| echo: false in the beginning of the r-chunk\n\n\nPress Render\n\nLet us improve the accessibility of the report by adding a table of content.\n\nChange the YAML as shown below. The toc is the table of contents.\n\n\n\nRender\n\nIsn’t this great!\nLet us now end by adding an image to the report\n\nDownload this image and upload it to your project in R Studio cloud\n\n\n\n\n Download risk meme as example image\n\n\n\n\nAdd the following text at the end of your testreport.qmd file\n\n## Risk\n\n*No matter what, it is difficult to save the world without acknowledging that risk involves our values and our uncertainties about the world.*\n\n![](twobuttons.jpg){width=40%}\n\nRender and view your report.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can integrate code and create figures in the same way in a Quarto presentation. The only difference is that there is a limitation on every slide.\nBegin a new slide by ## [header of the slide]\n\n\n\nTo simplify sharing code and organise files during the course we here recommend to use folders\ndata for storing data\nex for storing .rmd files\nimg for storing images"
  },
  {
    "objectID": "ex/ex_temp.html",
    "href": "ex/ex_temp.html",
    "title": "Ex temp",
    "section": "",
    "text": "To\n\n\n\n\n\n\n\n\n\n\nminutes\n\n\n\nBe prepared to report back at the end of the exercise. Write a report using the template and upload it on the assignment in canvas."
  },
  {
    "objectID": "ex/ex_temp.html#exercise-overview",
    "href": "ex/ex_temp.html#exercise-overview",
    "title": "Ex temp",
    "section": "",
    "text": "To\n\n\n\n\n\n\n\n\n\n\nminutes\n\n\n\nBe prepared to report back at the end of the exercise. Write a report using the template and upload it on the assignment in canvas."
  },
  {
    "objectID": "ex/ex_temp.html#content-1",
    "href": "ex/ex_temp.html#content-1",
    "title": "Ex temp",
    "section": "Content",
    "text": "Content"
  },
  {
    "objectID": "ex/ex_species_distribution_modelling.html",
    "href": "ex/ex_species_distribution_modelling.html",
    "title": "Exercise 16. Species Distribution Modelling Climate matching",
    "section": "",
    "text": "The use of species distribution modeling to predict the possible extent of suitable habitat for significant pests has been accepted as an efficient method for determining effective management and countermeasures. CLIMEX and MaxEnt are widely used software for creating species distribution models. CLIMEX predicts climatic suitability of a specific region for target species, whereas MaxEnt uses various environmental variables with presence-only data to assess potential distribution.\nBioclimatic modelling use species occurrence data to model distribution or perform climate matching.\nThe Global Biodiversity Information Facility https://www.gbif.org/ is an database for species occurrence data.\n\n\n\n\n\n\n\nTo perform climate matching to assess establisment potential for a potentially invasive species\nTo extract species occurrence data from a global database\n\n\n\n\nThe group that are to do CLIMEX will get a licence for CLIMEX for a personal computer - it costs 10 US dollars https://www.hearne.software/Software/CLIMEX-DYMEX/Editions\nHere in the exercise, we will use an online implementation of CLIMEX for New Zealand https://climate.b3nz.org.nz/\n\n\n\n45 minutes\n\n\n\nBe prepared to report back at the end of the exercise.\n\n\n\nSteven J. Phillips, Miroslav Dudík, Robert E. Schapire. [Internet] Maxent software for modeling species niches and distributions (Version 3.4.1). Available from url: http://biodiversityinformatics.amnh.org/open_source/maxent/. Accessed on 2023-9-19."
  },
  {
    "objectID": "ex/ex_species_distribution_modelling.html#exercise-overview",
    "href": "ex/ex_species_distribution_modelling.html#exercise-overview",
    "title": "Exercise 16. Species Distribution Modelling Climate matching",
    "section": "",
    "text": "The use of species distribution modeling to predict the possible extent of suitable habitat for significant pests has been accepted as an efficient method for determining effective management and countermeasures. CLIMEX and MaxEnt are widely used software for creating species distribution models. CLIMEX predicts climatic suitability of a specific region for target species, whereas MaxEnt uses various environmental variables with presence-only data to assess potential distribution.\nBioclimatic modelling use species occurrence data to model distribution or perform climate matching.\nThe Global Biodiversity Information Facility https://www.gbif.org/ is an database for species occurrence data.\n\n\n\n\n\n\n\nTo perform climate matching to assess establisment potential for a potentially invasive species\nTo extract species occurrence data from a global database\n\n\n\n\nThe group that are to do CLIMEX will get a licence for CLIMEX for a personal computer - it costs 10 US dollars https://www.hearne.software/Software/CLIMEX-DYMEX/Editions\nHere in the exercise, we will use an online implementation of CLIMEX for New Zealand https://climate.b3nz.org.nz/\n\n\n\n45 minutes\n\n\n\nBe prepared to report back at the end of the exercise.\n\n\n\nSteven J. Phillips, Miroslav Dudík, Robert E. Schapire. [Internet] Maxent software for modeling species niches and distributions (Version 3.4.1). Available from url: http://biodiversityinformatics.amnh.org/open_source/maxent/. Accessed on 2023-9-19."
  },
  {
    "objectID": "ex/ex_species_distribution_modelling.html#the-climate-matching-tool",
    "href": "ex/ex_species_distribution_modelling.html#the-climate-matching-tool",
    "title": "Exercise 16. Species Distribution Modelling Climate matching",
    "section": "The Climate Matching Tool",
    "text": "The Climate Matching Tool\n\nNZ - World ximilarities\n\nGo to the Climate Matching Tool and Access the tool\n\nNZ - World similarities shows the climate similarities between New Zealand and the world\nCMI stands for Climate Matching Index and is a value between 0 and 1, where 1 means perfect match and 0 no match.\n\nA CMI greater than 0.7 corresponds to a high climate match.\n\n\nScroll over the map and derive the match between New Zealand and Lund (southern Sweden)."
  },
  {
    "objectID": "ex/ex_species_distribution_modelling.html#upload-species-occurrences",
    "href": "ex/ex_species_distribution_modelling.html#upload-species-occurrences",
    "title": "Exercise 16. Species Distribution Modelling Climate matching",
    "section": "Upload species occurrences",
    "text": "Upload species occurrences\n\nDownload the example file and view it, e.g. in Excel or as text file\n\nThe file has three columns: the first is an identifier from the data base, the second column is latitude and the third column is longitude.\nThis is species occurrence data for Spotted lanternfly lycorma delicatula which is an invasive species in some parts of the words. The latitude and longitude point out reported places where the species occurs today.\n\nUpload the occurrences for lycorma delicatula to the online tool\n\nThe points on the map show occurrences. The species is found in the US and Eastern Asia. From this you cannot see its native range. On wiki it is states that the Spotted lanternfly is a planthopper indigenous to parts of China and Vietnam. It has spread invasively to Japan, South Korea, and the United States.\n\nGo to the tab CMI Cells.\n\nThe graph represents the distribution of the CMI in cells overlapping with unique occurrences.\nIf multiple occurrences coincided with a CMI cell, they were counted only once.\nThis graph can be used when the user wants to eliminate the effects of spatial correlation/sampling bias on occurrences.\nThe proportion of the occurrences with a CMI greater or equal to 0.7.\nThe online tool shows that the proportion of the 51 cells with CMIs &gt;= 0.7 is 86.3%. One way to interpret this is that there is a high overlap between the current climate of a species and the climate on New Zealand. If introduced to New Zealand, there is a high risk that the species will be able to establish taking into consideration the climate requirements."
  },
  {
    "objectID": "ex/ex_species_distribution_modelling.html#perform-climate-matching-for-another-species",
    "href": "ex/ex_species_distribution_modelling.html#perform-climate-matching-for-another-species",
    "title": "Exercise 16. Species Distribution Modelling Climate matching",
    "section": "Perform climate matching for another species",
    "text": "Perform climate matching for another species\n\nGo to GBIF and prepare to download species occurrence data for the brown bear ursus arctos\n\nYou have to create an account to download. If you don’t want to do that, you can get the file here\n\n\n\n Download csv file\n\n\n\nCitation for the data file: GBIF.org (19 September 2023) GBIF Occurrence Download https://doi.org/10.15468/dl.ccxnby\nThis file must be cleaned before entering the tool. You can download a cleaned version of the file containing the ID, latitude and longtitude.\n\n\n\n Download csv file\n\n\n\n\nUpload the occurrence data to the online tool and perform climate matching"
  },
  {
    "objectID": "ex/ex_species_distribution_modelling.html#solutions",
    "href": "ex/ex_species_distribution_modelling.html#solutions",
    "title": "Exercise 16. Species Distribution Modelling Climate matching",
    "section": "Solutions",
    "text": "Solutions\nThe match between NZ and Lund is 0.84 under the climate from 1985.\nThe map showing occurrences for the brown bear looks like this\n\nThe proportion of the 104 cells with CMIs &gt;= 0.7 is 92.3%. This means that an introduced brown bear would find the climate on New Zealand highly suitable and would be able to establish. Note that this doesn’t take into account biological facts, such as natural enemies, the possibility to reproduce, spread, habitat needs and specific dietary requirements."
  },
  {
    "objectID": "ex/ex_population_viability_analysis.html",
    "href": "ex/ex_population_viability_analysis.html",
    "title": "Ex 14 Population Viability Analysis of the Baltic Cod",
    "section": "",
    "text": "Jonzén at al. (2002) performed a risk analysis on the risk of collapse of Baltic Codfish, based on data from trawl surveys and commercial catch from 1980 to 1999. In order to predict the Codfish population within a range of 5, 10 and 30 years from 1999, they used a model that describe the change in biomass from one year to another as a function of initial biomass \\(B_t\\), intrinsic growth rate \\(\\rho\\), density dependence \\(\\delta\\), and commercial catch \\(C_t\\).\nFurthermore, hazardous events occurs in all ecological systems – giving us so called “process errors”. These were accounted for by adding some “noise” to the biomass growth function so that\n\\[B_t = \\left(B_{t-1} \\cdot e^{(\\pi-\\delta\\cdot B_{t-1})}\\, - \\, C_{t-1}\\right) \\cdot e^{N(0,\\sigma)}\\]\nwhere the term \\(N(0, \\sigma)\\) is a set of normally distributed values around a mean of 0 with standard deviation of \\(\\sigma\\). This term represents any form of unexpected event that could either increase or decrease the growth rate of biomass a given year, i.e. it includes some variability to the growth function.\nJonzén at al. (2002) used the estimated parameter values to calculate the probability of quasi-extinction (population density below a critical value) for a range of different fishing regimes and for a range of different estimates of fish biomass in 1999.\nIn this exercise, we are going to use a similar function for biomass growth as Jonzén et al. (2002), and we are going to evaluate the risk of codfish extinction in 15 years from now for three different commercial catches and for three different estimates of initial biomass.\n\n\n\nThe purpose of this exercise is to learn how to build a risk analysis problem evaluating different management options and to understand what a Population Viability Analysis can look like.\n\n\n\n\ninstructions in this file\n\n\n\n\n60 minutes\n\n\n\nBe prepared to report back at the end of the exercise.\n\n\n\nJonzen, N.,Cardinale, M., Gårdmark, A., Arrhenius, F. and Lundberg, P. 2002. Risk of collapse in Baltic Cod fishery. Mar. Ecol. Prog. Ser. Vol. 240: 225-233"
  },
  {
    "objectID": "ex/ex_population_viability_analysis.html#exercise-overview",
    "href": "ex/ex_population_viability_analysis.html#exercise-overview",
    "title": "Ex 14 Population Viability Analysis of the Baltic Cod",
    "section": "",
    "text": "Jonzén at al. (2002) performed a risk analysis on the risk of collapse of Baltic Codfish, based on data from trawl surveys and commercial catch from 1980 to 1999. In order to predict the Codfish population within a range of 5, 10 and 30 years from 1999, they used a model that describe the change in biomass from one year to another as a function of initial biomass \\(B_t\\), intrinsic growth rate \\(\\rho\\), density dependence \\(\\delta\\), and commercial catch \\(C_t\\).\nFurthermore, hazardous events occurs in all ecological systems – giving us so called “process errors”. These were accounted for by adding some “noise” to the biomass growth function so that\n\\[B_t = \\left(B_{t-1} \\cdot e^{(\\pi-\\delta\\cdot B_{t-1})}\\, - \\, C_{t-1}\\right) \\cdot e^{N(0,\\sigma)}\\]\nwhere the term \\(N(0, \\sigma)\\) is a set of normally distributed values around a mean of 0 with standard deviation of \\(\\sigma\\). This term represents any form of unexpected event that could either increase or decrease the growth rate of biomass a given year, i.e. it includes some variability to the growth function.\nJonzén at al. (2002) used the estimated parameter values to calculate the probability of quasi-extinction (population density below a critical value) for a range of different fishing regimes and for a range of different estimates of fish biomass in 1999.\nIn this exercise, we are going to use a similar function for biomass growth as Jonzén et al. (2002), and we are going to evaluate the risk of codfish extinction in 15 years from now for three different commercial catches and for three different estimates of initial biomass.\n\n\n\nThe purpose of this exercise is to learn how to build a risk analysis problem evaluating different management options and to understand what a Population Viability Analysis can look like.\n\n\n\n\ninstructions in this file\n\n\n\n\n60 minutes\n\n\n\nBe prepared to report back at the end of the exercise.\n\n\n\nJonzen, N.,Cardinale, M., Gårdmark, A., Arrhenius, F. and Lundberg, P. 2002. Risk of collapse in Baltic Cod fishery. Mar. Ecol. Prog. Ser. Vol. 240: 225-233"
  },
  {
    "objectID": "ex/ex_population_viability_analysis.html#preparations",
    "href": "ex/ex_population_viability_analysis.html#preparations",
    "title": "Ex 14 Population Viability Analysis of the Baltic Cod",
    "section": "Preparations",
    "text": "Preparations\nTo run the code below you need the following R packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nWrite a function that takes the starting population size \\(B_0\\) and harvest \\(C_t\\) as input arguments and returns the population biomass over time. Call it simB.\n\nWe recommend you modify the code below. See also the info box about how to create a function in R.\n\npi = 0.097\ndelta = 3.5*10^-5\nsigma = 2.5\n\nB0 = 250\nCt = 25\n\nyears = 15\nQ = 10 # quasi extinction threshold \n\nC = rep(Ct,years)\nB = rep(B0,years)\nfor(t in 2:years){\n  B[t] = (B[t-1]*exp(pi-delta*B[t-1]) - C[t-1])+rnorm(1,0,sigma)\n}\nB\n\n [1] 250.0000 249.1810 247.6534 245.6139 241.4520 240.3952 235.7450 232.3464\n [9] 229.9042 226.3738 219.4962 211.8858 209.3284 203.6611 197.2544\n\n\n\n\n\n\n\n\nNote\n\n\n\nfunction is a function in R to create a function in R.\nit is called by the following\n“name of your function” = function(){}\ninside the ()-brackets you put the arguments of the function\ninside the {}-brackets you put what the function is suppose to do\nthe function returns what you do last inside the {}-brackets\ne.g. simB = function(B0,Ct){“the calcluations” ending with B}\nSolutions to functions can be found at the end of these instructions.\n\n\n\nRun the model 10 times with initial biomass \\(B_0 = 250\\) and harvest \\(C_t=25\\) and plot the biomass over time for each iteration.\n\nBelow is one way to do it. The function lapply applies a function and saves the output in a list. The function do.call sorts the information in the list into a data frame so we can plot it using ggplot.\n\nniter = 10\ndo.call('rbind',lapply(1:niter,function(i){data.frame(B=simB(B0 = 250, Ct = 25),t=1:15,iter=i)})) %&gt;% \n  ggplot(aes(x=t,y=B,group=iter)) + \n  geom_line() +\n  ylim(0,300) +\n  geom_hline(yintercept=10,col='red') +\n  ylab(\"biomass\") +\n  xlab(\"year\") # + ggtitle()\n\n\n\n\n\nCreate a new function to calculate if the population has gone quasi extinct before year 15. Call it simQE.\n\nHint. You can determine if a population has gone quasi extinct by counting the number of years where the biomass is below the quasi extinction threshold and check if this sum is greater than zero. See the code below\n\nsum(B&lt;Q)&gt;0 # TRUE if quasi extinct\n\n[1] FALSE\n\n\nRun the model 10 times with initial biomass \\(B_0 = 250\\) and harvest \\(C_t=25\\) and calculate the proportion of times the population goes quasi extinct.\n\nniter = 10\nreplicate(niter,simQE(B0 = 250, Ct = 25))\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE"
  },
  {
    "objectID": "ex/ex_population_viability_analysis.html#risk-assessment-question",
    "href": "ex/ex_population_viability_analysis.html#risk-assessment-question",
    "title": "Ex 14 Population Viability Analysis of the Baltic Cod",
    "section": "Risk assessment question",
    "text": "Risk assessment question\n\nWhat is the probability of quasi-extinction for codfish 15 years from now?\nEvaluate this under three different fishing regimes (i.e. the size of the annual commercial catch), and under three different initial population sizes. Assume that the codfish is “quasi-extinct” when total biomass is equal to or below 10 ton.\n\n\nFishing scenarios in the Population Viability Analysis.\n\n\n\n\n\n\n\n\nInitial biomass \\(B_0\\)\nCommercial catch \\(C_t = 25\\)\n\\(C_t = 35\\)\n\\(C_t = 45\\)\n\n\n\n\n200\n\n\n\n\n\n250\n\n\n\n\n\n300"
  },
  {
    "objectID": "ex/ex_population_viability_analysis.html#monte-carlo-simulation-with-variability",
    "href": "ex/ex_population_viability_analysis.html#monte-carlo-simulation-with-variability",
    "title": "Ex 14 Population Viability Analysis of the Baltic Cod",
    "section": "Monte Carlo simulation with variability",
    "text": "Monte Carlo simulation with variability\n\nRun the model 10 000 times for each scenario and derive the proportion of times the population goes quasi extinct\n\n\nniter = 10^4\nmean(replicate(niter,simQE(B0 = 200, Ct = 25)))\n\n[1] 0.0571\n\nmean(replicate(niter,simQE(B0 = 200, Ct = 35)))\n\n[1] 1\n\nmean(replicate(niter,simQE(B0 = 200, Ct = 45)))\n\n[1] 1\n\nmean(replicate(niter,simQE(B0 = 250, Ct = 25)))\n\n[1] 0\n\nmean(replicate(niter,simQE(B0 = 250, Ct = 35)))\n\n[1] 0.9999\n\nmean(replicate(niter,simQE(B0 = 250, Ct = 45)))\n\n[1] 1\n\nmean(replicate(niter,simQE(B0 = 300, Ct = 25)))\n\n[1] 0\n\nmean(replicate(niter,simQE(B0 = 300, Ct = 35)))\n\n[1] 0\n\nmean(replicate(niter,simQE(B0 = 300, Ct = 45)))\n\n[1] 1"
  },
  {
    "objectID": "ex/ex_population_viability_analysis.html#monte-carlo-simulation-for-variability-and-parameter-uncertainty",
    "href": "ex/ex_population_viability_analysis.html#monte-carlo-simulation-for-variability-and-parameter-uncertainty",
    "title": "Ex 14 Population Viability Analysis of the Baltic Cod",
    "section": "Monte Carlo simulation for variability and parameter uncertainty",
    "text": "Monte Carlo simulation for variability and parameter uncertainty\nThe parameters \\(\\pi\\) and \\(\\delta\\) are difficult to estimate with high precision, and assessors describe their uncertainty as \\(\\pi \\sim N(0.097,0.003)\\) and \\(\\delta \\sim U(-2,5) \\cdot 10^{-5}\\).\n\nModify the function for biomass and quasi extinction to consider parameter uncertainty and call it simBu\n\n\nPlot the simulations of biomass when considering parameter uncertainty. Is there a difference compared to when parameters were fixed?\n\n\nniter = 10\ndo.call('rbind',lapply(1:niter,function(i){data.frame(B=simBu(B0 = 250, Ct = 25),t=1:15,iter=i)})) %&gt;% \n  ggplot(aes(x=t,y=B,group=iter)) + \n  geom_line() +\n  ylim(0,300) +\n  geom_hline(yintercept=10,col='red') +\n  ylab(\"biomass\") +\n  xlab(\"year\") # + ggtitle()\n\nWarning: Removed 11 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\nModify the function for quasi extinction to consider parameter uncertainty and call it simQEu\n\n\nsimQEu &lt;- function(B0,Ct){\npi = rnorm(1,0.097,0.003)\ndelta = runif(1,min=-2*10^-5,max=5*10^-5)\nsigma = 2.5\n\nyears = 15\nQ = 10 # quasi extinction threshold \n\nC = rep(Ct,years)\nB = rep(B0,years)\nfor(t in 2:years){\n  B[t] = (B[t-1]*exp(pi-delta*B[t-1]) - C[t-1])+rnorm(1,0,sigma)\n}\n\nsum(B&lt;Q)&gt;0 # TRUE if quasi extinct\n}\n\n\nRedo the assessment of extinction risk with parameter uncertainty.\n\n\nniter = 10^4\nmean(replicate(niter,simQEu(B0 = 200, Ct = 25)))\n\n[1] 0.051\n\nmean(replicate(niter,simQEu(B0 = 200, Ct = 35)))\n\n[1] 1\n\nmean(replicate(niter,simQEu(B0 = 200, Ct = 45)))\n\n[1] 1\n\nmean(replicate(niter,simQEu(B0 = 250, Ct = 25)))\n\n[1] 0\n\nmean(replicate(niter,simQEu(B0 = 250, Ct = 35)))\n\n[1] 0.9089\n\nmean(replicate(niter,simQEu(B0 = 250, Ct = 45)))\n\n[1] 1\n\nmean(replicate(niter,simQEu(B0 = 300, Ct = 25)))\n\n[1] 0\n\nmean(replicate(niter,simQEu(B0 = 300, Ct = 35)))\n\n[1] 2e-04\n\nmean(replicate(niter,simQEu(B0 = 300, Ct = 45)))\n\n[1] 0.9997\n\n\n\nDid consideration of parameter uncertainty change the quasi-extinction probabilities? If so, in which direction?"
  },
  {
    "objectID": "ex/ex_population_viability_analysis.html#a-note-on-the-separation-of-uncertainty-and-variability",
    "href": "ex/ex_population_viability_analysis.html#a-note-on-the-separation-of-uncertainty-and-variability",
    "title": "Ex 14 Population Viability Analysis of the Baltic Cod",
    "section": "A note on the separation of uncertainty and variability",
    "text": "A note on the separation of uncertainty and variability\nIn the above example, uncertainty and variability are mixed.\nA 2-dimensional Monte Carlo-simulation with one loop for variability and one for uncertainty allows for a separation when propagating uncertainty through the model."
  },
  {
    "objectID": "ex/ex_population_viability_analysis.html#solutions",
    "href": "ex/ex_population_viability_analysis.html#solutions",
    "title": "Ex 14 Population Viability Analysis of the Baltic Cod",
    "section": "Solutions",
    "text": "Solutions\n\nsimB &lt;- function(B0,Ct){\npi = 0.097\ndelta = 3.5*10^-5\nsigma = 2.5\n\nyears = 15\nQ = 10 # quasi extinction threshold \n\nC = rep(Ct,years)\nB = rep(B0,years)\nfor(t in 2:years){\n  B[t] = (B[t-1]*exp(pi-delta*B[t-1]) - C[t-1])+rnorm(1,0,sigma)\n}\nB\n}\n\n\nsimQE &lt;- function(B0,Ct){\npi = 0.097\ndelta = 3.5*10^-5\nsigma = 2.5\n\nyears = 15\nQ = 10 # quasi extinction threshold \n\nC = rep(Ct,years)\nB = rep(B0,years)\nfor(t in 2:years){\n  B[t] = (B[t-1]*exp(pi-delta*B[t-1]) - C[t-1])+rnorm(1,0,sigma)\n}\n\n#B\nsum(B&lt;Q)&gt;0 # TRUE if quasi extinct\n}\n\n\nsimBu &lt;- function(B0,Ct){\npi = rnorm(1,0.097,0.003)\ndelta = runif(1,min=-2*10^-5,max=5*10^-5)\nsigma = 2.5\n\nyears = 15\nQ = 10 # quasi extinction threshold \n\nC = rep(Ct,years)\nB = rep(B0,years)\nfor(t in 2:years){\n  B[t] = (B[t-1]*exp(pi-delta*B[t-1]) - C[t-1])+rnorm(1,0,sigma)\n}\n\nB\n}"
  },
  {
    "objectID": "ex/ex_exposure_assessment_databases.html",
    "href": "ex/ex_exposure_assessment_databases.html",
    "title": "Exercise 10. Exposure assessment from databases",
    "section": "",
    "text": "Do in groups of 1-3\n\n\nExposure assessment uses data on food consumption and drinking water, analytical data on the presence of substances in food and statistics of behaviours and lifestyles of populations of interest.\nData from studies and surveys have been collected into data bases that are available to be used in risk assessment.\n\n\n\n\nTo explore some data bases to support exposure assessment\nTo discuss how to derive exposure estimates\n\n\n\n\n\nThe European exposure data base\n\n\n\n\n45 minutes\n\n\n\nBe prepared to report back at the end of the exercise.\n\n\n\nIn text"
  },
  {
    "objectID": "ex/ex_exposure_assessment_databases.html#exercise-overview",
    "href": "ex/ex_exposure_assessment_databases.html#exercise-overview",
    "title": "Exercise 10. Exposure assessment from databases",
    "section": "",
    "text": "Do in groups of 1-3\n\n\nExposure assessment uses data on food consumption and drinking water, analytical data on the presence of substances in food and statistics of behaviours and lifestyles of populations of interest.\nData from studies and surveys have been collected into data bases that are available to be used in risk assessment.\n\n\n\n\nTo explore some data bases to support exposure assessment\nTo discuss how to derive exposure estimates\n\n\n\n\n\nThe European exposure data base\n\n\n\n\n45 minutes\n\n\n\nBe prepared to report back at the end of the exercise.\n\n\n\nIn text"
  },
  {
    "objectID": "ex/ex_exposure_assessment_databases.html#exposure-facts",
    "href": "ex/ex_exposure_assessment_databases.html#exposure-facts",
    "title": "Exercise 10. Exposure assessment from databases",
    "section": "Exposure facts",
    "text": "Exposure facts\n\nGo to the website at JRC and read about the Exposure facts\nWhat does JRC stands for?\nWhat type of information are there in the ExpoFacts Database?"
  },
  {
    "objectID": "ex/ex_exposure_assessment_databases.html#the-exposure-factors-handbook",
    "href": "ex/ex_exposure_assessment_databases.html#the-exposure-factors-handbook",
    "title": "Exercise 10. Exposure assessment from databases",
    "section": "The exposure factors handbook",
    "text": "The exposure factors handbook\nThe US EPA Exposure factors handbook contains information for exposure assessment.\n\nGo to chapter 5. Soil and dust ingestion and open the update from 2017\nWhat type of information is found in this chapter and what can it be used for?\nDiscuss the difference between these three terms\n\nSoil ingestion is the consumption of soil. This may result from various behaviors including, but not limited to, mouthing, contacting dirty hands, eating dropped food, or consuming soil directly.\nSoil pica is the recurrent ingestion of unusually high amounts of soil (i.e., on the order of 1,000−5,000 mg/day or more).\nGeophagy is the intentional ingestion of earths and is usually associated with cultural practices.\n\nDiscuss the difference between soil and dust (see definitions on Page 5-2 [10 in the PDF])\nGoto Table 5-3 and estract a high exposure to Aluminium via soil and dust combined for a child between 1 to 4 years old.\nDiscuss if high exposure should be evaluated on the 95th Percentile or the Maximum.\n\nBe prepared to report back."
  },
  {
    "objectID": "ex/ex_exposure_assessment_databases.html#dietary-exposure-dietex-tool",
    "href": "ex/ex_exposure_assessment_databases.html#dietary-exposure-dietex-tool",
    "title": "Exercise 10. Exposure assessment from databases",
    "section": "Dietary Exposure (DietEx) tool",
    "text": "Dietary Exposure (DietEx) tool\nThe Dietary Exposure DietEx tool is a user-friendly tool for estimating chronic dietary exposure to substances present in food (e.g. intentionally added or naturally present chemicals, contaminants, proteins, novel food ingredients).\nThe DietEx tool is currently unavailable due to ongoing validation by EFSA. So we skip this."
  },
  {
    "objectID": "ex/ex_exposure_assessment_databases.html#the-efsa-comprehensive-european-food-consumption-database",
    "href": "ex/ex_exposure_assessment_databases.html#the-efsa-comprehensive-european-food-consumption-database",
    "title": "Exercise 10. Exposure assessment from databases",
    "section": "The EFSA Comprehensive European Food Consumption Database",
    "text": "The EFSA Comprehensive European Food Consumption Database\nThe Comprehensive Food Consumption Database is a source of information on food consumption across the European Union (EU).\n\nGoto the site for the food consumption database.\nEnter the foodex2-level-1 window\nFilter the data according to Exposure hierarchy L1 - by deselecting (All) and then selecting Coffee, cocoa, tea and infusions only\n\n\n\nDiscuss the difference between the four categories of data?\nExpand the data sheet for Chronic Food Consumption Grams per kilogram of body weight per day (g/kg bw per day) - Consumers only\nYour task is now to assess the consumption for a high consumer of Coffee, cocoa, tea and infusions for two population groups in the EU:\n\n\nAdult and pregnant women\n\n\nDiscuss how to define a high consumer and how to derive the estimate. You are welcome to ask for advice. Be prepared to report back your suggestion and results."
  },
  {
    "objectID": "ex/ex_daily_intake_equation_MC.html",
    "href": "ex/ex_daily_intake_equation_MC.html",
    "title": "Exercise 13. Daily intake equation - solutions for Monte Carlo simulations",
    "section": "",
    "text": "In excercise 12 you got the task to suggest how to set up (implement) a Monte Carlo simulation for the dose equation provided in chapter 10.8.1.\n\\[Dose = \\frac{C \\cdot IR \\cdot EF}{bw}\\] where\n\n\\(C\\) is the concentration of the substance in the medium (mg/l)\n\\(IR\\) is intake rate (l/day)\n\\(EF\\) is exposure frequency (part of year; unitless), and\n\\(bw\\) body weight (g)\n\n\\[C \\sim N(0.00063,0.000063)\\] \\[ IR \\sim N(5,05)\\]\n\\[ EF \\sim U(0.12,0.18)\\]\n\\[ bw \\sim N(25.11,2.51)\\]\nHere we will look at a solution in Excel and a solution in R."
  },
  {
    "objectID": "ex/ex_daily_intake_equation_MC.html#problem",
    "href": "ex/ex_daily_intake_equation_MC.html#problem",
    "title": "Exercise 13. Daily intake equation - solutions for Monte Carlo simulations",
    "section": "",
    "text": "In excercise 12 you got the task to suggest how to set up (implement) a Monte Carlo simulation for the dose equation provided in chapter 10.8.1.\n\\[Dose = \\frac{C \\cdot IR \\cdot EF}{bw}\\] where\n\n\\(C\\) is the concentration of the substance in the medium (mg/l)\n\\(IR\\) is intake rate (l/day)\n\\(EF\\) is exposure frequency (part of year; unitless), and\n\\(bw\\) body weight (g)\n\n\\[C \\sim N(0.00063,0.000063)\\] \\[ IR \\sim N(5,05)\\]\n\\[ EF \\sim U(0.12,0.18)\\]\n\\[ bw \\sim N(25.11,2.51)\\]\nHere we will look at a solution in Excel and a solution in R."
  },
  {
    "objectID": "ex/ex_daily_intake_equation_MC.html#solution-for-monte-carlo-simulation-done-in-excel",
    "href": "ex/ex_daily_intake_equation_MC.html#solution-for-monte-carlo-simulation-done-in-excel",
    "title": "Exercise 13. Daily intake equation - solutions for Monte Carlo simulations",
    "section": "Solution for Monte Carlo simulation done in Excel",
    "text": "Solution for Monte Carlo simulation done in Excel\nDownload the file, open it and go to sheet 1.\n\n\n\n Download xlsx file\n\n\n\nRandom numbers are generated by using the functions RAND() and NORM.INV"
  },
  {
    "objectID": "ex/ex_daily_intake_equation_MC.html#solution-for-monte-carlo-simulation-done-r",
    "href": "ex/ex_daily_intake_equation_MC.html#solution-for-monte-carlo-simulation-done-r",
    "title": "Exercise 13. Daily intake equation - solutions for Monte Carlo simulations",
    "section": "Solution for Monte Carlo simulation done R",
    "text": "Solution for Monte Carlo simulation done R\nLoad useful packages\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nDraw niter random numbers from the input distributions\n\nniter = 10^4\ndf &lt;- data.frame(\n  C = rnorm(niter,0.00063,0.000063),\n  IR = rnorm(niter,5,0.5),\n  EF = runif(niter,min=0.12,max=0.18),\n  bw = rnorm(niter,25.11,2.51)) %&gt;%\n  mutate(dose = C*IR*EF/bw)   \n\nCalculate the probability that the dose exceeds \\(3 \\cdot 10^-5\\)\n\nthreshold = 0.00003 \nmean(df$dose &gt; threshold)\n\n[1] 0.008\n\n\nCalculate the 95th percentile for the dose\n\nquantile(df$dose, probs = 0.95)\n\n         95% \n2.614251e-05 \n\n\nVisualise the distribution of dose in a histogram\n\ndf %&gt;% \n  ggplot(aes(x = dose)) +\n  geom_histogram(binwidth = 0.000001) +\n  geom_vline(xintercept=threshold,col='red')"
  },
  {
    "objectID": "ex/ex_daily_intake_equation_MC.html#dependencies",
    "href": "ex/ex_daily_intake_equation_MC.html#dependencies",
    "title": "Exercise 13. Daily intake equation - solutions for Monte Carlo simulations",
    "section": "Dependencies",
    "text": "Dependencies\nAdd dependency between intake rate and body weight using the formula presented in Box 10.4 of the book where you assume a correlation of \\(r = 0.9\\)\n\nFor solution in Excel - study sheet 2 of the previously downloaded file\n\nniter = 10^4\nr = 0.9\nx1 = rnorm(niter)\nx2 = rnorm(niter)\n\ny1 = 5 + 0.5*x1\ny2 = 25.11 + 2.51*(r*x1 + x2*sqrt(1-r^2))\n\nplot(y1,y2)\n\n\n\n\n\ndf2 &lt;- data.frame(\n  C = rnorm(niter,0.00063,0.000063),\n  x1 = rnorm(niter),\n  x2 = rnorm(niter),\n  EF = runif(niter,min=0.12,max=0.18)) %&gt;%\n  mutate(IR = 5 + 0.5*x1) %&gt;%\n  mutate(bw = 25.11 + 2.51*(r*x1 + x2*sqrt(1-r^2))) %&gt;%\n  mutate(dose = C*IR*EF/bw)\n\n\ndf2 %&gt;% \n  ggplot(aes(x = dose)) +\n  geom_histogram(binwidth=0.000001) +\n  geom_vline(xintercept=threshold,col='red')\n\n\n\n\n\nmean(df2$dose &gt; threshold)\n\n[1] 0\n\n\n\nquantile(df2$dose, probs = 0.95)\n\n        95% \n2.39784e-05"
  },
  {
    "objectID": "ex/ex_bioaccumulation.html",
    "href": "ex/ex_bioaccumulation.html",
    "title": "Exercise 20. Exposure assessment using a model for bioaccumulation",
    "section": "",
    "text": "Do in groups of 1-3\n\n\nModels are often used in risk assessments, and to allow for critical evaluation of a model including methods associated with the model, or reproduction of the modelling, they should be described in a clear way.\nMonte Carlo simulation is a method that can be used to propagate uncertainty in a mathematical model. A careful description of a Monte Carlo simulation is not only needed for transparency and reliability of the results, but it is often a opportunity to resolve uncertainties associated to a description of a model.\nSensitivity analysis is applied to evaluate the influence of sources to uncertainty on the quantity of interest. It is recommended for use in uncertainty analysis to identify what information to collect to reduce uncertainty.\nMethods for sensitivity analysis range from qualitative to quantitative, and from local (considering the influence from sources of uncertainty one at a time) and global (considering the influence of sources of uncertainty in combination). A quantitative sensitivity analysis can be to change a parameter value up and down 10% from its nominal value and see how much it changes the quantity of interest.\nGiven a quantitative uncertainty analysis where parameter uncertainty is described by probability distributions, their influence on the quantity of interest can e.g. be explored\n\ngraphically using scatter plots, and\nby calculating sensitivity metrics and compare them.\n\nIt is recommended to combine a graphical visualisation with sensitivity metrics.\nBioaccumulation is the increase in the concentration of a chemical in a biological organism over time. Both human and ecological/environmental risk assessments consider bioaccumulation processes for exposure assessment.\n\n\n\n\nTo practice describing a Monte Carlo simulation used to propagate uncertainty in parameters of a mathematical model.\nTo apply sensitivity analysis to evaluate the influence of parameters on uncertainty in a quantity of interest.\nTo explore common models for bioaccumulation.\n\n\n\n\n\nPrinted table\n\n\n\n\n45 minutes\n\n\n\nBe prepared to report back at the end of the exercise."
  },
  {
    "objectID": "ex/ex_bioaccumulation.html#exercise-overview",
    "href": "ex/ex_bioaccumulation.html#exercise-overview",
    "title": "Exercise 20. Exposure assessment using a model for bioaccumulation",
    "section": "",
    "text": "Do in groups of 1-3\n\n\nModels are often used in risk assessments, and to allow for critical evaluation of a model including methods associated with the model, or reproduction of the modelling, they should be described in a clear way.\nMonte Carlo simulation is a method that can be used to propagate uncertainty in a mathematical model. A careful description of a Monte Carlo simulation is not only needed for transparency and reliability of the results, but it is often a opportunity to resolve uncertainties associated to a description of a model.\nSensitivity analysis is applied to evaluate the influence of sources to uncertainty on the quantity of interest. It is recommended for use in uncertainty analysis to identify what information to collect to reduce uncertainty.\nMethods for sensitivity analysis range from qualitative to quantitative, and from local (considering the influence from sources of uncertainty one at a time) and global (considering the influence of sources of uncertainty in combination). A quantitative sensitivity analysis can be to change a parameter value up and down 10% from its nominal value and see how much it changes the quantity of interest.\nGiven a quantitative uncertainty analysis where parameter uncertainty is described by probability distributions, their influence on the quantity of interest can e.g. be explored\n\ngraphically using scatter plots, and\nby calculating sensitivity metrics and compare them.\n\nIt is recommended to combine a graphical visualisation with sensitivity metrics.\nBioaccumulation is the increase in the concentration of a chemical in a biological organism over time. Both human and ecological/environmental risk assessments consider bioaccumulation processes for exposure assessment.\n\n\n\n\nTo practice describing a Monte Carlo simulation used to propagate uncertainty in parameters of a mathematical model.\nTo apply sensitivity analysis to evaluate the influence of parameters on uncertainty in a quantity of interest.\nTo explore common models for bioaccumulation.\n\n\n\n\n\nPrinted table\n\n\n\n\n45 minutes\n\n\n\nBe prepared to report back at the end of the exercise."
  },
  {
    "objectID": "ex/ex_bioaccumulation.html#references",
    "href": "ex/ex_bioaccumulation.html#references",
    "title": "Exercise 20. Exposure assessment using a model for bioaccumulation",
    "section": "References",
    "text": "References\nUS EPA, KABAM Version 1.0 User’s Guide and Technical Documentation - Appendix A - Description of Bioaccumulation Model https://www.epa.gov/pesticide-science-and-assessing-pesticide-risks/kabam-version-10-users-guide-and-technical-7\nInformation about Monte Carlo simulation and sensitivity analysis at the EFSA Tutorial on Uncertainty"
  },
  {
    "objectID": "ex/ex_bioaccumulation.html#bioaccumulation-model",
    "href": "ex/ex_bioaccumulation.html#bioaccumulation-model",
    "title": "Exercise 20. Exposure assessment using a model for bioaccumulation",
    "section": "Bioaccumulation model",
    "text": "Bioaccumulation model\nGo to the US EPA’s page with a description of Bioaccumulation a model.\nThe model presented can be used to assess various quantities of interest related to bioaccumulation at different trophic levels.\nThe recommended approach is to first calculate the concentration in organisms at the lowest level of the aquatic food chain (i.e. phytoplankton) before calculating concentrations at higher levels.\nThe page describes several equations to calculate different quantities of interest.\n\nUseful model terminology\n\nGo to the part of the Bioaccumulation model referred to as A.1 Calculation of Fraction of Chemical in the Water Column That Is Freely Dissolved and identify the following:\n\n\nThe mathematical model\nThe parameters within the model\nInput variables or arguments to the model\nThe output from the model\nThe quantity of interest"
  },
  {
    "objectID": "ex/ex_bioaccumulation.html#uncertainty-analysis",
    "href": "ex/ex_bioaccumulation.html#uncertainty-analysis",
    "title": "Exercise 20. Exposure assessment using a model for bioaccumulation",
    "section": "Uncertainty analysis",
    "text": "Uncertainty analysis\n\nPlan\nYour boss has given you the tasks to perform a quantitative uncertainty analysis (using Monte Carlo simulation) of this model and to identify the parameter with the highest influence to uncertainty in the quantity of interest.\n\nMake a plan on how to perform the Monte Carlo simulation. Describe the setting for the Monte Carlo simulation, in particular for which model components you will specify probability distributions (you do not have to come up with the distributions now). Create a new table similar to Table A4 that can be used when you report back to your boss.\nDiscuss you plan with the tutor for feedback\n\nIf there is time, you have at the end of, or after, the exercise, the possibility to execute the Monte Carlo simulation using e.g. R or Excel."
  },
  {
    "objectID": "ex/ex_bioaccumulation.html#sensitivity-analysis",
    "href": "ex/ex_bioaccumulation.html#sensitivity-analysis",
    "title": "Exercise 20. Exposure assessment using a model for bioaccumulation",
    "section": "Sensitivity analysis",
    "text": "Sensitivity analysis\nWe will start exploring two quantitative methods for sensitivity analysis using a simple model. Then, if there is time, you will be asked to apply it on the selected part of the US EPA Bioaccumulation model.\n\nA model\n\nDefine a model with two parameters (\\(\\alpha\\) and \\(\\beta\\)) calculating \\(y\\) given the input variable \\(z\\)\n\n\\[y = \\frac{1}{z\\cdot\\alpha\\cdot\\beta}\\]\nWe have the following information\n\n\n\nSymbol\nDefinition\nValue\nUnit\n\n\n\n\n\\(z\\)\ninput variable\nuser defined\nkg/L\n\n\n\\(\\alpha\\)\nparameter\n3\nnone\n\n\n\\(\\beta\\)\nparameter\n0.2\nnone\n\n\n\\(y\\)\nquantity of interest\ncalculated\nkg/L\n\n\n\nUncertainty in parameters are specified to be\n\n\n\nParameter\nProbability distribution\n\n\n\n\n\\(\\alpha\\)\nN(3,0.1)\n\n\n\\(\\beta\\)\nU(0.1,0.2)\n\n\n\nWe set up a Monte Carlo simulation sampling from the parameter distributions and propagating this uncertainty to the quantity of interest for a given value on the input variable \\(z\\). Here we set \\(z = 1\\).\n\nniter = 10^3\nz = 1\nalpha = rnorm(niter,3,0.1)\nbeta = runif(niter,0.1,0.2)\n\ny = 1/(z*alpha*beta)\n\n\n\nGraphical visualisation of the influence of parameters on a quantity of interest\n\nMake scatter plots with the sampled parameters values on the x-axis and the simulated values of the quantity of interest on the y-axis\n\n\nplot(y~alpha)\n\n\n\n\n\nplot(y~beta)\n\n\n\n\n\nFrom looking at these graphs, which of the parameters do you think have the most contribution to uncertainty in the quantity of interest? influential?\n\nTo get a better overview one can produce a matrix of scatter plots\n\ndf &lt;- data.frame(alpha = alpha, beta = beta, y = y)\nplot(df)\n\n\n\n\n\n\nDerivation of metrics for sensitivity\nSensitivity can be measured in several ways. One option is to calculate the correlation between the sampled values for the parameter and the simulated values for the quantity of interest\n\nCalculate the correlation\n\n\ncor(alpha,y)\n\n[1] -0.1408836\n\n\n\ncor(beta,y)\n\n[1] -0.9699941\n\n\nOr do it in one go\n\ncor(df)\n\n            alpha        beta          y\nalpha  1.00000000 -0.02604574 -0.1408836\nbeta  -0.02604574  1.00000000 -0.9699941\ny     -0.14088364 -0.96999407  1.0000000\n\n\n\nConsidering the correlation, which parameter contributes most to uncertainty in the quantity of interest?"
  },
  {
    "objectID": "ex/ex_bioaccumulation.html#execute-monte-carlo-simulation-and-perform-sensitivity-analysis",
    "href": "ex/ex_bioaccumulation.html#execute-monte-carlo-simulation-and-perform-sensitivity-analysis",
    "title": "Exercise 20. Exposure assessment using a model for bioaccumulation",
    "section": "Execute Monte Carlo simulation and perform sensitivity analysis",
    "text": "Execute Monte Carlo simulation and perform sensitivity analysis\nIf you have the time, implement your Monte Carlo simulation and identify the most influential parameter.\nUllrika will share a solution after the exercise"
  },
  {
    "objectID": "ex/expressing_uncertainty.html",
    "href": "ex/expressing_uncertainty.html",
    "title": "Expressions of uncertainty - How probable is probable?",
    "section": "",
    "text": "Work in groups of 4-5\n\n\n\nWe all use a wide variety of terms to indicate uncertainty - “could”, “maybe”, “possible” and so on. Can these be intepreted as numerical probabilities?\n\n\n\n\nTo show that the interpretation of verbal expressions as numerical probabilities varies between individuals and contexts\nTo demonstrate ways to standardise verbal expressions and allow the student to comment and critique these efforts\n\n\n\n\n\nInterpretation of words and matching to probability scales\n\n\n\n\n45 minutes\n\n\n\nBe prepared to report back at the end of the exercise.\n\n\n\nThe origin to this exercise is from the book Teaching Probability by Jenny Gage and David Spiegelhalter.\nEFSA’s probability scale is defined in The principles and methods behind EFSA’s Guidance on Uncertainty Analysis in Scientific Assessment\nThe table with the IARC classification system is taken from wiki.cancer.org.au/policy/IARC_classifications"
  },
  {
    "objectID": "ex/expressing_uncertainty.html#exercise-overview",
    "href": "ex/expressing_uncertainty.html#exercise-overview",
    "title": "Expressions of uncertainty - How probable is probable?",
    "section": "",
    "text": "Work in groups of 4-5\n\n\n\nWe all use a wide variety of terms to indicate uncertainty - “could”, “maybe”, “possible” and so on. Can these be intepreted as numerical probabilities?\n\n\n\n\nTo show that the interpretation of verbal expressions as numerical probabilities varies between individuals and contexts\nTo demonstrate ways to standardise verbal expressions and allow the student to comment and critique these efforts\n\n\n\n\n\nInterpretation of words and matching to probability scales\n\n\n\n\n45 minutes\n\n\n\nBe prepared to report back at the end of the exercise.\n\n\n\nThe origin to this exercise is from the book Teaching Probability by Jenny Gage and David Spiegelhalter.\nEFSA’s probability scale is defined in The principles and methods behind EFSA’s Guidance on Uncertainty Analysis in Scientific Assessment\nThe table with the IARC classification system is taken from wiki.cancer.org.au/policy/IARC_classifications"
  },
  {
    "objectID": "ex/expressing_uncertainty.html#interpretation-of-words",
    "href": "ex/expressing_uncertainty.html#interpretation-of-words",
    "title": "Expressions of uncertainty - How probable is probable?",
    "section": "Interpretation of words",
    "text": "Interpretation of words\nRead the following paragraph\n\nArthur was worried. It was almost certain there would be a maths test today, and he hadn’t been paying much attention recently. Sally would probably get more marks than hi, but here was a distinct possibility that Zak would mess up. The weather forecast said it might rain, so took a coat, and as he walked to school he thought he was likely to meet Zak, who always played around, and could make him late. If he were late, he was certain to get into trouble. Perhaps there would be a fire drill to disrupt the test? But really there was little chance of that and it was also extremely unlikely an asteroid would hit the school. It was going to be a difficult sort of day.\n\nDo individually\n\nUnderline the words or phrases that express uncertainty, such as “could”, “likely” and so on.\nMake a list of these words, and rank them in terms of highest to lowest probability.\nPut each word on the vertical probability scale. For example, if you think that “almost certain” is near 50% write it next to 50%.\n\nCollect together the responses and dicuss the ranges in opinion."
  },
  {
    "objectID": "ex/expressing_uncertainty.html#high-and-small-risk",
    "href": "ex/expressing_uncertainty.html#high-and-small-risk",
    "title": "Expressions of uncertainty - How probable is probable?",
    "section": "High and small risk",
    "text": "High and small risk\nRead the following text\n\nPregnant women usually have a screening test for possible problems with their foetus. A test result that shows any probability above 1 in 150 (0.6%) of having a baby with Down’s syndrome is called a “higher-risk result” of the NHS Choices website. Such women are offered an aminocentesis to confirm or rule out the diagnosis, but this procedure carries some risk of causing a miscarriage - this risk is estimated to be about 1%, and is described as a “small associated risk” by NHS Choices.\n\nDiscuss the following questions:\n\nWhat do you think of this?\nWhy do you think this wording has been used?"
  },
  {
    "objectID": "ex/expressing_uncertainty.html#probability-scales",
    "href": "ex/expressing_uncertainty.html#probability-scales",
    "title": "Expressions of uncertainty - How probable is probable?",
    "section": "Probability scales",
    "text": "Probability scales\nLet us consider a situation where the assessment question is a Yes/No question. The assessor’s uncertainty (or certainty) about the conclusion can be expressed as the probability that the answer is Yes.\nMany organisations have tried to standardise terms expressing uncertainty. Below is the probabilty scale defined by the European Food Safety Authority (EFSA) to harmonise the use of verbal expressions when communicating uncertainty to the public.\n\n\n\n\nTable 1: EFSAs Approximate probability scale\n\n\nProbability term\nSubjective probability range\nAdditional options\n\n\n\n\n\nAlmost certain\n99-100%\nMore likely than not: &gt;50%\nUnable to give any probability: range is 0-100%. Report as 'inconclusive', 'cannot conclude' or 'unknown'\n\n\nExtremely likely\n95-99%\n\n\nVery likely\n90-95%\n\n\nLikely\n66-90%\n\n\nAbout as likely as not\n33-66%\n\n\n\nUnlikely\n10-33%\n\n\nVery unlikely\n5-10%\n\n\nExtremely unlikely\n1-5%\n\n\nAlmost impossible\n0-1%\n\n\n\n\n\n\n\n\nDiscuss the following questions:\n\nWhat do you think of this scale?\nHow does the scale fit with the matching between verbal expressions and probabilities made in the first exercise?\nWhat do you think comes first when summarising uncertainty in the conclusion of an assessment - the quantitative or the verbal expression?\nWhy do you think it is called the approximate probability scale?"
  },
  {
    "objectID": "ex/expressing_uncertainty.html#verbal-expressions-for-the-level-of-evidence",
    "href": "ex/expressing_uncertainty.html#verbal-expressions-for-the-level-of-evidence",
    "title": "Expressions of uncertainty - How probable is probable?",
    "section": "Verbal expressions for the level of evidence",
    "text": "Verbal expressions for the level of evidence\nThe International Agency for Research on Cancer (IARC) have devised a system of categories to evaluate the carcinogenicity of an agent to humans. An agent is classified based on scientific evidence derived from human and experimental animal studies and from mechanistic and other relevant data. The list of categories and their definition are as follows:\n\nTable 2. IARC classifications of carcinogenic agents\n\n\n\n\n\n\n\nGroup\nDescription\nDefinition\n\n\n\n\nGroup 1\nCarcinogenic to humans\nSufficient evidence of carcinogenicity OR Evidence of carcinogenicity in humans is less than sufficient but there is sufficient evidence of carcinogenicity in experimental animals and strong evidence in exposed humans that the agent acts through a relevant mechanism of carcinogenicity\n\n\nGroup 2A\nProbably carcinogenic to humans*\nLimited evidence of carcinogenicity in humans and sufficient evidence of carcinogenicity in experimental animals OR Inadequate evidence of carcinogenicity in humans and sufficient evidence of carcinogenicity in experimental animals and strong evidence that the carcinogenesis is mediated by a mechanism that also operates in humans OR Limited evidence of carcinogenicity in humans, but belongs, based on mechanistic considerations, to a class of agents for which one or more members have been classified in Group 1 or Group 2A\n\n\nGroup 2B\nPossibly carcinogenic to humans*\nLimited evidence of carcinogenicity in humans and less than sufficient evidence of carcinogenicity in experimental animals OR Inadequate evidence of carcinogenicity in humans but sufficient evidence of carcinogenicity in experimental animals OR Inadequate evidence of carcinogenicity in humans and less than sufficient evidence of carcinogenicity in experimental animals, but with supporting evidence from mechanistic and other relevant data\n\n\nGroup 3\nNot classifiable as to its carcinogenicity to humans\nEvidence of carcinogenicity is inadequate in humans and inadequate or limited in experimental animals OR Evidence of carcinogenicity is inadequate in humans but sufficient in experimental animals, but strong evidence that the mechanism of carcinogenicity in experimental animals does not operate in humans OR Agents that do not fall into any other group. Agents in Group 3 are not determined to be non-carcinogenic or safe overall, but often means that further research is needed.\n\n\n\n* Footnote to the table: The terms “probably carcinogenic” and “possibly carcinogenic” have no quantitative significance and are used simply as descriptors of different levels of evidence of human carcinogenicity, with probably carcinogenic signifying a higher level of evidence than possibly carcinogenic.\nDiscuss the following questions:\n\nHow do you understand the term “probably” and “possibly” in this classification system?\nIs there a difference between expressing the level of evidence and the certainty about a conclusion\nSuggest probabilities (or ranges of probabilities) corresponding to each group!\n\n\n\n\n\n\n\nNote\n\n\n\nThe distinction between expressing the level of evidence and the certainty about a conclusion is also refereed to as indirect and direct uncertainty see van der Bles et al. 2019. In risk assessment you will come across both these ways of expressing uncertainty. We will discuss in a later exercises how expert judgements or modelling can be used to consider indirect uncertainty when quantifying uncertainty in the conclusion."
  },
  {
    "objectID": "ex/ex8_report_template.html#model",
    "href": "ex/ex8_report_template.html#model",
    "title": "Exercise 8. Observe, fit and simulate",
    "section": "Model",
    "text": "Model\n\nDescription\n\n\nJustification\n\n\nSimulation from the model"
  },
  {
    "objectID": "ex/ex4_report_template.html",
    "href": "ex/ex4_report_template.html",
    "title": "Exercise 4. Risk classification",
    "section": "",
    "text": "Which of the two predictors do you think is the better indicator to classify cancers into malignant or benign?\nAnswer:"
  },
  {
    "objectID": "ex/ex4_report_template.html#data-visualisation-and-initial-comparison-of-predictors",
    "href": "ex/ex4_report_template.html#data-visualisation-and-initial-comparison-of-predictors",
    "title": "Exercise 4. Risk classification",
    "section": "",
    "text": "Which of the two predictors do you think is the better indicator to classify cancers into malignant or benign?\nAnswer:"
  },
  {
    "objectID": "ex/ex4_report_template.html#frequency-of-errors-for-the-binary-classification-model-with-mean-radius-as-predictor-and-cutoff-15",
    "href": "ex/ex4_report_template.html#frequency-of-errors-for-the-binary-classification-model-with-mean-radius-as-predictor-and-cutoff-15",
    "title": "Exercise 4. Risk classification",
    "section": "Frequency of errors for the binary classification model with mean radius as predictor and cutoff 15",
    "text": "Frequency of errors for the binary classification model with mean radius as predictor and cutoff 15\nFor the binary classification model using mean radius as predictor:\nFrequency of False positives (FP) is \nFrequency of False negatives (FN) is \nWhich of these two errors do you think is worse? Motivate your answer."
  },
  {
    "objectID": "ex/ex4_report_template.html#sensitivity-and-specificity-for-the-binary-classification-model-with-mean-radius-as-predictor-and-cutoff-15",
    "href": "ex/ex4_report_template.html#sensitivity-and-specificity-for-the-binary-classification-model-with-mean-radius-as-predictor-and-cutoff-15",
    "title": "Exercise 4. Risk classification",
    "section": "Sensitivity and specificity for the binary classification model with mean radius as predictor and cutoff 15",
    "text": "Sensitivity and specificity for the binary classification model with mean radius as predictor and cutoff 15\nFor the binary classification model using mean radius as predictor:\nFrequency of True Positives (TP) is \nFrequency of True Negatives (TN) is \nSensitivity, i.e. \\(\\frac{TP}{TP + FN}\\) is \nSpecificity \\(\\frac{TN}{FP + TN}\\) is"
  },
  {
    "objectID": "ex/ex4_report_template.html#model-comparison-using-the-roc-curve-methodology",
    "href": "ex/ex4_report_template.html#model-comparison-using-the-roc-curve-methodology",
    "title": "Exercise 4. Risk classification",
    "section": "Model comparison using the ROC curve methodology",
    "text": "Model comparison using the ROC curve methodology\n\n\n\n\n\nWhich of the two binary classification models have the best performance according to the AUC measure?\n\nSuggest three things that could be done to build a better classification model?\n\nsuggestion 1\nsuggestion 2\nsuggestion 3"
  },
  {
    "objectID": "ex/chance_belief_frequency.html",
    "href": "ex/chance_belief_frequency.html",
    "title": "Chance, belief and frequency",
    "section": "",
    "text": "Work in groups of 4-5\n\n\n\nProbability is a mathematical concept defined basic rules.\n\n\n\n\n\n\nThe basic rules of probability\n\n\n\nDefinition: The probability of an event A, denoted P(A), is a number between 0 and 1, with P(A) = 0 corresponding to A being impossible, and P(A) = 1 to A being certain.\nComplement rule: P(not A) = 1 - P(A)\nAddition rule:\nfor mutually exclusive events A, B: P(A or B) = P(A) + P(B)\nfor non-mutually exclusive events A, B: P(A or B) = P(A) + P(B) - P(A and B)\nMultiplication rule:\nfor independent events A, B: P(A and B) = P(A) x P(B)\nfor dependent events A, B: P(A and B) = P(A|B) x P(B) where P(A|B) is the conditional probability of A given B\n\n\nThere are different ways to interpret and use probability, sometimes within the same assessment. In this exercise you will be exposed to probability as a\n\nTheoretical probability: The number of outcomes favouring the event, divided by the total number of outcomes, assuming the outcomes are all equally likely.\nFrequency: The proportion of times, in the long run of identical circumstances that the event occurs.\nSubjective probability: A persons confidence that an event will occur, expressed as a number between 0 and 1.\n\n\n\n\n\nTo understand common interpretations of probability and for what they are used\n\n\n\n\n\nExperiment to illustrate the frequency interpretation of probability\nTheoretical probability vs Expected frequency\nSubjective probability\n\n\n\n\n30 minutes\n\n\n\nNo reporting required\n\n\n\nI have used examples and text from the book Teaching probability by Jenny Gage and David Spiegelhalter from 2016. Cambridge University Press."
  },
  {
    "objectID": "ex/chance_belief_frequency.html#exercise-overview",
    "href": "ex/chance_belief_frequency.html#exercise-overview",
    "title": "Chance, belief and frequency",
    "section": "",
    "text": "Work in groups of 4-5\n\n\n\nProbability is a mathematical concept defined basic rules.\n\n\n\n\n\n\nThe basic rules of probability\n\n\n\nDefinition: The probability of an event A, denoted P(A), is a number between 0 and 1, with P(A) = 0 corresponding to A being impossible, and P(A) = 1 to A being certain.\nComplement rule: P(not A) = 1 - P(A)\nAddition rule:\nfor mutually exclusive events A, B: P(A or B) = P(A) + P(B)\nfor non-mutually exclusive events A, B: P(A or B) = P(A) + P(B) - P(A and B)\nMultiplication rule:\nfor independent events A, B: P(A and B) = P(A) x P(B)\nfor dependent events A, B: P(A and B) = P(A|B) x P(B) where P(A|B) is the conditional probability of A given B\n\n\nThere are different ways to interpret and use probability, sometimes within the same assessment. In this exercise you will be exposed to probability as a\n\nTheoretical probability: The number of outcomes favouring the event, divided by the total number of outcomes, assuming the outcomes are all equally likely.\nFrequency: The proportion of times, in the long run of identical circumstances that the event occurs.\nSubjective probability: A persons confidence that an event will occur, expressed as a number between 0 and 1.\n\n\n\n\n\nTo understand common interpretations of probability and for what they are used\n\n\n\n\n\nExperiment to illustrate the frequency interpretation of probability\nTheoretical probability vs Expected frequency\nSubjective probability\n\n\n\n\n30 minutes\n\n\n\nNo reporting required\n\n\n\nI have used examples and text from the book Teaching probability by Jenny Gage and David Spiegelhalter from 2016. Cambridge University Press."
  },
  {
    "objectID": "ex/chance_belief_frequency.html#frequency",
    "href": "ex/chance_belief_frequency.html#frequency",
    "title": "Chance, belief and frequency",
    "section": "Frequency",
    "text": "Frequency\nThe experiment is setup as follows:\n\nAssign one student to flip the symmetric coin of the type Antoninus Pius - Bronze Sestertius - Roman Empire using the virtual coin flipper on random.org\nRecord if the outcome is heads or tails.\nAssign another student to throw a six sided dice using the virtual dice roller on random.org\nRecord if the outcome is a number in the range 1 to 5 or a six\nRepeat N=5 times\n\nAssign one student to record the outcomes in this frequency tree (replace N and # with numbers).\n\n\n\n\nflowchart LR\n  A(\"N\") --&gt;|heads|B(\"#\")\n  A(\"N\") --&gt;|tails|C(\"#\")\n  B(\"#\") --&gt;|1 to 5|D(\"#\")\n  B(\"#\") --&gt;|six|E(\"#\")\n  C(\"#\") --&gt;|1 to 5|F(\"#\")\n  C(\"#\") --&gt;|six|G(\"#\")\n\n\n\n\n\nAnswer the following questions:\n\nWhat is the observed frequency of the event “heads followed by a six”?\n\n\nExpected frequency\n\nIs this a reliable estimate of the expected frequency? If not, what can one do to make it more reliable?\nWhat do you expect the frequency to be if N would be a very large number?\n\n\n\n\n\n\n\nTip\n\n\n\nDefine the events A = “heads” and B = “six”.\nSpecify P(A) and P(B).\nCalculate P(A and B) using the multiplication rule for two independent events.\nDon’t forget to multiply by N to get the expected frequency.\n\n\nRepeat the experiment with N = 100 to verify if estimates of the expected frequencies become more reliable with a larger number of observations.\n\n\n\n\n\n\nTip\n\n\n\nYou can use the prepared spreadsheet in frequency_experiment.xlsx\n\n\n\n Download xlsx file\n\n\n\nFirst you have to figure out how to expand the formulas for 100 iterations.\n\n\n\n\nChance\nNow let us go back to the step where you specified the probabilities P(A) and P(B). How did you do that? One way to do it is to look at the outcome space, find the outcomes that correspond to the event and divide by the total number of outcomes.\nFor the coin the outcome space is “heads” and “tails”, i.e. n = 2. The event of a getting “heads” can occur in one of the outcomes, i.e. m = 1. Under the assumption that all outcomes are equally likely, the theoretical probability for “heads” is \\(\\frac{m}{n} = \\frac{1}{2}\\).\nFor the dice, the outcome space is 1, 2, 3, 4, 5, and 6, i.e. n = 6. The event of getting a “six” can occur in one way, i.e. m = 1. The theoretical probability for the event “six” is therefore \\(\\frac{1}{6}\\).\n\n\n\n\n\n\nNote\n\n\n\nNotice that theoretical probabilities can only be used in balanced situations such as dice, cards, or lottery tickets where it justified to assume symmetry (equal probability) for all possible outcomes.\n\n\n\n\nRelative frequency\nIf we divide the frequency of an event by the number of trials N, we get the relative frequency which is a good estimate of a probability for the event to occur at the next iteration of the same experiment.\nLet \\(m\\) be the number of times the event has occurred. \\(E(\\frac{m}{N})=\\frac{E(m)}{N}=\\frac{N\\cdot P(event)}{N} = P(event)\\)\nRelative frequencies can be used to estimate the probability for an event as long as the observations are equally likely across the full outcome space.\n\n\n\n\n\n\nMake sure N is large\n\n\n\nThe more observations (i.e. larger N) the better estimate.\nThe more extreme event, i.e. very low or high probability of occurring, the more observations are needed.\nBe very skeptical to estimates of probabilities that are either 0 and 1, when the event is possible to occur.\n\n\n\n\nBelief\nTake one of the thumbtacks provided in the exercise and a cup. Put the thumbtack in the cup, shake and place the cup upside down on a table without revealing the outcome.\n\nWhat outcomes are possible?\n\nFocus on the outcome that the thumbtack is having its head down with the needle pointing upwards.\n\nLet everyone in the group state their personal probability of this event as a number between 0 and 1, where 0 means that it is impossible to occur and 1 means that it is certain to occur.\n\n\n\n\n\n\n\nCromwell’s rule\n\n\n\nProbabilities 1 (“the event will definitely occur”) or 0 (“the event will definitely not occur”) should be avoided, except when applied to statements that are logically true or false.\n\n\n\nDiscuss if and why the probabilities differ.\n\nThis is an example of probability as a subjective probability that is purely a personal judgement based on available evidence and assumptions.\nMore evidence ought to result in smaller divergence in judgements. One way to illustrate this is to lift the cup and let everyone revise their judgement.\n\nDo that!\n\nGiven that the evidence is revealing the outcome, the subjective probabilities held by the students in the group should now be either 1 or 0.\nIn risk assessment, we seldom have such full certainty as in this example. Probabilities are almost inevitably based on judgements and assumptions such as random sampling.\n\n\n\n\n\n\nA general advice\n\n\n\nProbability can be thought of as an expected frequency. Instead of saying that “the probability of the event is 0.20 (or 20%)”, you can say “out of 100 situations like this, we would expect the event to occur 20 times”.\nBy carefully stating the denominator (reference class), ambiguity about the meaning of probability can be avoided.\nThis advice applies to any of the interpretations."
  },
  {
    "objectID": "ex/ex4_report_template_withanswers.html",
    "href": "ex/ex4_report_template_withanswers.html",
    "title": "Exercise 4. Risk classification",
    "section": "",
    "text": "Which of the two predictors do you think is the better indicator to classify cancers into malignant or benign?\nAnswer:"
  },
  {
    "objectID": "ex/ex4_report_template_withanswers.html#data-visualisation-and-initial-comparison-of-predictors",
    "href": "ex/ex4_report_template_withanswers.html#data-visualisation-and-initial-comparison-of-predictors",
    "title": "Exercise 4. Risk classification",
    "section": "",
    "text": "Which of the two predictors do you think is the better indicator to classify cancers into malignant or benign?\nAnswer:"
  },
  {
    "objectID": "ex/ex4_report_template_withanswers.html#frequency-of-errors-for-the-binary-classification-model-with-mean-radius-as-predictor-and-cutoff-15",
    "href": "ex/ex4_report_template_withanswers.html#frequency-of-errors-for-the-binary-classification-model-with-mean-radius-as-predictor-and-cutoff-15",
    "title": "Exercise 4. Risk classification",
    "section": "Frequency of errors for the binary classification model with mean radius as predictor and cutoff 15",
    "text": "Frequency of errors for the binary classification model with mean radius as predictor and cutoff 15\nFor the binary classification model using mean radius as predictor:\nFrequency of False positives (FP) is\n\n\n[1] 12\n\n\nFrequency of False negatives (FN) is\n\n\n[1] 51\n\n\nWhich of these two errors do you think is worse? Motivate your answer."
  },
  {
    "objectID": "ex/ex4_report_template_withanswers.html#sensitivity-and-specificity-for-the-binary-classification-model-with-mean-radius-as-predictor-and-cutoff-15",
    "href": "ex/ex4_report_template_withanswers.html#sensitivity-and-specificity-for-the-binary-classification-model-with-mean-radius-as-predictor-and-cutoff-15",
    "title": "Exercise 4. Risk classification",
    "section": "Sensitivity and specificity for the binary classification model with mean radius as predictor and cutoff 15",
    "text": "Sensitivity and specificity for the binary classification model with mean radius as predictor and cutoff 15\nFor the binary classification model using mean radius as predictor:\nFrequency of True Positives (TP) is\n\n\n[1] 161\n\n\nFrequency of True Negatives (TN) is\n\n\n[1] 345\n\n\nSensitivity, i.e. \\(\\frac{TP}{TP + FN}\\) is\n\n\n[1] 0.759434\n\n\nSpecificity \\(\\frac{TN}{FP + TN}\\) is\n\n\n[1] 0.9663866"
  },
  {
    "objectID": "ex/ex4_report_template_withanswers.html#model-comparison-using-the-roc-curve-methodology",
    "href": "ex/ex4_report_template_withanswers.html#model-comparison-using-the-roc-curve-methodology",
    "title": "Exercise 4. Risk classification",
    "section": "Model comparison using the ROC curve methodology",
    "text": "Model comparison using the ROC curve methodology\n\n\n\n\n\nWhich of the two binary classification models have the best performance according to the AUC measure?\n\nSuggest three things that could be done to build a better classification model?\n\nsuggestion 1\nsuggestion 2\nsuggestion 3"
  },
  {
    "objectID": "ex/expert_judgement.html",
    "href": "ex/expert_judgement.html",
    "title": "Expert judgement",
    "section": "",
    "text": "this is an exercise done jointly in class\n\n\n\nExpert judgement are common in risk assessment. To ensure rigour of the assessment, these judgements should be collected in a structured way. Methods have been developed to reduce linguistic uncertainty and cognitive biases when experts make judgements, and to aggregate judgements by a group of experts.\nQuantitative judgements, e.g. judgements expressed by subjective probabilities are preferable over qualitative expressions of uncertainty. The reasons are that\n\nqualitative judgements have different meanings for different people, and\nquantitative judgements can be combined using probability rules (probability calculations)\n\nEFSA defines Expert Knowledge Elicitation as\n\nA systematic, documented and reviewable process to retrieve expert judgements from a group of experts, often in the form of a probability distribution.\n\nIn general, it is possible to make a quantitative judgement when the question asked to an expert is well-defined.\nThe expert should also feel that she has some basis to make his/her judgement.\n\nA good expert makes judgements where she has domain knowledge and is hesitant to make judgements for questions where she feels there is not enough basis for a judgement.\n\nIt is also important that experts receive training in making probabilistic judgements to ensure they understand them.\n\n\n\n\nTo get basic training in probability distributions\nTo demonstrate how quantitative judgements can be elicited and aggregated from experts\n\n\n\n\n\nThe student receives training in making probabilistic judgements\nThe teacher elicit student’s judgements on a binary outcome and a continuous quantity. Judgements are anonymous.\nWe “test” the Wisdom of crowds by considering the aggregated judgement about an observable and compare to the true value.\n\n\n\n\n45 minutes\n\n\n\nNo need to report other than contributing with judgements\n\n\n\nEFSA (2014). Guidance on Expert Knowledge Elicitation in Food and Feed Safety Risk Assessment.\nSheffield Elicitation Framework SHELF web page"
  },
  {
    "objectID": "ex/expert_judgement.html#exercise-overview",
    "href": "ex/expert_judgement.html#exercise-overview",
    "title": "Expert judgement",
    "section": "",
    "text": "this is an exercise done jointly in class\n\n\n\nExpert judgement are common in risk assessment. To ensure rigour of the assessment, these judgements should be collected in a structured way. Methods have been developed to reduce linguistic uncertainty and cognitive biases when experts make judgements, and to aggregate judgements by a group of experts.\nQuantitative judgements, e.g. judgements expressed by subjective probabilities are preferable over qualitative expressions of uncertainty. The reasons are that\n\nqualitative judgements have different meanings for different people, and\nquantitative judgements can be combined using probability rules (probability calculations)\n\nEFSA defines Expert Knowledge Elicitation as\n\nA systematic, documented and reviewable process to retrieve expert judgements from a group of experts, often in the form of a probability distribution.\n\nIn general, it is possible to make a quantitative judgement when the question asked to an expert is well-defined.\nThe expert should also feel that she has some basis to make his/her judgement.\n\nA good expert makes judgements where she has domain knowledge and is hesitant to make judgements for questions where she feels there is not enough basis for a judgement.\n\nIt is also important that experts receive training in making probabilistic judgements to ensure they understand them.\n\n\n\n\nTo get basic training in probability distributions\nTo demonstrate how quantitative judgements can be elicited and aggregated from experts\n\n\n\n\n\nThe student receives training in making probabilistic judgements\nThe teacher elicit student’s judgements on a binary outcome and a continuous quantity. Judgements are anonymous.\nWe “test” the Wisdom of crowds by considering the aggregated judgement about an observable and compare to the true value.\n\n\n\n\n45 minutes\n\n\n\nNo need to report other than contributing with judgements\n\n\n\nEFSA (2014). Guidance on Expert Knowledge Elicitation in Food and Feed Safety Risk Assessment.\nSheffield Elicitation Framework SHELF web page"
  },
  {
    "objectID": "ex/expert_judgement.html#training-in-making-probabilistic-judgements",
    "href": "ex/expert_judgement.html#training-in-making-probabilistic-judgements",
    "title": "Expert judgement",
    "section": "Training in making probabilistic judgements",
    "text": "Training in making probabilistic judgements\nExample of how a training text to experts can look like:\n\nAs an expert you will be asked to express your uncertainty about the true value of a quantity by a probability distribution that quantifies the range and probability of possible values.\nThe probabilities are interpreted as your subjective probability.\nNote that there is only one true value on the quantity – the distribution represents your uncertainty and not variability.\n\n\n\n\n\n\n\nWe generally use 5 judgements to elicit a full distribution\n\nFirst, the lower and upper plausible bounds (P01 and P99)\nthen the median (P50)\nand finally the lower and upper quartiles (P25 and P75)\n\n\nThis is done in this order to reduce the risk of bias\n\n\n\n\n\n\nQuartiles\n\n\n\nQuartiles divide a probability distribution into four sections of equal probability. The first quartile is the 25% quantile (P25). The second quartile is the median (P50). The third quartile is the 75% quantile (P75), which means that it is 25% chance that the true value is above the P75.\n\n\n\n\n\n\n\n\nIn some cases it may be sufficient to elicit a single probability, e.g. that the quantity is above 5\n\n\n\n\n\n\n\nor below 2"
  },
  {
    "objectID": "ex/expert_judgement.html#elicitation-of-a-continuous-quantity",
    "href": "ex/expert_judgement.html#elicitation-of-a-continuous-quantity",
    "title": "Expert judgement",
    "section": "Elicitation of a continuous quantity",
    "text": "Elicitation of a continuous quantity\nNow we will practice doing an elicitation where you are the experts.\nThe elicitor (Ullrika) will introduce the quantity to be elicited in class and make sure the quantity is well defined. She will then ask you to make your judgements on a piece of paper.\n\nFit single distribution to judgements\nWe will use an online app from the SHELF software to fit and explore the judgements. Go to SHELF webpage and open the app for a single expert.\n\n\nProvide your lower and upper plausible bounds by replacing the values 0 and 100 in the box Parameter limits\nProvide your judgements on the median, and the lower and upper quartiles by replacing the values 25, 50 and 75 in the box Parameter values\nClick on the Quartiles tab to view a visualisation of your judgements\nClick on the CDF tab to view a visualisation of your judgements.\nStay on the CDF tab and tick the box show fitted CDF\n\n\nNow you see a line added through the points. This “distribution” is here called a histogram (Note - it is not an actual distribution).\nThe software will help the elicitor to “fit” a probability distribution that represents your judgements by finding one with a CDF that is in good agreement with the points on the CDF graph.\n\nClick on the black triangle and select the Best fitting distribution\n\n\n\nWhat type of distribution is it (name above the graph).\nDo you think the CDF of the fitted distribution corresponds well to your judgements?\n\nYou can try other distributions and see how the fit looks like, but keep the best fitting when continuing.\n\nNow we want to see how the fitted probability distributions intended to represent your uncertainty about the quantity looks like. Tick show fitted PDF and go to the PDF tab.\n\n\nThe PDF representation of a probability distribution can be easier to relate to.\nDo you think the PDF can be a representation of your uncertainty?\n\n\nYou are welcome to change your judgements until you accept the fitted distribution\n\nThe tutor has provided an important threshold for the quantity on the board. Use the fitted probability distribution to calculate the probability that the quantity is below a value provided by the tutor!\n\nThe quantiles P10 and P90 can be seen as bounds of a probability interval where you are 80% certain that the true value lies.\n\nTake note of the calculated quantiles in the table to the right in the section Fitted quantiles and cumulative probabilities below the PDF graph\n\nReplace the 0 with the value for the critical threshold in the box Feedback probabilities\n\n\n\nTake note of the calculated probability in the table to the right in the section Fitted quantiles and cumulative probabilities below the PDF graph\nTo prepare for the next step, fill in your judgements and the calculated quantiles P10 and P90 and calculated probabilities in this google form\n\n\n\nAggregation of expert judgements\nThis part of the exercise is led by the tutor (Ullrika)\nShe will use the SHELF online tool for multiple experts"
  },
  {
    "objectID": "ex/expert_judgement.html#wisdom-of-the-crowds",
    "href": "ex/expert_judgement.html#wisdom-of-the-crowds",
    "title": "Expert judgement",
    "section": "Wisdom of the crowds",
    "text": "Wisdom of the crowds\nThe wisdom of the crowds interesting wiki page states that pooling judgements using a trusted process from a diverse group of experts, that made their judgements independent of each other, is better than using one expert.\nThe tutor has the true answer\nWe will pool your judgments by making averages of the PDF over each point, also known as a linear pool\n\nIs the linear pool a better judgement?"
  },
  {
    "objectID": "ex/ex_benchmark_dose_modelling.html",
    "href": "ex/ex_benchmark_dose_modelling.html",
    "title": "Exercise 11. Benchmark Dose Modelling",
    "section": "",
    "text": "Do in groups of 2-3\n\n\nRisk from chemical exposure on environment and human health is commonly assessed by comparing assessments of exposure and effects. Conservative estimates of exposures resulting in adverse effects can be derived from observed concentrations where there is no observed effect or lowest observed effect. Another approach is to find a suitable model for the dose-response relationship and use the model (or models) to derive the highest dose that corresponds with a benchmark response of interest.\nAn example: Data from an epidemiological study consists of the incidence of a disease for groups of the population with different levels of lifetime exposure to a chemical. A dose-response curve is fitted through the data points. The benchmark response (BMR) is set to 5%. The benchmark dose (BMD) is the dose which corresponds to the BMR.\nIt is recommended to use a lower bound for the BMD resulting from consideration of the influence from uncertainty in the shape of the dose-response curve (including the parameters for the curve). This bound is referred to as the BMDL.\nThe width between the lower and upper bound of the BMD tells something about the reliability of the model.\nMethods have been developed to choose between alternative models using the AIC as goodness-of-fit, or to derive the BMDL by model averaging.\nThere are several software for BMD modelling. The PROAST fits models using maximum likelihood and derives the lower and upper bounds on the BMD using bootstrapping. The US EPA BMDS software uses maximum likelihood for fitting models and Bayesian inference for model averaging. EFSA has a software for BMD modelling using Bayesian inference to fit and derive a probability distribution for the BMD. There are more software as well as R-packages for BMD modelling."
  },
  {
    "objectID": "ex/ex_benchmark_dose_modelling.html#exercise-overview",
    "href": "ex/ex_benchmark_dose_modelling.html#exercise-overview",
    "title": "Exercise 11. Benchmark Dose Modelling",
    "section": "",
    "text": "Do in groups of 2-3\n\n\nRisk from chemical exposure on environment and human health is commonly assessed by comparing assessments of exposure and effects. Conservative estimates of exposures resulting in adverse effects can be derived from observed concentrations where there is no observed effect or lowest observed effect. Another approach is to find a suitable model for the dose-response relationship and use the model (or models) to derive the highest dose that corresponds with a benchmark response of interest.\nAn example: Data from an epidemiological study consists of the incidence of a disease for groups of the population with different levels of lifetime exposure to a chemical. A dose-response curve is fitted through the data points. The benchmark response (BMR) is set to 5%. The benchmark dose (BMD) is the dose which corresponds to the BMR.\nIt is recommended to use a lower bound for the BMD resulting from consideration of the influence from uncertainty in the shape of the dose-response curve (including the parameters for the curve). This bound is referred to as the BMDL.\nThe width between the lower and upper bound of the BMD tells something about the reliability of the model.\nMethods have been developed to choose between alternative models using the AIC as goodness-of-fit, or to derive the BMDL by model averaging.\nThere are several software for BMD modelling. The PROAST fits models using maximum likelihood and derives the lower and upper bounds on the BMD using bootstrapping. The US EPA BMDS software uses maximum likelihood for fitting models and Bayesian inference for model averaging. EFSA has a software for BMD modelling using Bayesian inference to fit and derive a probability distribution for the BMD. There are more software as well as R-packages for BMD modelling."
  },
  {
    "objectID": "ex/ex_benchmark_dose_modelling.html#purpose",
    "href": "ex/ex_benchmark_dose_modelling.html#purpose",
    "title": "Exercise 11. Benchmark Dose Modelling",
    "section": "Purpose",
    "text": "Purpose\n\nTo perform a simple BMD modelling the dataset to derive the BMDL\n\n\nContent\nWe have chosen two online BMD tools that does not require any login.\n\nThe BMD modelling software PROAST developed by RIVM https://www.rivm.nl/en/proast. We ask you to open the RIVM web application PROAST online no account\nThe BMD online tool from US EPAhttps://bmdsonline.epa.gov/\nData from an epidemiological study on the effect of Arsenic on the incidence of lung cancer in a population.\n\n\n\nDuration\n60 minutes\n\n\nReporting\nSubmit the reports from the BMD tool and be prepared to report back in class.\n\n\nReferences\nUS EPA page about benchmark tools\nEFSA (2022) Guidance on the use of the benchmark dose approach in risk assessment"
  },
  {
    "objectID": "ex/ex_benchmark_dose_modelling.html#data",
    "href": "ex/ex_benchmark_dose_modelling.html#data",
    "title": "Exercise 11. Benchmark Dose Modelling",
    "section": "Data",
    "text": "Data\nIn this exercise we will use epidemiological data on the incidence of lung cancer from Steinmaus et al. (2013). Exposure is estimated as lifetime average, all years, based on arsenic daily intakes. This data is one of the studies used in the Arsenic case study.\nData consists of three variables:\nExposure.µg.kg.bw.per.day is the average life time exposure to Arsenic for groups of the population.\nN is the total number of persons in each group\nAdj.cases is the number of cases of lung cancer in each group\n\n\n# A tibble: 4 × 3\n  Exposure.µg.kg.bw.per.day      N Adj.cases\n                      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1                      0.58 138516        70\n2                      1.55 136406        60\n3                      3.46 108281        68\n4                      4.67  66797       107\n\n\nBelow you can see the code to load data and generate new variables for plotting. It is shown here to examplify how to generate the plot below. You do not have to do it yourself.\n\ndf &lt;- as_tibble(read_delim(\"../data/lung_cancer.txt\",show_col_types=FALSE, delim = \"\\t\")) %&gt;% \n  rename(dose = Exposure.µg.kg.bw.per.day) %&gt;%\n  mutate(response = Adj.cases/N) %&gt;%\n  mutate(SE = sqrt((Adj.cases/N)*(1-Adj.cases/N)/N)) %&gt;%\n  mutate(lower = response-2*SE) %&gt;%\n  mutate(upper = response+2*SE)\n\nThe assessment used a value of 0.0005 as the BMR. This is plotted as a red dashed line in the graph below.\nStatistical estimation errors are indiced by error bars for each group.\n\nvalue_for_CES = 0.0005\ndf %&gt;%\n  ggplot(aes(x=dose, y = response)) +\n  geom_point() + \n  geom_errorbar(aes(ymin=lower,ymax=upper)) +\n  geom_hline(yintercept = value_for_CES, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\nDownload and view the lung cancer data in a text editor (you can for example use Notepad).\n\n\n\n\n lung cancer\n\n\n\n\nChoose one of the online BMD tools listed above and open a new session. We recommend you do both. We will report back in class when all groups have done at least one module.\nLoad data into the software\n\n\nPROASTBMDS\n\n\nChoose File and press Upload\nProvide an Analysis name\nPress “Next: Specify”\nSelect the correct variable for the Dose column\nSelect the correct variable for the Response column\nTick Quantal as Type of response data\nSelect the N for the Group size column\nSet BMR (CES) to 0.0005\nRun the analysis without ticking Model averaging\nWhen the State of the analysis is Finished, click on the icon for the Report Analysis results.\nGraphical ouptut - the graphs are not that interesting since the limits on the y-axis are too big.\nGo back to the Overview on PROASTweb and click the icon for Advanced plotting\nChange X axis scaling to None\nChange Y axis to 0.002 and press Apply\nOpen the Report Analysis results again. Now the graphs of the dose-response curve per model should be better.\nFitted models - here you can see the goodness of fit (AIC) for alternative models and the corresponding BMD with lower and upper bounds. conv tells you if the model has converged. Only converged models should be used.\nDiscuss the meaning of converged. Is it possible to see any difference in the visualisation for the converged and non-converged models?\nTake note of the lowest BMDL and highest BMDU over the six models that has converged.\nSave the report from the webpage (e.g. by printing it to a PDF) into NN_lung_cancer.pdf where NN is your name.\nGo back to the previous page (https://proastweb.rivm.nl/) and click on the icon named Specify.\nTick the box for Model averaging, use 200 for the Number of bootstrap runs and Run the analysis.\nThis might take some time.\nWhen the State of the analysis is Finished, click on the icon for the Report Analysis results.\nModel weights - this shows the weights for models used in the model averaging.\nDiscuss the meaning of the weights.\nWhich models are included in the model averaging?\nWhich model has the highest weight?\nNote the BMDL and BMDU based on model averaging.\nSave the report from the webpage (e.g. by printing it to a PDF) into NN_lung_cancer_MA.pdf where NN is your name\n\n\nIn Settings, provide an Analysis Name\nChoose model Type to be Dichotomous\nGoto Data, select Dichotomous as New dataset and click on +New\nYou can type in the Doses, N and Incidences by hand. You can also paste it in by first opening the lung_cancer.txt file in a text editor, select and copy data (no headers). Then you open the table button and paste the data in there and press load.\nYou can zoom in the graph so the y-axis range up to 0.003 (I have not figured out how to zoom out).\nGoto Settings and set the BMR (bottom of the page) to 0.0005.\nClick on Save Analysis\nClick on Run Analysis\nWhen it is done, you will be taken to Output\nZoom in on the graph to the right until it looks something like this\n\nScroll over the models in the table. You will see the curve plotted in the right graph.\nDiscuss the meaning of restricted and unrestricted\nDiscuss the meaning of questionable, viable and recommended.\nSelect a best-fitting model and save model selection\nTake note of the BMDL and BMDU for the selected model.\nClick on Actions and download a report and save it as NN_lung_cancer_BDMS where NN is your name.\nGo back to Settings and select gamma, loglogistic, LogProbit and Weibull under Alternatives/Bayesian Model Averaging\nDiscuss why these models were selected\nClick on Save Analysis\nClick on Run Analysis\nIn Outputs, there is a new table called Bayesian Model Results. Discuss the meaning of prior and posterior weights\nWhich model has the highest weight in the Bayesian model averaging?\nNote the BMDL and BMDU based on model averaging.\nSave the report into NN_lung_cancer_BDMS_MA where NN is your name\n\n\n\n\nWe will discuss the BMD modelling from both software in class. You are welcome to try out the other software."
  },
  {
    "objectID": "ex/ex_daily_intake_equation.html",
    "href": "ex/ex_daily_intake_equation.html",
    "title": "Exercise 12. Daily intake equation",
    "section": "",
    "text": "Do individually\n\n\n\n\n\n\nTo apply interval arithmetic\nTo explore the principle of 1 dimensional Monte Carlo simulation\n\n\n\n\n\n\n\n20 minutes\n\n\n\nBe prepared to report back at the end of the exercise.\n\n\n\nThe course book"
  },
  {
    "objectID": "ex/ex_daily_intake_equation.html#exercise-overview",
    "href": "ex/ex_daily_intake_equation.html#exercise-overview",
    "title": "Exercise 12. Daily intake equation",
    "section": "",
    "text": "Do individually\n\n\n\n\n\n\nTo apply interval arithmetic\nTo explore the principle of 1 dimensional Monte Carlo simulation\n\n\n\n\n\n\n\n20 minutes\n\n\n\nBe prepared to report back at the end of the exercise.\n\n\n\nThe course book"
  },
  {
    "objectID": "ex/ex_daily_intake_equation.html#the-daily-dose-equation",
    "href": "ex/ex_daily_intake_equation.html#the-daily-dose-equation",
    "title": "Exercise 12. Daily intake equation",
    "section": "The daily dose equation",
    "text": "The daily dose equation\n\\[Dose = \\frac{C \\cdot IR \\cdot EF}{bw}\\]"
  },
  {
    "objectID": "ex/ex_daily_intake_equation.html#interval-artithmetic",
    "href": "ex/ex_daily_intake_equation.html#interval-artithmetic",
    "title": "Exercise 12. Daily intake equation",
    "section": "Interval artithmetic",
    "text": "Interval artithmetic\n\nReplicate these calculations chapter 9.3.2"
  },
  {
    "objectID": "ex/ex_daily_intake_equation.html#monte-carlo-simulation",
    "href": "ex/ex_daily_intake_equation.html#monte-carlo-simulation",
    "title": "Exercise 12. Daily intake equation",
    "section": "Monte Carlo simulation",
    "text": "Monte Carlo simulation\nSuggest how to set up (implement) a Monte Carlo simulation for the example provided in chapter 10.8.1.\nThe aim is that you should be able to do this in Excel and R."
  },
  {
    "objectID": "ex/ex_environ_exposure_assessment.html",
    "href": "ex/ex_environ_exposure_assessment.html",
    "title": "Exercise 9. Environmental exposure assessment",
    "section": "",
    "text": "Do in groups of 1-3\n\n\nPredictive models of exposure of chemical substances in the environment are common in risk assessment.\nThere are several mass-balance models to support exposure assessment.\nAn assessments starts with the source and level of emissions. The exposure assessment models derive masses of chemical substances after reaching steady state."
  },
  {
    "objectID": "ex/ex_environ_exposure_assessment.html#exercise-overview",
    "href": "ex/ex_environ_exposure_assessment.html#exercise-overview",
    "title": "Exercise 9. Environmental exposure assessment",
    "section": "",
    "text": "Do in groups of 1-3\n\n\nPredictive models of exposure of chemical substances in the environment are common in risk assessment.\nThere are several mass-balance models to support exposure assessment.\nAn assessments starts with the source and level of emissions. The exposure assessment models derive masses of chemical substances after reaching steady state."
  },
  {
    "objectID": "ex/ex_environ_exposure_assessment.html#purpose",
    "href": "ex/ex_environ_exposure_assessment.html#purpose",
    "title": "Exercise 9. Environmental exposure assessment",
    "section": "Purpose",
    "text": "Purpose\n\nTo extract output values from a common environmental exposure assessment model\n\n\nContent\n\nExcel and SimpleBox-TRAM vs 3.24.\n\n\n\nDuration\n30 minutes\n\n\nReporting\nBe prepared to report back at the end of the exercise.\n\n\nReferences"
  },
  {
    "objectID": "ex/ex_environ_exposure_assessment.html#simplebox",
    "href": "ex/ex_environ_exposure_assessment.html#simplebox",
    "title": "Exercise 9. Environmental exposure assessment",
    "section": "SimpleBox",
    "text": "SimpleBox\nSimpleBox is a multimedia mass balance model for evaluating the fate of chemical substances developed by RIVM.\nThe environment is modelled as consisting of well-mixed environmental compartments (air, water, sediment, soil, etc.), at three spatial scales. Emissions to the compartments, transfer and partitioning between the compartments, and removal from the compartments are used to compute the steady state and quasi-dynamic masses of chemical substance in the environment.\nThe SimpleBox model simulates the environmental fates of different substances in different landscape settings, of which the characteristics are provided with the model database. In its default settings, SimpleBox returns results for a typical chemical, given a typical emission, to a typical environment.\n\nGo to the website at RIVM and read about the SimpleBox model\nWhat does RIVM stands for?\nWho has developed SimpleBox?"
  },
  {
    "objectID": "ex/ex_environ_exposure_assessment.html#assess-exposure-from-emissions-of-hexachlorobenzene-to-agricultural-soil",
    "href": "ex/ex_environ_exposure_assessment.html#assess-exposure-from-emissions-of-hexachlorobenzene-to-agricultural-soil",
    "title": "Exercise 9. Environmental exposure assessment",
    "section": "Assess exposure from emissions of hexachlorobenzene to agricultural soil",
    "text": "Assess exposure from emissions of hexachlorobenzene to agricultural soil\nYou will use the version of SimpleBox that comes with the Targeted Risk Assessment tool from ECETOC\n\nDownload the excel file for SimpleBox-TRAM and open it. Enable Content!\n\n\n\n\n SimpleBox-TRAM\n\n\n\n\nGoto sheet chembase and note the ID number for hexachlorobenzene.\nGoto sheet input and write down the row number ID in cell I44.\n\nThe model will now use substance specific parameters for the calculations.\n\nSet USE volume at local scale to 1 in cell I85\nSet the value on EMISSION to agricultural soil to be 2\nGoto sheet level 3 output and note the concentration at local level in fresh water in cell D13\nStudy the graphical output, i.e. the steady-state mass flows in the same sheet starting in cell BL1.\n\nWe will talk more about this model during a seminar/lecture."
  },
  {
    "objectID": "ex/ex_exposure_bioaccumulation.html",
    "href": "ex/ex_exposure_bioaccumulation.html",
    "title": "Ex Exposure",
    "section": "",
    "text": "Work together in groups of 1 to 3\n\n\n\n\n\n\nTo apply Monte Carlo simulation on an equation impelemented in R\nTo perform sensitivity analysis towards\n\n\n\n\n\n\n\n45 minutes\n\n\n\nBe prepared to report back at the end of the exercise."
  },
  {
    "objectID": "ex/ex_exposure_bioaccumulation.html#exercise-overview",
    "href": "ex/ex_exposure_bioaccumulation.html#exercise-overview",
    "title": "Ex Exposure",
    "section": "",
    "text": "Work together in groups of 1 to 3\n\n\n\n\n\n\nTo apply Monte Carlo simulation on an equation impelemented in R\nTo perform sensitivity analysis towards\n\n\n\n\n\n\n\n45 minutes\n\n\n\nBe prepared to report back at the end of the exercise."
  },
  {
    "objectID": "ex/ex_exposure_bioaccumulation.html#section",
    "href": "ex/ex_exposure_bioaccumulation.html#section",
    "title": "Ex Exposure",
    "section": "",
    "text": "Detta är en övning om hur flundror ackumulerar det PCB som finns i havsvattnet där de bor. Modellen som används är någorlunda invecklad, men bör ändå inte vara svår att skriva in i R.\nÖvningen innehåller några olika moment. Ni ska göra känslighetsanalys av er modell; ni ska anta att några av de ingående variablerna är korrelerade (vilket kan påverka känslighetsanalysen); och slutligen ska ni studera hur ackumulationen förändras över tiden, genom att lägga in en tidsserie i modellen.\nEkvationen som modellen bygger på är:\n\\[ V_t = V_0 e^{-(K+G)t} \\, \\frac{U_{conc}\\,\\alpha C_jW_j}{K\\,+\\,G}\\left( 1 - e^{-(K+G)t}\\right)\\] och vad de olika variablerna betyder kan ni se i tabellen här.\n\n\n\n\n\n\n\n\nParameter\nDescription\nDistribution\n\n\n\n\n\\(V_0\\) | Belastning av PCB vid tiden 0 (\\(\\mu\\) g PCB’s / g) | N(10, 2) |\n\n\n\n\n\\(K\\) | Utsöndringshastighet av PCB (1/d) | U(0.045, 0.095)|\n\n\n\n\n\\(G\\) | Tillväxtkoefficient (g/g/d) | U(0.012, 0.014)|\n\n\n\n\n\\(U\\) | Upptagskoefficient over gälarna (L/g/d) | N(0.005, 0.0002)|\n\n\n\n\n\\(conc\\) | Koncentration av PCB i vattnet (\\(\\mu\\)g / L) | N(4,1)|\n\n\n\n\n\\(\\alpha\\) | Assimilation av PCB (dimensionslös) | T(0.05, 0.18, 0.5)|\n\n\n\n\n\\(C_j\\) | Konsumtionshastighet av bytestyp j (g/g/d) | logN(2.4, 0.5)|\n\n\n\n\n\\(W_j\\) | Belastning av PCB hos byten av typ j (\\(\\mu\\)g PCB / g) | logN(1.95, 0.3)|\n\n\n\n\n\\(t\\) | tid i dagar | 1|\n\n\n\n\n\\(V_t\\) | Belastning av PCB vid tiden t (\\(\\mu\\)g PCBs /g) ||\n\n\n\n\n\nFundera gärna lite grann, men bara lagom mycket, på vad ekvationen egentligen säger.\n\nEkvationen gäller för en viss åldersklass när dessa fiskar äter bytesarterna \\(j\\). Vi bryr oss bara om en åldersklass (oklart vilken), men däremot så antar vi att de äter flera olika byten, vilket beskrivs av fördelningarna för \\(C_j\\) och \\(W_j\\).\nEkvationen räknar ut ackumulationen efter den tid man stoppar in i \\(t\\). Till att börja med låter vi detta vara 1 dag, dvs \\(t = 1\\).\n\n\nSensitivity analysis\n\nGör en funktion som slumpar \\(N\\) värden från sannolikhetsfördelningarna för modellens ingångsvärden och beräknar \\(V_t\\). Funktionen ska skapa en \\(N x 8\\) matris av slumptalen från ingångvärdena, och en vektor med \\(N\\) värden på \\(V_t\\).\n\nreturn( list( input = “N x 8 matris”, output =“N slumptal av Vt”) )\n\nKör modellen med 10 000 slumptal och uppskatta risken att flundror har ackumulerat mer än 7.5 g per gram kroppsvikt efter en dag. Beräkna även medelvärdet av \\(V_t\\).\nNu ska ni göra en enkel känslighetsanalys av variablerna i er modell. Detta gör ni enklast genom att beräkna en korrelation mellan \\(V_t\\) och alla de ingående variablerna i analysen. Ni ska använda funktionen \\(cor\\) som beräknar alla parvisa korrelationer mellan olika variabler. Variablerna måste finnas som kolumner i en matris. Denna matris är\n\nmat = cbind(output,input)\nAlla parvisa korrelationer är\ncor(mat)\nVi är bara intresserade av korrelation mellan ingående och den utgående variabeln. Därför kan vi välja att titta på den första kolumnen\ncor(mat)[,1]\nVarför är det första talet 1?\nBäst är att göra en rank-korrelation, som inte antar ett linjärt samband mellan variablerna. Ett exempel på en sådan här korrelationsanalys kan då se ut så här:\ncor(mat, method=“spearman”)[-1,1]\n\nGör också figurer som visar hur \\(V_t\\) beror av samtliga variabler.\nFör vilken, eller vilka, variabler är belastning av PCB vid tiden t (dvs modellens utgående variabel) mest känslig?\n\n\n\nKänslighet mot korrelation i ingångsvärden}\n\nMan kan tänka sig att bytesdjur med högt PCB-innehåll är lätta för flundrorna att fånga, och att det därför blir en korrelation mellan \\(C_j\\) och \\(W_j\\). Skapa en ganska kraftig korrelation mellan dessa parametrar. Använd funktionen “cornode” i mc2d-paketet i R.\nHur påverkar korrelationen risken att de ackumulerar mer än 7.5 g?\nHur påverkar korrelationen modellens känslighet för olika variabler?\nMan kan också tänka sig, att flundror med mycket PCB i sig från början, blir slöa och därmed har en lägre konsumtionshastighet. Hur blir det då?\n\n\n\nKänslighet mot val av fördelning för osäkerhet i ingångsvärden\n\nTa bort korrelationerna och utgå ifrån grundvärdena på parametrarna. Ändra fördelningen för \\(C_j\\) till en likformig inom intervallet 0 till 25. Det är ungefär samma intervall som normalfördelningen skulle ha. Kör om simuleringen.\nHur känslig verkar modellen för förändring i denna parameters fördelning?\n\n\n\nKänslighet mot tidperspektiv i riskbedömningen\n\nHittills har vi bara studerat hur ackumulationen ser ut efter en dags exponering för giftet. Ändra tiden till 15 dagar, dvs. två veckor senare. Hur mycket har medelvärdet för ackumulationen ändrats? Har känsligheten för någon av parametrarna ändrats? Varför då, då? (Använd alla grundvärden för parametrarna och ingen korrelation).\nKör modellen för olika tidpunkter, säg var 14:e dag i ett helt år.\nVad är sannolikheten att de har ackumulerat mer än 7.5 g efter 1 dag, 15 dagar, 29 dagar etc upp till ett år? Plotta den beräknande sannolikheten mot tid.\nVad är sannolikheten att de ackumulerat mer än 0.75 g efter samma tid? Plotta sannolikheten mot tid i samma bild som föregående.\nVad beror det på att bioackumulationen inte bara fortsätter att öka, ju längre tiden går?\nHur ändras känsligheten för de olika variablerna ju längre tid man tittar på? Kan ni förklara det?"
  },
  {
    "objectID": "ex/ex_report_template.html",
    "href": "ex/ex_report_template.html",
    "title": "Report of exercise [number]",
    "section": "",
    "text": "Write text here"
  },
  {
    "objectID": "ex/ex_report_template.html#header",
    "href": "ex/ex_report_template.html#header",
    "title": "Report of exercise [number]",
    "section": "",
    "text": "Write text here"
  },
  {
    "objectID": "ex/ex_species_sensitivity_distribution.html",
    "href": "ex/ex_species_sensitivity_distribution.html",
    "title": "Exercise 15. Hazard assessment using Species Sensitivity Distributions",
    "section": "",
    "text": "The Species Sensitivity Distribution methodology is a common way in hazard assessment for setting safe limits on chemical concentrations in freshwaters. It usually require experimentally determined NOEC values for a number of species from different taxonomic groups.\nThe species sensitivity distribution (SSD) is a statistical approach that is used to estimate either the concentration of a chemical that is hazardous to no more than x% of all species (the HCx) or the proportion of species potentially affected by a given concentration of a chemical.\nThe SSD approach follows a three-step procedure:\nFirst, compile results from separate toxicity tests on a given chemical for various aquatic animal species.\nSecond, fit candidate probability distribution(s) to the data.\nThird, use the fitted distribution(s) to infer a concentration that will be protective of a desired proportion of species in a hypothetical aquatic community.\nThere are several SSDs tools available for risk assessors.\nIf a SSD cannot be applied (which is usually the case), the hazard assessment is performed using assessment factors, where the general principle is to divide the result from a laboratory test by an appropriate assessment factor. The table below is taken from ECHAs dose response guidance, go to the original document to read more about this.\n\n\n\n\n\nTo apply the SSD approach to find a hazardous concentration\nTo explore one of the many tools for SSD, namely the R-package ssdtools, in which the modelling can be done in an user friendly Shiny app\nTo compare with a situation of a SSD with more data, and with a situation of less data and the use of assessment factors\n\n\n\n\n\nFind hazardous concentration by SSD modelling bases on dose-response data on the chemical metolachlor\n\n\n\n\n90 minutes\n\n\n\nWrite a report using a quarto document and upload it on the assignment in canvas.\n\nUse the following information in the YAML (i.e. the top information in a .qmd document)\n\ntitle: “Exercise 15. Hazard assessment using Species Sensitivity Distributions” subtitle: “Report” author: “Your name” date: today format: html: toc: true code-fold: true message: false warning: false embed-resources: true\n\nYou will explore the SSD approach using an app. Note that it is possible to get the full R-code for the SSD modelling. You can use this R-code to generate your report.\n\n\n\n\n\n\n\nTip\n\n\n\nI recommend you copy on the whole code into an R-chunk and then divide the chunk into multiple chunks. This allows you to have better control of the output from each chunk.\nNote that you cannot open the shinyapp in a qmd that is being rendered. An alternative to remove it is to put a # in front of the command\nSometimes you get messages or warnings when running a code. These might or might not be important. I judge the messages Ive seen on your computer as not important. You can suppress a message or warning by typing #| warning: false and #| message: false in the first row directly after the curly bracket in the R-chunk.\nIf you just want to look at your data you can e.g. open it in Excel. When your data is a csv-file, I recommend you open an empty Excel-file, got to Data&gt;Get data&gt;From File&gt;From Text/CSV and follow the instructions.\n\n\n\n\n\nECHA Guidance on dose response modelling\nFox, D.R., van Dam, R.A., Fisher, R., Batley, G.E., Tillmanns, A.R., Thorley, J., Schwarz, C.J., Spry, D.J. and McTavish, K. (2021), Recent Developments in Species Sensitivity Distribution Modeling. Environ Toxicol Chem, 40: 293-308. https://doi.org/10.1002/etc.4925\nThe shinyssdtools developed for the British Columbia Ministry of Environment and Climate Change Strategy. https://github.com/bcgov/shinyssdtools"
  },
  {
    "objectID": "ex/ex_species_sensitivity_distribution.html#exercise-overview",
    "href": "ex/ex_species_sensitivity_distribution.html#exercise-overview",
    "title": "Exercise 15. Hazard assessment using Species Sensitivity Distributions",
    "section": "",
    "text": "The Species Sensitivity Distribution methodology is a common way in hazard assessment for setting safe limits on chemical concentrations in freshwaters. It usually require experimentally determined NOEC values for a number of species from different taxonomic groups.\nThe species sensitivity distribution (SSD) is a statistical approach that is used to estimate either the concentration of a chemical that is hazardous to no more than x% of all species (the HCx) or the proportion of species potentially affected by a given concentration of a chemical.\nThe SSD approach follows a three-step procedure:\nFirst, compile results from separate toxicity tests on a given chemical for various aquatic animal species.\nSecond, fit candidate probability distribution(s) to the data.\nThird, use the fitted distribution(s) to infer a concentration that will be protective of a desired proportion of species in a hypothetical aquatic community.\nThere are several SSDs tools available for risk assessors.\nIf a SSD cannot be applied (which is usually the case), the hazard assessment is performed using assessment factors, where the general principle is to divide the result from a laboratory test by an appropriate assessment factor. The table below is taken from ECHAs dose response guidance, go to the original document to read more about this.\n\n\n\n\n\nTo apply the SSD approach to find a hazardous concentration\nTo explore one of the many tools for SSD, namely the R-package ssdtools, in which the modelling can be done in an user friendly Shiny app\nTo compare with a situation of a SSD with more data, and with a situation of less data and the use of assessment factors\n\n\n\n\n\nFind hazardous concentration by SSD modelling bases on dose-response data on the chemical metolachlor\n\n\n\n\n90 minutes\n\n\n\nWrite a report using a quarto document and upload it on the assignment in canvas.\n\nUse the following information in the YAML (i.e. the top information in a .qmd document)\n\ntitle: “Exercise 15. Hazard assessment using Species Sensitivity Distributions” subtitle: “Report” author: “Your name” date: today format: html: toc: true code-fold: true message: false warning: false embed-resources: true\n\nYou will explore the SSD approach using an app. Note that it is possible to get the full R-code for the SSD modelling. You can use this R-code to generate your report.\n\n\n\n\n\n\n\nTip\n\n\n\nI recommend you copy on the whole code into an R-chunk and then divide the chunk into multiple chunks. This allows you to have better control of the output from each chunk.\nNote that you cannot open the shinyapp in a qmd that is being rendered. An alternative to remove it is to put a # in front of the command\nSometimes you get messages or warnings when running a code. These might or might not be important. I judge the messages Ive seen on your computer as not important. You can suppress a message or warning by typing #| warning: false and #| message: false in the first row directly after the curly bracket in the R-chunk.\nIf you just want to look at your data you can e.g. open it in Excel. When your data is a csv-file, I recommend you open an empty Excel-file, got to Data&gt;Get data&gt;From File&gt;From Text/CSV and follow the instructions.\n\n\n\n\n\nECHA Guidance on dose response modelling\nFox, D.R., van Dam, R.A., Fisher, R., Batley, G.E., Tillmanns, A.R., Thorley, J., Schwarz, C.J., Spry, D.J. and McTavish, K. (2021), Recent Developments in Species Sensitivity Distribution Modeling. Environ Toxicol Chem, 40: 293-308. https://doi.org/10.1002/etc.4925\nThe shinyssdtools developed for the British Columbia Ministry of Environment and Climate Change Strategy. https://github.com/bcgov/shinyssdtools"
  },
  {
    "objectID": "ex/ex_species_sensitivity_distribution.html#get-ssd-data",
    "href": "ex/ex_species_sensitivity_distribution.html#get-ssd-data",
    "title": "Exercise 15. Hazard assessment using Species Sensitivity Distributions",
    "section": "Get SSD data",
    "text": "Get SSD data\nDownload a ssd data set.\nThis data set consists of cronic EC10 or LOEC values from exposure to the substance metolachlor for six different species in an aquatic system. This data is a selection from a bigger data set on 21 species.\n\n\n\n Download csv file"
  },
  {
    "objectID": "ex/ex_species_sensitivity_distribution.html#a-tool-for-species-sensitivity-distributions",
    "href": "ex/ex_species_sensitivity_distribution.html#a-tool-for-species-sensitivity-distributions",
    "title": "Exercise 15. Hazard assessment using Species Sensitivity Distributions",
    "section": "A tool for Species Sensitivity Distributions",
    "text": "A tool for Species Sensitivity Distributions\n\nInstall and open the SSD tool.\n\nThese commands will install a R-package, load it into R and then open a shinyapp.\n\ndevtools::install_github(\"bcgov/shinyssdtools\")\nlibrary(shinyssdtools)\nshinyssdtools::run_app()\n\n\n\nLoading required package: ssdtools\n\n\nWarning: package 'ssdtools' was built under R version 4.3.1\n\n\nWarning in checkMatrixPackageVersion(): Package version inconsistency detected.\nTMB was built with Matrix version 1.5.4\nCurrent Matrix version is 1.6.1.1\nPlease re-install 'TMB' from source using install.packages('TMB', type = 'source') or ask CRAN for a binary version of 'TMB' matching CRAN's 'Matrix' package\n\n\nPlease replace the following in your scripts:\n- `ssdtools::boron_data` with `ssddata::ccme_boron`\n- `ssdtools::ccme_data` with `ssddata::ccme_data`\n\n\n\nChoose to open the app in the browser.\n\n\n\n\n\n\n\nTip\n\n\n\nStop the shiny app by closing the webrower and click on the STOP-sign in the R-console\n\nRun the app by the command\n\nshinyssdtools::run_app()\n\n\n\n\nLoad the data metolachlor_ssd.csv to the app using option 2. Upload a csv file\n\n\n\nLook at the data, what does it contain?\nGo to tab 2. Fit\n\n\nStudy the goodness of fit measures and the visual fits between curves and data points, and select your top three best distributions. Write down the arguments why you choose these three distributions. \n\nGo to to the tab predict\n\n\n\nSet the threshold that defines the hazardous concentration, e.g. 5% defines HC5. Use 5%!\n\n\n\nAt this point you can read out HC5 for each model and the model average.\n\nIf we want to consider statistical errors there is an option to derive an uncertainty interval for the hazardous concentration and select its lower bound. Uncertainty intervals can be characterised by bootstrapping which is a method to sample with replacement from data, refit the model and do this many times.\n\nSet the number of bootstrap samples. Use 1000 to avoid it taking too long time.\nClick on the button CL to start the bootstrapping (this will take some time)\n\n\n\nTake note of the lower bound (lcl)of the uncertainty interval for the hazardous concentration corresponding to protection of at least 95% of the population.\nDownload the R-code for the analysis you did and put it into the report.\n\n\n\nRender the document. Note that you might have to change the path to data.\n\nWrite in the report:\n\nThe three distributions that were chosen and the justification for this choice.\nA lower bound for the concentration of metolachlor that is hazardous to no more than 5% of all species in an aquatic system.\nCompare to the numbers 0.72 and 0.107 - this is the model average and lower uncertainty bound from a SSD using the full data set of 21 species\nCompare to a hazardous concentration derived from two species and an appropriate assessment factor. Select two species from your data and select the assessment factor from the table in the beginning of this document.\n\n\nSubmit on the assignment in canvas"
  },
  {
    "objectID": "ex/ex_uncertainty_analysis.html",
    "href": "ex/ex_uncertainty_analysis.html",
    "title": "Exercise 19. Food safety assessment with uncertainty analysis",
    "section": "",
    "text": "We do this jointly in class.\n\n\nWe can never be completely certain about the future, either in science, or in everyday life. Even when there is strong evidence that something will happen, there will almost always be uncertainty about the outcome. But by taking account of this uncertainty, we often can make better, more transparent decisions about things that may affect the outcome.\nThe European Food Safety Authority (EFSA) has developed a guidance for uncertainty analysis in scientific assessment which requires all assessment to say\n\nwhat sources of uncertainty have been identified and contain\na characterisation of their overall impact on the assessment conclusion.\n\nThe reason is that uncertainty of scientific conclusions has important implications for decision making and it is important to communicate this uncertainty for the transparency of assessments.\n\n\n\n\nTo perform an iterated uncertainty analysis, consisting of probability bound analysis followed by a fully probabilistic analysis.\n\n\n\n\n\nA human chemical risk assessment problem\nA probabilistic uncertainty analysis using input from expert judgement and Monte Carlo-simulation\nA probability bound analysis using probability intervals from the same distributions\n\n\n\n\n45 minutes (+ tutorial videos)\n\n\n\nBe prepared to report back during and at the end of the exercise.\n\n\n\nTutorial videos on EFSA’s topic page on uncertainty (examples in the chemical area)"
  },
  {
    "objectID": "ex/ex_uncertainty_analysis.html#exercise-overview",
    "href": "ex/ex_uncertainty_analysis.html#exercise-overview",
    "title": "Exercise 19. Food safety assessment with uncertainty analysis",
    "section": "",
    "text": "We do this jointly in class.\n\n\nWe can never be completely certain about the future, either in science, or in everyday life. Even when there is strong evidence that something will happen, there will almost always be uncertainty about the outcome. But by taking account of this uncertainty, we often can make better, more transparent decisions about things that may affect the outcome.\nThe European Food Safety Authority (EFSA) has developed a guidance for uncertainty analysis in scientific assessment which requires all assessment to say\n\nwhat sources of uncertainty have been identified and contain\na characterisation of their overall impact on the assessment conclusion.\n\nThe reason is that uncertainty of scientific conclusions has important implications for decision making and it is important to communicate this uncertainty for the transparency of assessments.\n\n\n\n\nTo perform an iterated uncertainty analysis, consisting of probability bound analysis followed by a fully probabilistic analysis.\n\n\n\n\n\nA human chemical risk assessment problem\nA probabilistic uncertainty analysis using input from expert judgement and Monte Carlo-simulation\nA probability bound analysis using probability intervals from the same distributions\n\n\n\n\n45 minutes (+ tutorial videos)\n\n\n\nBe prepared to report back during and at the end of the exercise.\n\n\n\nTutorial videos on EFSA’s topic page on uncertainty (examples in the chemical area)"
  },
  {
    "objectID": "ex/ex_uncertainty_analysis.html#preparations",
    "href": "ex/ex_uncertainty_analysis.html#preparations",
    "title": "Exercise 19. Food safety assessment with uncertainty analysis",
    "section": "Preparations",
    "text": "Preparations\nView the first two tutorial videos\nKey concepts (17 minutes)\nMethods and options for basic assessment of uncertainty (27 minutes)"
  },
  {
    "objectID": "ex/ex_uncertainty_analysis.html#description-of-the-assessment",
    "href": "ex/ex_uncertainty_analysis.html#description-of-the-assessment",
    "title": "Exercise 19. Food safety assessment with uncertainty analysis",
    "section": "Description of the assessment",
    "text": "Description of the assessment\nThe example is taken from the human health risk assessment of inorganic arsenic in food by EFSA that constituted one of the case studies.\n\nHazard assessment\nA Reference Point \\(RP\\) of 0.062 μg iAs/kg bw per day was identified by the CONTAM Panel for skin cancer. This value is the BMDL from a model average of BMD modelling on skin cancer.\nModel Averaged BMD model\n\n\n\nBMDL\nBMD\nBMDU\n\n\n\n\n0.062\n0.147\n0.21\n\n\n\n\n\nExposure assessment\nThe range of the dietary exposure estimates for iAs was assessed as\n\n0.03 - 0.15 μg/kg bw per day for an average adult consumer and\n0.07 - 0.33 μg/kg bw per day for a high-level adult consumers in Europe.\n\nLet us denote an estimate of a high exposure as \\(HE\\).\n\n\nRisk characterisation\nA Margin of Exposure (MOE) is defined as \\[MOE=\\frac{RP}{HE}\\]\nIn the risk assessment on iAs the derived MOEs range between\n\n2.1 - 0.4 for average adult and\n0.9 - 0.2 for high level adult consumers, respectively.\n\nA MOE of 1 describes the exposure level that could be associated with a 5% increase relative to the background incidence for skin cancer. (Note that the value 5% comes from the BMD modelling).\nA MOE greater than 1, implies that a High Exposure is not exceeding the Reference Point, and thereby a low risk.\nRisk managers set what is an acceptable MOE. It can e.g. be 1, 10 or 1000.\nThe purpose of the assessment is to conclude if the MOE is above the threshold for the acceptable MOE. Here this threshold was set to 1."
  },
  {
    "objectID": "ex/ex_uncertainty_analysis.html#uncertainty-analysis",
    "href": "ex/ex_uncertainty_analysis.html#uncertainty-analysis",
    "title": "Exercise 19. Food safety assessment with uncertainty analysis",
    "section": "Uncertainty analysis",
    "text": "Uncertainty analysis\nThe purpose of the uncertainty analysis is to identify sources of uncertainty and evaluate their combined impact on the answer to the overall assessment question.\n\nOutcome of interest\nIn this case it the outcome of interest is if the MOE is above the threshold of 1.\nA quantitative uncertainty analysis can be specified to result in an expression of the expert’s certainty in the outcome of interest as a % probability, i.e. \\(P(MOE &gt; 1)\\).\nThe risk managers has informed the risk assessors that they find a certainty of 90% or more as acceptable for decision making.\n\nConfirm to yourself that the following expressions are the same\n\n\n\\(P(MOE &gt; 1)\\)\n\\(P(HE &lt; RP)\\)\n\\(P(\\frac{HE}{RP}&lt; 1)\\)\n\n\n\nIdentify sources of uncertainty\nSources of uncertainty have been identified related to inputs and methods in the parts of the assessment, i.e. related to:\n\nChemical characterization and analytical methods\nHazard identification and characterisation\nExposure assessment\nRisk characterisation\n\nFurther information about these sources can be found in the drafted assessment.\n\n\nEvaluate the combined impact of sources of uncertainty on the outcome of interest\nThere are several options to evaluate the combined impact of all these sources of uncertainty on the conclusion whether the MOE is greater than 1.\nA tiered uncertainty analysis is done in several steps steps:\n\nBasic option\nThe basic option for uncertainty analysis is to assess the combined impact in a single step using expert judgement. This is done by asking the experts to agree how certain they are as a group that the MOE is greater than 1, given that all identified (non-standard or standard) sources of uncertainty has been resolved.\nThis can be done by reaching a consensus judgement (following procedures for expert knowledge elicitation using behavioural aggregation)\nThis judgement requires expertise on both toxicity and exposure assessment.\nThis is a useful option when the Highest Exposure is far below the Reference Point.\n\n\nRefined option with probability bounds\nA refined approach is needed when there is a small difference between the Reference Point and the Highest Exposure.\nA refined option is to assess uncertainties separately before combining them by calculations and assessing overall uncertainty.\nFirst we explore how to do this with probability bounds.\nIn this example, the experts are asked to express their uncertainty in the Reference Point and in a High Exposure for adults in the EU.\nThe experts judge that they are 95% certain that the Reference Point is above 0.062 μg iAs/kg bw per day.\n\\[P(RP &gt; 0.062) = 0.95\\]\n\n\n\nP below\nvalue\nP above\n\n\n\n\n5%\n0.062\n95%\n\n\n\nThe experts then judge that they are 90% certain that a High Exposure is below 0.33 μg iAs/kg bw per day.\n\\[P( HE &lt; 0.33) = 0.90\\]\n\n\n\nP below\nvalue\nP above\n\n\n\n\n90%\n0.33\n10%\n\n\n\nThen we use probability bounds analysis (and the rule of “lost probabilities” as described in the third video) to calculate that it is at least \\[ 100 - [(100-95)+(100-90)]=85\\%\\] probable that the ratio of the highest estimate of High Exposure to the Reference Point is less than or equal than 0.33/0.062 = 5.32\n\\[P(\\frac{HE}{RP} &lt; 5.32) \\geq 85\\%\\]\n\n\n\nP below\nvalue\nP above\n\n\n\n\n\\(\\leq 15\\%\\)\n5.32\n\\(\\geq 85\\%\\)\n\n\n\n\nGo through the calculations for the lost probability rule and try to understand what it does.\n\nUnfortunately, this result is not enough to reach a conclusion whether a High Exposure is below the Reference Point (i.e. that the MOE is greater than 1). See Post scriptum 3 for how the refined option with probability bounds can be designed to cosnider what level of certainty that is required.\nTherefore we move on to a refined option for uncertainty analysis with probability distributions.\n\n\nRefined option with probability distributions\nA refined option can alternatively be done with with probability distributions which are combined by Monte Carlo simulation.\nThis is an option, when the outcome of a probability bound analysis was not conclusive.\nIt can also be an option when the assessors prefers to choose to justify probability distributions for uncertainty from available information, combine uncertainties by performing the probability calculations, and then ask the experts to make a judgement on the outcome of interest directly. Note that an explicit elicitation of the outcome of interest following the Monte Carlo simulation is a condition for this approach to result in a characterisation of the experts uncertainty considering the combined impact of all sources of uncertainty in the assessment.\nA probability distribution representing uncertainty in the Reference Point can be taken from the BMD uncertainty distribution generated by bootstrapping or Bayesian modelling.\nHere is an example of what it looks like when using the EFSA BMD software:\n\nWe let the BMDL, the BMD, and the BMDU define the 5th percentile (P05), the median (P50), and the 95th percentile (P95) of the probability distribution for uncertainty in the RP.\n\n\n\nBMDL\nBMD\nBMDU\n\n\n\n\nP05\nP50\nP95\n\n\n0.062\n0.147\n0.21\n\n\n\n\nGo to the online elicitation app for one distribution link to SHELF app and find a probability distribution that matches these quantiles. Note that you have to specify the lower and upper bound of the plausible range. You can get them from studying the previous graph from the EFSA BMD software.\nYou can find the R-code to generate random numbers from the selected probability distribution in the report that is possible to download from SHELF\n\n\nAs an example, the remaining instructions will use the distribution selected by me. You can replace the code with your own distribution or use mine. I used 0 and 0.23 as the plausible range and selected a generalised beta-distribution over the interval 0 and 0.23 for which random draws can be generated by\n\n0.23 * rbeta(n = 10, shape1 = 3.15, shape2 = 1.92)\n\n [1] 0.17222618 0.14032722 0.07645197 0.15046437 0.18267979 0.19103709\n [7] 0.12796682 0.15876219 0.15831355 0.18732431\n\n\nThis beta-distribution is defined as\n\\(\\frac{RP}{0.23} \\sim beta(3.15, 1.92)\\)\n\nIf you have a different distribution, specify it with a similar format RP ~ … (ask for help)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate 10 000 random numbers from the probability distribution for the RP and save it into an object named rp.\n\nLet us visualise the sample from the distribution in a histogram.\n\n\n\n\n\nA probability distribution representing uncertainty in the estimates of High Exposure over different surveys can be specified by assuming that the range of the dietary exposure estimates corresponds to the bounds of an 80% probability interval.\n\n\n\nmin HE\nmax HE\n\n\n\n\nP10\nP90\n\n\n0.07\n0.33\n\n\n\nOne choice could be to use a normal distribution to represent uncertainty in a High Exposure. A normal distribution has two parameters, the mean (\\(\\mu\\)) and the standard deviation (\\(\\sigma\\)).\n\\[HE \\sim N(\\mu,\\sigma)\\]\nWe derive the mean by taking the mid value in the range\n\\[ \\mu = \\frac{0.33+0.07}{2}= 0.2\\] The standard deviation is specified by considering the formula for a 90% quantile in the normal distributions as a function of the parameters.\n\\[ P90 = \\mu + 1.28\\cdot \\sigma\\]\nand rearranging it to be a function of \\(\\sigma\\)\n\\[ \\sigma = \\frac{P90 - \\mu}{1.28} =  \\frac{0.33-0.13}{1.28}=0.1\\]\nSpecification of the parameters and draw of random numbers from the normal distribution\n\nm = (0.07+0.33)/2\ns = (0.33-m)/qnorm(0.9)\n\nhe = rnorm(n = niter, mean = m, sd = s)\nhe[he&lt;0] = 0 # truncate\n\n\nAt the end of the above chunck we truncate the values. What does that mean? Why is a normal distribution not an ideal choice in this case?\n\nThe distributions for High Exposure and the Reference Point overlap.\n\n\n\n\n\nThe calculated probability that the MOE is above 1 is 30 %.\n\nCalculate the proportion of random samples in your Monte Carlo simulation that corresponds to a MOE &gt; 1.\n\n\nmoe = rp/he\nmean(moe&gt;1)\n\n\n\nVisualise the resulting distribution for uncertainty in MOE with your choice of distributions.\n\n\n\n\n\n\nIn my analysis, the distribution for MOE looks weird. The reason is that the probability distribution for exposure generates values that are zero or close to zero, and when we divide by that the MOE is extremely large.\nUncertainty in the ratio HE over RP (i.e. 1/MOE) offers a better visualisation of the generated distribution.\n\nConsider, revising the visualisation but for 1/MOE using the code below.\n\n\ndf &lt;- density(he/rp, from=0)\ndf &lt;- data.frame(x=df$x,y=df$y)\nggplot(data.frame(x=df$x,y=df$y),aes(x=x, y=y)) +\n  geom_line() +\n  geom_ribbon(data=subset(df,x&lt;1),  aes(x=x,ymax=y),ymin=0,fill=\"red\", alpha=0.5) +\n  xlab(\"1/MOE\") +\n  ylab(\"density\") +\n  ggtitle(\"Uncertainty in 1/MOE\", subtitle=paste0(\"P(MOE&gt;1)=\",round(mean(rp/he&gt;1)*100),\"%\"))\n\n\n\n\nThe red area corresponds to a MOE greater than 1.\nAs a final step, the experts are invited to adjust this probability upwards or downwards considering additional sources of uncertainty, not previously taken into account in the analysis.\nFor the purpose of this exercise we assume that the experts subsequently judged that any additional sources of uncertainty were small enough that there was no need to adjust the calculated % probability that the MOE is above 1.\nAn additional sources of uncertainty could e.g. be the choice of distributions used in the Monte Carlo simulation and any limitations in the quality of the exposure and toxicity data that are not taken into account in those distributions."
  },
  {
    "objectID": "ex/ex_uncertainty_analysis.html#uncertainty-in-the-conclusion",
    "href": "ex/ex_uncertainty_analysis.html#uncertainty-in-the-conclusion",
    "title": "Exercise 19. Food safety assessment with uncertainty analysis",
    "section": "Uncertainty in the conclusion",
    "text": "Uncertainty in the conclusion\nIn my analysis, the attempts to arrive at a high certainty that the MOE is acceptable (i.e. above 1) has failed. This is because the level of certainty for the outcome of interest is 30% which is not in the region of practical certainty (i.e. &lt;10% or &gt;90%).\n\n\n\nHealth concern\nInconclusive\nNo health concern\n\n\n\n\n&lt;10%\n10 - 90%\n&gt;90%\n\n\n\nAt this point, the assessment should be reported as inconclusive.\nThe risk managers can decide to support generation of new studies to fill data gaps and perform a re-evaluation of the risk assessment."
  },
  {
    "objectID": "ex/ex_uncertainty_analysis.html#post-exercise-do-this",
    "href": "ex/ex_uncertainty_analysis.html#post-exercise-do-this",
    "title": "Exercise 19. Food safety assessment with uncertainty analysis",
    "section": "Post exercise (Do this)",
    "text": "Post exercise (Do this)\nView the third tutorial video Refined options for assessing uncertainty (30 minutes)."
  },
  {
    "objectID": "ex/ex_uncertainty_analysis.html#extra-material-outside-of-the-exercise",
    "href": "ex/ex_uncertainty_analysis.html#extra-material-outside-of-the-exercise",
    "title": "Exercise 19. Food safety assessment with uncertainty analysis",
    "section": "Extra material outside of the exercise",
    "text": "Extra material outside of the exercise\n\nPost scriptum 1\nAn alternative choice for a probability distribution to represent uncertainty in a High Exposure would be a distribution that resonates properties of percentiles. to be continued\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost scriptum 2\nIn some situations, RP is the BMDL and then it is difficult to talk about RP as some other value. If so, one can introduce an uncertainty factor (UF) that considers uncertainty associated with the hazard assessment. The focus is then on \\(RP/UF\\) instead of \\(RP\\). In the description below, I use this amendment.\n\n\nPost scriptum 3\nThe refined option with probability bounds can be modified to consider the probability threshold for a “firm conclusion” (also known as the level for practical certainty). In this example, the risk managers and the experts have agreed that experts must be at least 90% certain that a High Exposure \\(HE\\) is below the Reference Point divided by an Uncertainty Factor \\(RP/UF\\) in order for risk managers to make a decision.\nThis means that the assessment is conclusive when the experts are at least 90% certain that there is a health concern or that they are at least 90% certain that there is no a health concern.\n\nSubtract the probability threshold from 100% to set an upper bound for the the sum of the lost probabilities.\n\nE.g. \\(100\\% - 90\\% = 10\\%\\)\n\nDivide this into two parts, one for exposure and one for hazard.\n\nE.g. \\(5\\%\\) and \\(5\\%\\)\n\nSubtract the lost probability for exposure from 100, to set the probability required for a conservative estimate of a High Exposure.\n\nE.g. \\(100\\% - 5\\% = 95\\%\\)\n\nElicit a conservative estimate of a High Exposure from the exposure experts (asking them to consider the identified sources of uncertainty for the exposure assessment).\n\nE.g. \\(P(HE &lt; x) = 95\\%\\)\n\nSubtract the lost probability for hazard from 100, to set the probability required for a conservative estimate of the Reference Point.\n\nE.g. \\(100\\% - 5\\% = 95\\%\\)\n\nElicit a conservative estimate of the Reference Point (or RP/UF) from the toxicology experts (asking them to consider the identified sources of uncertainty for the hazard assessment).\n\nE.g. \\(P(\\frac{RP}{UF} &lt; y) = 95\\%\\)\n\nDivide the conservative estimate for the exposure with the conservative estimate for the hazard\n\n\\(\\frac{x}{y}\\)\n\nIf the ratio is less than the threshold on MOE, then there is sufficient certainty for a firm conclusion that exposure is below the hazardous concentration.\n\nE.g. Is \\(\\frac{x}{y} &lt; 1\\)?"
  },
  {
    "objectID": "ex/observe_fit_simulate.html",
    "href": "ex/observe_fit_simulate.html",
    "title": "Observe, fit and simulate",
    "section": "",
    "text": "We encourage collaboration\nReporting is individual\n\n\n\nAssessors often face the task to inform a model with available data.\nThe way data has been collected gives valuable information on how data is related to the quantity\nSample size, or rather the number of independent observations, should be large enough to establish a good fit.\n\n\n\n\nTo find a suitable model (a probability distribution) for variability or measurement errors of a given random sample,\ninform the model with the random sample, and\ndraw random numbers from the chosen probability distribution\n\n\n\n\n\nData set to choose between\nFunctions to fit a model to data\n\n\n\n\n60 minutes\n\n\n\nWrite a report using a qmd document and upload it on the assignment in canvas. Instructions at the end of this page."
  },
  {
    "objectID": "ex/observe_fit_simulate.html#exercise-overview",
    "href": "ex/observe_fit_simulate.html#exercise-overview",
    "title": "Observe, fit and simulate",
    "section": "",
    "text": "We encourage collaboration\nReporting is individual\n\n\n\nAssessors often face the task to inform a model with available data.\nThe way data has been collected gives valuable information on how data is related to the quantity\nSample size, or rather the number of independent observations, should be large enough to establish a good fit.\n\n\n\n\nTo find a suitable model (a probability distribution) for variability or measurement errors of a given random sample,\ninform the model with the random sample, and\ndraw random numbers from the chosen probability distribution\n\n\n\n\n\nData set to choose between\nFunctions to fit a model to data\n\n\n\n\n60 minutes\n\n\n\nWrite a report using a qmd document and upload it on the assignment in canvas. Instructions at the end of this page."
  },
  {
    "objectID": "ex/observe_fit_simulate.html#choose-data-set",
    "href": "ex/observe_fit_simulate.html#choose-data-set",
    "title": "Observe, fit and simulate",
    "section": "Choose data set",
    "text": "Choose data set\n\nChoose a data set collected in the previous exercise. You can continue with the one you worked with before or take another one. Download the data and upload it in your project in the folder named data\n\n\n\n\n lengths of blocks 1\n\n\n\n\n\n\n lengths of blocks 2\n\n\n\n\n\n\n weights of apples\n\n\n\n\n\n\n time betwen cars\n\n\n\n\n\n\n cars passing 1\n\n\n\n\n\n\n cars passing 2\n\n\n\n\n\n\n cars passing 3\n\n\n\n\n\n\n cars passing 4\n\n\n\n\n\n\n cars passing 5\n\n\n\n\nLoad useful libraries for reading and plotting data\n\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nView meta-data in R (replace xxx with the missing part in the chosen filename) and decide if the data is continuous or discrete.\n\n\nread_excel(path=\"data/ex7_xxx.xlsx\",sheet=\"metadata\")\n\n\n\n# A tibble: 5 × 2\n  info          specification   \n  &lt;chr&gt;         &lt;chr&gt;           \n1 collected by: Ullrika         \n2 date:         September       \n3 quantity      quantity made up\n4 data type     continuous      \n5 unit          na              \n\n\n\nRead in data into a data frame named df.\n\nThe data will be stored in the variable obs\n\ndf &lt;- read_excel(path=\"data/ex7_xxx.xlsx\",sheet=\"data\")\ndf$obs\n\n\nMake a histogram of data.\n\nChange binwidth to modify the smoothness of the histogram.\n\nggplot(df,aes(x=obs))+\n  geom_histogram(binwidth=2)"
  },
  {
    "objectID": "ex/observe_fit_simulate.html#find-a-suitable-model-for-data",
    "href": "ex/observe_fit_simulate.html#find-a-suitable-model-for-data",
    "title": "Observe, fit and simulate",
    "section": "Find a suitable model for data",
    "text": "Find a suitable model for data\nWe will find a suitable model for data by fitting a probability distribution to data.\n\nA distribution that fits well to data AND that seems reasonable for that quantity can be seen as a suitable model to describe variability for the quantity and/or measurement error for observations of the quantity.\n\n\nInstall the R-package fitdistrplus.\n\nNote that this only needs to be done one time and you can do it in the Console.\n\ninstall.packages(\"fitdistrplus\")\n\n\nLoad the R-package fitdistrplus\n\n\nlibrary(fitdistrplus)\n\n\nFit a model\nWe will find a model by fitting alternative reasonable probability distributions to data and then compare the fitted distributions.\n\nContinuous quantityDiscrete quantity\n\n\nWe will use a normal, lognormal and an exponential distribution as candidate models for the continuous quantity.\n\nFit a normal distribution to data using the function fitdist\n\n\n\n\n\n\n\nTip\n\n\n\nNote that you can type a question mark in front of a function in the Console to view the help\n\n\n\nfitnorm = fitdist(df$obs, distr = \"norm\")\n\n\nStudy goodness of fit by comparing the histogram and fitted PDF and Empirical and theoretical CDFs\n\n\nplot(fitnorm)\n\n\n\n\n\nFit data to a lognormal distribution and view the PDF, CDF and quantiles\n\n\nfitlnorm = fitdist(df$obs, distr = \"lnorm\")\nplot(fitlnorm)\n\n\n\n\n\nFit data to an exponential distribution and view the PDF, CDF and quantiles\n\n\nfitexp = fitdist(df$obs, distr = \"exp\")\nplot(fitexp)\n\n\n\n\nIn addition to studying graphs, one can compare a measures of goodness-of-fit.\n\nCompare the goodness-of-fit measure AIC, where the lower AIC is the better.\n\n\nfitnorm$aic\n\n[1] 576.1772\n\nfitlnorm$aic\n\n[1] 576.86\n\nfitexp$aic\n\n[1] 1021.93\n\n\n\n\nWe will use negative binomial, binomial and poisson as candidate distributions for the discrete quantity.\n\nFit a negative binomial distribution to data using the function fitdist\n\n\n\n\n\n\n\nTip\n\n\n\nNote that you can type a question mark in front of a function in the Console to view the help\n\n\n\nfitnbinom = fitdist(df$obs, distr = \"nbinom\", discrete=TRUE)\n\n\nplot(fitnbinom)\n\n\n\n\n\nFit a binomial distribution to data\n\nNote that in order to use the binomial you need to fix the number of trials and suggest a starting value for the probability to succeed in one trial.\nIn the code below, the binomial model is fitted using quantile matching estimation, and that is why we also provide two quantiles (probs) as input arguments to the function.\n\nfitbinom = fitdist(df$obs, distr = \"binom\", discrete=TRUE, start= list(size = 20, prob = mean(df$obs)/20), method = \"qme\", probs = c(0.25,0.75))\n\nplot(fitbinom)\n\n\n\n\n\nFit a poisson distribution to data\n\n\nfitpois = fitdist(df$obs, distr = \"pois\", discrete=TRUE)\n\nplot(fitpois)\n\n\n\n\nIn addition to studying graphs, one can compare a measures of goodness-of-fit.\n\nCompare the goodness-of-fit measure AIC, where the lower AIC is the better.\n\n\nfitnbinom$aic\n\n[1] 426.7706\n\nfitbinom$aic\n\n[1] 417.4536\n\nfitpois$aic\n\n[1] 424.7696\n\n\n\n\n\n\nChoose the best model and justify your decision"
  },
  {
    "objectID": "ex/observe_fit_simulate.html#simulate",
    "href": "ex/observe_fit_simulate.html#simulate",
    "title": "Observe, fit and simulate",
    "section": "Simulate",
    "text": "Simulate\nAt this point you have a model for a quantity that is either a continuous or a discrete probability distribution.\nNow you are to simulate this model and visualise the results. Simulation is done by generating random numbers from the chosen probability distribution.\n\nSample 10 000 random values from your choice of the best model.\n\n\nSimulate NormalSimulate Poisson\n\n\nHere is a code how to sample ten values from a fitted normal distribution.\n\nThe parameters of the fitted distribution is found by typing $estimate after the fitted object. For a normal it is:\n\n\nfitnorm$estimate\n\n     mean        sd \n60.319213  4.228923 \n\n\n\nuse the function rnorm to sample niter = 10 values\n\n\nniter = 10\n\nrnorm(niter,fitnorm$estimate[\"mean\"],fitnorm$estimate[\"sd\"])\n\n [1] 65.45302 66.21972 59.53139 57.93953 58.49201 56.60500 58.72130 60.53618\n [9] 59.56470 62.29739\n\n\n\nsample and plot as a histogram.\n\nHere is a code how to do it for the normal distribution.\n\ndata.frame(x=rnorm(niter,fitnorm$estimate[\"mean\"],fitnorm$estimate[\"sd\"])) %&gt;%\n  ggplot(aes(x=x)) +\n  geom_histogram()\n\n\n\n\n\n\nHere is a code how to sample ten values from a fitted poisson distribution.\n\nThe parameters of the fitted distribution is found by typing $estimate after the fitted object. For a poisson it is:\n\n\nfitpois$estimate\n\nlambda \n     6 \n\n\n\nuse the function rpois to sample niter = 10 values\n\n\nniter = 10\n\nrpois(niter,fitpois$estimate[\"lambda\"])\n\n [1]  8  7 11  6  9  8  4  7  7  5\n\n\n\nsample and plot as a histogram.\n\nHere is a code how to do it for the normal distribution.\n\ndata.frame(x=rpois(niter,fitpois$estimate[\"lambda\"])) %&gt;%\n  ggplot(aes(x=x)) +\n  geom_histogram()"
  },
  {
    "objectID": "ex/observe_fit_simulate.html#report",
    "href": "ex/observe_fit_simulate.html#report",
    "title": "Observe, fit and simulate",
    "section": "Report",
    "text": "Report\nPrepare a report including:\n\nyour name\nyour choice of data set\nyour chosen model\ninclude your justification\n\nrefer to if the data is for a continuous or discrete quantity\nthe goodness-of-fit measured by AIC in comparison to alternative models\na graph comparing the histogram with the PDF of the fitted model and the empirical and theoretical CDFs\n\na histogram visualising a random sample of 10 000 values from the quantity\n\nUse this template for the report\n\n\n\n template for report ex 8"
  },
  {
    "objectID": "ex/risk_classification_ROC.html",
    "href": "ex/risk_classification_ROC.html",
    "title": "Risk classification",
    "section": "",
    "text": "Work in pairs or alone.\n\n\n\nA classification model is a model that assigns cases into two or more classes. Here we focus on the problem to classify breast cancers into malignant (cancerous) or benign (non cancerous).\nThere are two types of errors\n\na benign cancer is classified as positive - this is referred to as a false positive (TP)\na malignant cancer is classified as negative - this is referred to as a false negative (FN)\n\nAll four possible outcomes of applying a binary classification model can be presented in a confusion matrix\n\nConfusion matrix\n\n\nTrue situation\nPositive prediction\nNegative prediction\n\n\n\n\nCancerous\nTrue positive (TP)\nFalse negative (FN)\n\n\nNon cancerous\nFalse positive (TP)\nTrue negative (TN)\n\n\n\nDesirable properties of a classification model are that its performance has\n\nHigh probability of correct classifications\nLow probability of both type of errors\n\nIn simple terms a binary classifier consists of\n\nan indicator (a quantity that can be a single predictor or derived from a combination of several predictors)\na cutoff for the indicator\n\nA modeler sets the cutoff of the indicator to achieve a desired performance of the model using\n\na data set with known cases and values on the predictors\na rule to trade-off the two types of errors\n\nThe Receiver Operating Curve (ROC) methodology is one way to make such trade-off of a binary classifier.\n\n\n\nThe purpose of this exercise is to\n\nbecome familiar with errors from a classification model\nwork with a classifier using the ROC methodology\ngain experience in comparing alternative classification models\nto gain more R skills\nto practice making a report using Quarto\n\n\n\n\n\nLoad data\nBuild a simple classifier and calculate frequency of different types of errors\nEvaluate the classifier using the ROC curve methodology\nBuild another classifier and compare the two models\nInstructions how to write the report\n\n\n\n\n45 minutes - partly as home work\n\n\n\nWrite a report using the template provided. Start preparing the report after you have worked through the exercise.\nAdd your name(s) to create a group under E4 in canvas and upload the report on the assignment in canvas.\n\n\n\nThe data set Binary Classification Prediction for type of Breast Cancer is downloaded from Kaggle"
  },
  {
    "objectID": "ex/risk_classification_ROC.html#exercise-overview",
    "href": "ex/risk_classification_ROC.html#exercise-overview",
    "title": "Risk classification",
    "section": "",
    "text": "Work in pairs or alone.\n\n\n\nA classification model is a model that assigns cases into two or more classes. Here we focus on the problem to classify breast cancers into malignant (cancerous) or benign (non cancerous).\nThere are two types of errors\n\na benign cancer is classified as positive - this is referred to as a false positive (TP)\na malignant cancer is classified as negative - this is referred to as a false negative (FN)\n\nAll four possible outcomes of applying a binary classification model can be presented in a confusion matrix\n\nConfusion matrix\n\n\nTrue situation\nPositive prediction\nNegative prediction\n\n\n\n\nCancerous\nTrue positive (TP)\nFalse negative (FN)\n\n\nNon cancerous\nFalse positive (TP)\nTrue negative (TN)\n\n\n\nDesirable properties of a classification model are that its performance has\n\nHigh probability of correct classifications\nLow probability of both type of errors\n\nIn simple terms a binary classifier consists of\n\nan indicator (a quantity that can be a single predictor or derived from a combination of several predictors)\na cutoff for the indicator\n\nA modeler sets the cutoff of the indicator to achieve a desired performance of the model using\n\na data set with known cases and values on the predictors\na rule to trade-off the two types of errors\n\nThe Receiver Operating Curve (ROC) methodology is one way to make such trade-off of a binary classifier.\n\n\n\nThe purpose of this exercise is to\n\nbecome familiar with errors from a classification model\nwork with a classifier using the ROC methodology\ngain experience in comparing alternative classification models\nto gain more R skills\nto practice making a report using Quarto\n\n\n\n\n\nLoad data\nBuild a simple classifier and calculate frequency of different types of errors\nEvaluate the classifier using the ROC curve methodology\nBuild another classifier and compare the two models\nInstructions how to write the report\n\n\n\n\n45 minutes - partly as home work\n\n\n\nWrite a report using the template provided. Start preparing the report after you have worked through the exercise.\nAdd your name(s) to create a group under E4 in canvas and upload the report on the assignment in canvas.\n\n\n\nThe data set Binary Classification Prediction for type of Breast Cancer is downloaded from Kaggle"
  },
  {
    "objectID": "ex/risk_classification_ROC.html#instructions-to-excercise-and-reporting",
    "href": "ex/risk_classification_ROC.html#instructions-to-excercise-and-reporting",
    "title": "Risk classification",
    "section": "Instructions to excercise and reporting",
    "text": "Instructions to excercise and reporting\nOpen an new Quarto document in R Studio cloud, save it in a folder named ex and paste in the code or modify the code following the steps in the exercise.\nWhen you have gone through all steps, it is time to prepare a report."
  },
  {
    "objectID": "ex/risk_classification_ROC.html#load-and-visualise-data",
    "href": "ex/risk_classification_ROC.html#load-and-visualise-data",
    "title": "Risk classification",
    "section": "Load and visualise data",
    "text": "Load and visualise data\n\nRead in data\n\nCreate a new folder named data in the directory of your project on R Studio cloud.\nDownload the data file and save it in the the directory of your project on R Studio cloud.\n\n\n\n\n Download csv file\n\n\n\n\nLoad the data into a data frame that you name df\n\nTo do this you load two R packages. One to read in files and one to tidy your data.\nTo view the data fram type df (done in the code below).\n\nlibrary(readr)\nlibrary(tidyr)\ndf &lt;- as_tibble(read_csv(\"../data/breast-cancer.csv\"))\ndf\n\n# A tibble: 569 × 32\n         id diagnosis radius_mean texture_mean perimeter_mean area_mean\n      &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1   842302 M                18.0         10.4          123.      1001 \n 2   842517 M                20.6         17.8          133.      1326 \n 3 84300903 M                19.7         21.2          130       1203 \n 4 84348301 M                11.4         20.4           77.6      386.\n 5 84358402 M                20.3         14.3          135.      1297 \n 6   843786 M                12.4         15.7           82.6      477.\n 7   844359 M                18.2         20.0          120.      1040 \n 8 84458202 M                13.7         20.8           90.2      578.\n 9   844981 M                13           21.8           87.5      520.\n10 84501001 M                12.5         24.0           84.0      476.\n# ℹ 559 more rows\n# ℹ 26 more variables: smoothness_mean &lt;dbl&gt;, compactness_mean &lt;dbl&gt;,\n#   concavity_mean &lt;dbl&gt;, `concave points_mean` &lt;dbl&gt;, symmetry_mean &lt;dbl&gt;,\n#   fractal_dimension_mean &lt;dbl&gt;, radius_se &lt;dbl&gt;, texture_se &lt;dbl&gt;,\n#   perimeter_se &lt;dbl&gt;, area_se &lt;dbl&gt;, smoothness_se &lt;dbl&gt;,\n#   compactness_se &lt;dbl&gt;, concavity_se &lt;dbl&gt;, `concave points_se` &lt;dbl&gt;,\n#   symmetry_se &lt;dbl&gt;, fractal_dimension_se &lt;dbl&gt;, radius_worst &lt;dbl&gt;, …\n\n\n\nNarrow down the data set to include two predictors for the classifier: mean radius and mean compactness of the cancer.\n\nThe R-package dplyr has useful functions for wrangling data. The %&gt;% is called a pipe that makes it possible to add functions to functions.\n\nlibrary(dplyr)\ndf %&gt;% select(c(diagnosis, radius_mean, compactness_mean))\n\n# A tibble: 569 × 3\n   diagnosis radius_mean compactness_mean\n   &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n 1 M                18.0           0.278 \n 2 M                20.6           0.0786\n 3 M                19.7           0.160 \n 4 M                11.4           0.284 \n 5 M                20.3           0.133 \n 6 M                12.4           0.17  \n 7 M                18.2           0.109 \n 8 M                13.7           0.164 \n 9 M                13             0.193 \n10 M                12.5           0.240 \n# ℹ 559 more rows\n\n\n\n\nVisualise predictors\nIt is always good to look at data to get a feel for it. Is it continuous, categorical or discrete numbers? What is the range of values.\nSince we will use the predictors to classify, it can be useful to summarise the values of the predictors after dividing the data according to the diagnosis. If it is a good predictor, we expect the data to look different between the groups.\n\nVisualise the predictors per diagnose group.\n\nBelow we use density graphs, which can be thought of as a smooth histogram.\n\nlibrary(ggplot2)\ndf %&gt;% select(c(diagnosis, radius_mean, compactness_mean)) %&gt;% \nggplot(aes(x=radius_mean, fill=diagnosis)) +\n  geom_density(alpha=0.5)\n\n\n\n\n\ndf %&gt;% select(c(diagnosis, radius_mean, compactness_mean)) %&gt;% \nggplot(aes(x=compactness_mean, fill=diagnosis)) +\n  geom_density(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\nInformation to put in the report\n\n\n\nWhich of the two predictors do you think is the better indicator to classify cancers into malignant or benign? Motivate your choice based on the two visualisations plots.\nInclude the plots in the report.\n\n\nIn this exercise you will build two models, one per predictor and then compare them."
  },
  {
    "objectID": "ex/risk_classification_ROC.html#model-with-mean-radius-as-predictor",
    "href": "ex/risk_classification_ROC.html#model-with-mean-radius-as-predictor",
    "title": "Risk classification",
    "section": "Model with mean radius as predictor",
    "text": "Model with mean radius as predictor\n\nSelect a cutoff of 15 for the predictor radius_mean and classify data into positive if the predictor is above the cutoff and negative if the predictor is below the cutoff. Save the predictions as pred_rad.\n\nIn the code this is done by the functions mutate and case_when.\n\ndf %&gt;% select(c(diagnosis, radius_mean, compactness_mean)) %&gt;%\n  mutate(pred_rad=case_when(radius_mean&gt;15 ~ \"pos\", .default = 'neg'))\n\n# A tibble: 569 × 4\n   diagnosis radius_mean compactness_mean pred_rad\n   &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n 1 M                18.0           0.278  pos     \n 2 M                20.6           0.0786 pos     \n 3 M                19.7           0.160  pos     \n 4 M                11.4           0.284  neg     \n 5 M                20.3           0.133  pos     \n 6 M                12.4           0.17   neg     \n 7 M                18.2           0.109  pos     \n 8 M                13.7           0.164  neg     \n 9 M                13             0.193  neg     \n10 M                12.5           0.240  neg     \n# ℹ 559 more rows\n\n\n\nDerive the frequency of the confusion matrix for the classification model with the mean radius as predictor.\n\nThe code below selects the two variables and counts the frequency of each combination of the two variables using the function table.\n\n\n\n\n\n\nTip\n\n\n\nTo read the help text for a function type a ? followed by the name of the function in the Console. Or put your cursor on top of the function and press F1.\n\n\n\ntable(df %&gt;% select(c(diagnosis, radius_mean, compactness_mean)) %&gt;%\n  mutate(pred_rad=case_when(radius_mean&gt;15 ~ \"pos\", .default = 'neg')) %&gt;% \n  select(c(diagnosis,pred_rad)))\n\n         pred_rad\ndiagnosis neg pos\n        B 345  12\n        M  51 161\n\n\nNote that the output from table is here in the reversed order compared to the standard format of a confusion matrix. The reason is that the categories are ordered in alphabetic order.\n\n\n\n\n\n\nInformation to add in the report\n\n\n\nFor the binary classification model using mean radius as predictor:\n\nWhat is the frequency of False positives (FP)?\nWhat is the frequency of False negatives (FN)?\nWhich of these two errors do you think is worse? Motivate your answer.\n\n\n\n\nSensitivity and specificty to measure performance of a binary classifier\nSensitivity is the fraction of true positives, i.e. \\(\\frac{TP}{TP + FN}\\) and describes the proportion of malignant cancers correctly predicted as positive.\nSpecificity is the fraction of true negatives, i.e. \\(\\frac{TN}{FP + TN}\\) and describes the proportion of benign cancers correctly predicted as negative.\n\n\n\n\n\n\nInformation to add in the report\n\n\n\nFor the binary classification model using mean radius as predictor:\n\nWhat is the frequency of True Positives (TP)?\nWhat is the frequency of True Negatives (TN)?\nWhat is the sensitivity and specificity?\n\n\n\nSensitivity and specificity measures performance of the classification model. Sensitivity and specificity should be as high as possible, but increasing one will decrease the other.\nThe model can be tuned towards better performance by changing the cutoff value. See for example what happens when the cutoff is changed to 11.\n\ntable(df %&gt;% select(c(diagnosis, radius_mean, compactness_mean)) %&gt;%\n  mutate(pred_rad=case_when(radius_mean&gt;11 ~ \"pos\", .default = 'neg')) %&gt;% \n  select(c(diagnosis,pred_rad)))\n\n         pred_rad\ndiagnosis neg pos\n        B  84 273\n        M   1 211\n\n\nAll but one malignant cancer is classified as positive, which is good, but it comes with a cost of classifying 272 benign cancers as positive.\nIn other words, when changing the cutoff from 15 to 11 the\n\nsensitivity is \\(\\frac{211}{211+1}=0.95\\) and\nspecificity is \\(\\frac{84}{273+84}=0.24\\)\n\n\n\nThe ROC curve methodology\nA ROC curve is a plot of sensitivity versus 1-specificity for all values of the cutoff. It can illustrate how well the model performs and help choosing the cutoff.\n\nLoad one of the many R-packages for ROC curve analysis.\n\nBefore loading the library you might have to install it using install.packages(“pROC”). This only needs to be done ones.\nThe ROC curve analysis is run using the functions roc and coords\n\nlibrary(pROC)\n\nWarning: package 'pROC' was built under R version 4.3.1\n\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\nr = roc(diagnosis ~ radius_mean, data=df)\n\nSetting levels: control = B, case = M\n\n\nSetting direction: controls &lt; cases\n\ncoords(r)\n\n    threshold specificity sensitivity\n1        -Inf 0.000000000 1.000000000\n2      7.3360 0.002801120 1.000000000\n3      7.7100 0.005602241 1.000000000\n4      7.7445 0.008403361 1.000000000\n5      7.9780 0.011204482 1.000000000\n6      8.2075 0.014005602 1.000000000\n7      8.3950 0.016806723 1.000000000\n8      8.5840 0.019607843 1.000000000\n9      8.5975 0.022408964 1.000000000\n10     8.6080 0.025210084 1.000000000\n11     8.6445 0.028011204 1.000000000\n12     8.6985 0.030812325 1.000000000\n13     8.7300 0.033613445 1.000000000\n14     8.8060 0.036414566 1.000000000\n15     8.8830 0.039215686 1.000000000\n16     8.9190 0.042016807 1.000000000\n17     8.9750 0.044817927 1.000000000\n18     9.0145 0.047619048 1.000000000\n19     9.0355 0.050420168 1.000000000\n20     9.1075 0.053221289 1.000000000\n21     9.2205 0.056022409 1.000000000\n22     9.2815 0.058823529 1.000000000\n23     9.3140 0.061624650 1.000000000\n24     9.3650 0.064425770 1.000000000\n25     9.4010 0.067226891 1.000000000\n26     9.4140 0.070028011 1.000000000\n27     9.4295 0.072829132 1.000000000\n28     9.4505 0.075630252 1.000000000\n29     9.4845 0.078431373 1.000000000\n30     9.5355 0.081232493 1.000000000\n31     9.5865 0.084033613 1.000000000\n32     9.6365 0.086834734 1.000000000\n33     9.6675 0.089635854 1.000000000\n34     9.6720 0.092436975 1.000000000\n35     9.6795 0.095238095 1.000000000\n36     9.7015 0.098039216 1.000000000\n37     9.7255 0.100840336 1.000000000\n38     9.7345 0.103641457 1.000000000\n39     9.7400 0.106442577 1.000000000\n40     9.7485 0.112044818 1.000000000\n41     9.7660 0.114845938 1.000000000\n42     9.7820 0.117647059 1.000000000\n43     9.8170 0.120448179 1.000000000\n44     9.8615 0.123249300 1.000000000\n45     9.8900 0.128851541 1.000000000\n46     9.9670 0.131652661 1.000000000\n47    10.0400 0.134453782 1.000000000\n48    10.0650 0.137254902 1.000000000\n49    10.1200 0.140056022 1.000000000\n50    10.1650 0.142857143 1.000000000\n51    10.1750 0.145658263 1.000000000\n52    10.1900 0.148459384 1.000000000\n53    10.2250 0.151260504 1.000000000\n54    10.2550 0.154061625 1.000000000\n55    10.2750 0.162464986 1.000000000\n56    10.3050 0.165266106 1.000000000\n57    10.3800 0.168067227 1.000000000\n58    10.4600 0.170868347 1.000000000\n59    10.4850 0.176470588 1.000000000\n60    10.5000 0.182072829 1.000000000\n61    10.5400 0.187675070 1.000000000\n62    10.5850 0.193277311 1.000000000\n63    10.6250 0.196078431 1.000000000\n64    10.6550 0.198879552 1.000000000\n65    10.6850 0.201680672 1.000000000\n66    10.7300 0.204481793 1.000000000\n67    10.7750 0.207282913 1.000000000\n68    10.8100 0.212885154 1.000000000\n69    10.8400 0.215686275 1.000000000\n70    10.8700 0.218487395 1.000000000\n71    10.8900 0.221288515 1.000000000\n72    10.9050 0.224089636 1.000000000\n73    10.9250 0.226890756 1.000000000\n74    10.9450 0.229691877 1.000000000\n75    10.9550 0.229691877 0.995283019\n76    10.9650 0.232492997 0.995283019\n77    11.0050 0.235294118 0.995283019\n78    11.0500 0.240896359 0.995283019\n79    11.0700 0.249299720 0.995283019\n80    11.1050 0.252100840 0.990566038\n81    11.1350 0.257703081 0.990566038\n82    11.1450 0.260504202 0.990566038\n83    11.1550 0.263305322 0.990566038\n84    11.1800 0.266106443 0.990566038\n85    11.2100 0.268907563 0.990566038\n86    11.2350 0.274509804 0.990566038\n87    11.2550 0.277310924 0.990566038\n88    11.2650 0.282913165 0.990566038\n89    11.2750 0.288515406 0.990566038\n90    11.2850 0.291316527 0.990566038\n91    11.2950 0.294117647 0.990566038\n92    11.3050 0.296918768 0.990566038\n93    11.3150 0.299719888 0.990566038\n94    11.3250 0.302521008 0.990566038\n95    11.3350 0.305322129 0.990566038\n96    11.3500 0.310924370 0.990566038\n97    11.3650 0.313725490 0.990566038\n98    11.3900 0.316526611 0.990566038\n99    11.4150 0.322128852 0.990566038\n100   11.4250 0.322128852 0.985849057\n101   11.4400 0.327731092 0.985849057\n102   11.4550 0.330532213 0.985849057\n103   11.4650 0.333333333 0.985849057\n104   11.4800 0.336134454 0.985849057\n105   11.4950 0.338935574 0.985849057\n106   11.5050 0.341736695 0.985849057\n107   11.5150 0.344537815 0.985849057\n108   11.5300 0.350140056 0.985849057\n109   11.5550 0.355742297 0.985849057\n110   11.5850 0.358543417 0.985849057\n111   11.6050 0.366946779 0.985849057\n112   11.6150 0.369747899 0.985849057\n113   11.6250 0.372549020 0.985849057\n114   11.6350 0.375350140 0.985849057\n115   11.6500 0.378151261 0.985849057\n116   11.6650 0.380952381 0.985849057\n117   11.6750 0.383753501 0.985849057\n118   11.6850 0.386554622 0.985849057\n119   11.6950 0.389355742 0.985849057\n120   11.7050 0.392156863 0.985849057\n121   11.7250 0.400560224 0.985849057\n122   11.7450 0.406162465 0.985849057\n123   11.7550 0.411764706 0.985849057\n124   11.7800 0.414565826 0.981132075\n125   11.8050 0.417366947 0.976415094\n126   11.8250 0.420168067 0.976415094\n127   11.8450 0.422969188 0.971698113\n128   11.8600 0.425770308 0.971698113\n129   11.8800 0.428571429 0.971698113\n130   11.8950 0.436974790 0.971698113\n131   11.9150 0.439775910 0.971698113\n132   11.9350 0.445378151 0.971698113\n133   11.9450 0.450980392 0.971698113\n134   11.9700 0.453781513 0.971698113\n135   11.9950 0.456582633 0.971698113\n136   12.0150 0.462184874 0.971698113\n137   12.0350 0.464985994 0.971698113\n138   12.0450 0.467787115 0.971698113\n139   12.0550 0.473389356 0.971698113\n140   12.0650 0.478991597 0.971698113\n141   12.0850 0.481792717 0.971698113\n142   12.1300 0.484593838 0.971698113\n143   12.1700 0.487394958 0.971698113\n144   12.1850 0.495798319 0.971698113\n145   12.1950 0.498599440 0.971698113\n146   12.2050 0.501400560 0.971698113\n147   12.2150 0.507002801 0.971698113\n148   12.2250 0.509803922 0.971698113\n149   12.2400 0.512605042 0.971698113\n150   12.2600 0.518207283 0.971698113\n151   12.2850 0.523809524 0.971698113\n152   12.3050 0.529411765 0.971698113\n153   12.3150 0.532212885 0.971698113\n154   12.3300 0.535014006 0.971698113\n155   12.3500 0.543417367 0.966981132\n156   12.3750 0.549019608 0.966981132\n157   12.3950 0.551820728 0.966981132\n158   12.4100 0.554621849 0.966981132\n159   12.4250 0.557422969 0.966981132\n160   12.4400 0.560224090 0.966981132\n161   12.4550 0.563025210 0.962264151\n162   12.4650 0.568627451 0.957547170\n163   12.4800 0.574229692 0.957547170\n164   12.5150 0.577030812 0.957547170\n165   12.5500 0.582633053 0.957547170\n166   12.5700 0.585434174 0.957547170\n167   12.6000 0.588235294 0.957547170\n168   12.6250 0.593837535 0.957547170\n169   12.6400 0.596638655 0.957547170\n170   12.6600 0.599439776 0.957547170\n171   12.6750 0.602240896 0.957547170\n172   12.6900 0.602240896 0.952830189\n173   12.7100 0.605042017 0.952830189\n174   12.7350 0.610644258 0.952830189\n175   12.7550 0.613445378 0.952830189\n176   12.7650 0.619047619 0.952830189\n177   12.7750 0.624649860 0.948113208\n178   12.7900 0.627450980 0.948113208\n179   12.8050 0.630252101 0.948113208\n180   12.8200 0.633053221 0.948113208\n181   12.8400 0.635854342 0.943396226\n182   12.8550 0.638655462 0.943396226\n183   12.8650 0.644257703 0.943396226\n184   12.8750 0.649859944 0.943396226\n185   12.8850 0.655462185 0.943396226\n186   12.8950 0.663865546 0.943396226\n187   12.9050 0.666666667 0.943396226\n188   12.9250 0.669467787 0.943396226\n189   12.9450 0.672268908 0.943396226\n190   12.9550 0.675070028 0.943396226\n191   12.9700 0.677871148 0.943396226\n192   12.9850 0.680672269 0.943396226\n193   12.9950 0.683473389 0.943396226\n194   13.0050 0.689075630 0.938679245\n195   13.0200 0.691876751 0.938679245\n196   13.0400 0.694677871 0.938679245\n197   13.0650 0.703081232 0.938679245\n198   13.0950 0.705882353 0.938679245\n199   13.1250 0.708683473 0.933962264\n200   13.1450 0.711484594 0.933962264\n201   13.1550 0.714285714 0.933962264\n202   13.1650 0.717086835 0.933962264\n203   13.1850 0.719887955 0.924528302\n204   13.2050 0.725490196 0.924528302\n205   13.2250 0.731092437 0.924528302\n206   13.2550 0.733893557 0.924528302\n207   13.2750 0.739495798 0.924528302\n208   13.2900 0.742296919 0.919811321\n209   13.3200 0.745098039 0.919811321\n210   13.3550 0.747899160 0.919811321\n211   13.3750 0.750700280 0.919811321\n212   13.3900 0.753501401 0.919811321\n213   13.4150 0.756302521 0.915094340\n214   13.4350 0.756302521 0.910377358\n215   13.4450 0.756302521 0.905660377\n216   13.4550 0.759103641 0.905660377\n217   13.4650 0.764705882 0.905660377\n218   13.4750 0.767507003 0.905660377\n219   13.4850 0.767507003 0.900943396\n220   13.4950 0.770308123 0.900943396\n221   13.5050 0.773109244 0.900943396\n222   13.5200 0.775910364 0.900943396\n223   13.5350 0.778711485 0.900943396\n224   13.5500 0.781512605 0.900943396\n225   13.5750 0.784313725 0.900943396\n226   13.6000 0.789915966 0.900943396\n227   13.6150 0.789915966 0.891509434\n228   13.6300 0.792717087 0.891509434\n229   13.6450 0.798319328 0.891509434\n230   13.6550 0.801120448 0.891509434\n231   13.6700 0.806722689 0.891509434\n232   13.6850 0.809523810 0.891509434\n233   13.6950 0.812324930 0.891509434\n234   13.7050 0.815126050 0.891509434\n235   13.7200 0.817927171 0.886792453\n236   13.7350 0.817927171 0.882075472\n237   13.7450 0.820728291 0.882075472\n238   13.7600 0.823529412 0.882075472\n239   13.7750 0.826330532 0.877358491\n240   13.7900 0.829131653 0.877358491\n241   13.8050 0.829131653 0.872641509\n242   13.8150 0.829131653 0.867924528\n243   13.8350 0.829131653 0.863207547\n244   13.8550 0.837535014 0.863207547\n245   13.8650 0.837535014 0.858490566\n246   13.8750 0.843137255 0.858490566\n247   13.8900 0.845938375 0.858490566\n248   13.9200 0.851540616 0.858490566\n249   13.9500 0.854341737 0.858490566\n250   13.9700 0.854341737 0.853773585\n251   14.0000 0.854341737 0.849056604\n252   14.0250 0.857142857 0.849056604\n253   14.0350 0.859943978 0.849056604\n254   14.0450 0.862745098 0.849056604\n255   14.0550 0.865546218 0.849056604\n256   14.0850 0.868347339 0.849056604\n257   14.1500 0.871148459 0.849056604\n258   14.1950 0.871148459 0.844339623\n259   14.2100 0.873949580 0.844339623\n260   14.2350 0.876750700 0.839622642\n261   14.2550 0.876750700 0.830188679\n262   14.2650 0.882352941 0.830188679\n263   14.2800 0.882352941 0.825471698\n264   14.3150 0.885154062 0.825471698\n265   14.3700 0.887955182 0.825471698\n266   14.4050 0.890756303 0.825471698\n267   14.4150 0.893557423 0.825471698\n268   14.4300 0.896358543 0.820754717\n269   14.4450 0.899159664 0.820754717\n270   14.4600 0.899159664 0.816037736\n271   14.4750 0.901960784 0.816037736\n272   14.4900 0.901960784 0.811320755\n273   14.5150 0.904761905 0.811320755\n274   14.5350 0.910364146 0.811320755\n275   14.5600 0.910364146 0.806603774\n276   14.5850 0.913165266 0.801886792\n277   14.5950 0.915966387 0.801886792\n278   14.6050 0.915966387 0.797169811\n279   14.6150 0.918767507 0.797169811\n280   14.6300 0.921568627 0.797169811\n281   14.6600 0.927170868 0.797169811\n282   14.6850 0.927170868 0.792452830\n283   14.7000 0.929971989 0.792452830\n284   14.7250 0.929971989 0.787735849\n285   14.7500 0.932773109 0.787735849\n286   14.7700 0.935574230 0.787735849\n287   14.7900 0.935574230 0.783018868\n288   14.8050 0.938375350 0.783018868\n289   14.8350 0.941176471 0.783018868\n290   14.8650 0.943977591 0.778301887\n291   14.8850 0.946778711 0.773584906\n292   14.9100 0.946778711 0.768867925\n293   14.9350 0.949579832 0.768867925\n294   14.9550 0.952380952 0.764150943\n295   14.9650 0.955182073 0.764150943\n296   14.9800 0.960784314 0.764150943\n297   14.9950 0.963585434 0.759433962\n298   15.0200 0.966386555 0.759433962\n299   15.0450 0.969187675 0.759433962\n300   15.0550 0.969187675 0.754716981\n301   15.0700 0.969187675 0.750000000\n302   15.0900 0.969187675 0.745283019\n303   15.1100 0.971988796 0.740566038\n304   15.1250 0.971988796 0.735849057\n305   15.1600 0.971988796 0.731132075\n306   15.2050 0.974789916 0.731132075\n307   15.2450 0.974789916 0.726415094\n308   15.2750 0.977591036 0.726415094\n309   15.2900 0.977591036 0.721698113\n310   15.3100 0.977591036 0.716981132\n311   15.3300 0.977591036 0.712264151\n312   15.3550 0.977591036 0.707547170\n313   15.4150 0.977591036 0.702830189\n314   15.4750 0.977591036 0.688679245\n315   15.4950 0.977591036 0.683962264\n316   15.5150 0.977591036 0.679245283\n317   15.5700 0.977591036 0.674528302\n318   15.6350 0.977591036 0.669811321\n319   15.6800 0.977591036 0.665094340\n320   15.7050 0.977591036 0.660377358\n321   15.7200 0.980392157 0.660377358\n322   15.7400 0.983193277 0.660377358\n323   15.7650 0.983193277 0.650943396\n324   15.8150 0.983193277 0.641509434\n325   15.9350 0.983193277 0.636792453\n326   16.0250 0.983193277 0.632075472\n327   16.0500 0.983193277 0.627358491\n328   16.0900 0.983193277 0.622641509\n329   16.1200 0.983193277 0.617924528\n330   16.1350 0.983193277 0.608490566\n331   16.1500 0.985994398 0.608490566\n332   16.1650 0.985994398 0.603773585\n333   16.2050 0.988795518 0.603773585\n334   16.2450 0.988795518 0.599056604\n335   16.2550 0.988795518 0.594339623\n336   16.2650 0.988795518 0.589622642\n337   16.2850 0.988795518 0.584905660\n338   16.3250 0.991596639 0.584905660\n339   16.4050 0.991596639 0.580188679\n340   16.4800 0.991596639 0.575471698\n341   16.5500 0.994397759 0.575471698\n342   16.6250 0.994397759 0.570754717\n343   16.6700 0.994397759 0.566037736\n344   16.7150 0.994397759 0.561320755\n345   16.7600 0.994397759 0.556603774\n346   16.8100 0.994397759 0.551886792\n347   16.9250 0.997198880 0.551886792\n348   17.0150 0.997198880 0.547169811\n349   17.0350 0.997198880 0.542452830\n350   17.0550 0.997198880 0.537735849\n351   17.0700 0.997198880 0.533018868\n352   17.1100 0.997198880 0.528301887\n353   17.1650 0.997198880 0.523584906\n354   17.1950 0.997198880 0.518867925\n355   17.2350 0.997198880 0.514150943\n356   17.2800 0.997198880 0.509433962\n357   17.2950 0.997198880 0.504716981\n358   17.3250 0.997198880 0.500000000\n359   17.3850 0.997198880 0.495283019\n360   17.4400 0.997198880 0.490566038\n361   17.4650 0.997198880 0.485849057\n362   17.5050 0.997198880 0.481132075\n363   17.5550 0.997198880 0.476415094\n364   17.5850 0.997198880 0.471698113\n365   17.6400 0.997198880 0.466981132\n366   17.7150 0.997198880 0.462264151\n367   17.8000 0.997198880 0.457547170\n368   17.8800 1.000000000 0.457547170\n369   17.9200 1.000000000 0.452830189\n370   17.9400 1.000000000 0.448113208\n371   17.9700 1.000000000 0.443396226\n372   18.0000 1.000000000 0.433962264\n373   18.0200 1.000000000 0.429245283\n374   18.0400 1.000000000 0.424528302\n375   18.0650 1.000000000 0.419811321\n376   18.1500 1.000000000 0.415094340\n377   18.2350 1.000000000 0.405660377\n378   18.2800 1.000000000 0.400943396\n379   18.3800 1.000000000 0.391509434\n380   18.4550 1.000000000 0.386792453\n381   18.4750 1.000000000 0.382075472\n382   18.5500 1.000000000 0.377358491\n383   18.6200 1.000000000 0.372641509\n384   18.6400 1.000000000 0.367924528\n385   18.6550 1.000000000 0.363207547\n386   18.7150 1.000000000 0.358490566\n387   18.7900 1.000000000 0.353773585\n388   18.8150 1.000000000 0.349056604\n389   18.8800 1.000000000 0.344339623\n390   18.9700 1.000000000 0.339622642\n391   19.0100 1.000000000 0.334905660\n392   19.0450 1.000000000 0.330188679\n393   19.0850 1.000000000 0.325471698\n394   19.1300 1.000000000 0.320754717\n395   19.1650 1.000000000 0.316037736\n396   19.1750 1.000000000 0.311320755\n397   19.1850 1.000000000 0.306603774\n398   19.2000 1.000000000 0.301886792\n399   19.2400 1.000000000 0.297169811\n400   19.3350 1.000000000 0.292452830\n401   19.4200 1.000000000 0.283018868\n402   19.4450 1.000000000 0.278301887\n403   19.4900 1.000000000 0.273584906\n404   19.5400 1.000000000 0.264150943\n405   19.5700 1.000000000 0.254716981\n406   19.6350 1.000000000 0.245283019\n407   19.6850 1.000000000 0.240566038\n408   19.7100 1.000000000 0.235849057\n409   19.7600 1.000000000 0.231132075\n410   19.7950 1.000000000 0.226415094\n411   19.8050 1.000000000 0.221698113\n412   19.8500 1.000000000 0.216981132\n413   19.9900 1.000000000 0.212264151\n414   20.1100 1.000000000 0.207547170\n415   20.1450 1.000000000 0.202830189\n416   20.1700 1.000000000 0.198113208\n417   20.1900 1.000000000 0.188679245\n418   20.2300 1.000000000 0.183962264\n419   20.2750 1.000000000 0.179245283\n420   20.3000 1.000000000 0.174528302\n421   20.3250 1.000000000 0.169811321\n422   20.3900 1.000000000 0.165094340\n423   20.4550 1.000000000 0.160377358\n424   20.4750 1.000000000 0.155660377\n425   20.4950 1.000000000 0.150943396\n426   20.5300 1.000000000 0.146226415\n427   20.5600 1.000000000 0.141509434\n428   20.5750 1.000000000 0.136792453\n429   20.5850 1.000000000 0.132075472\n430   20.5950 1.000000000 0.127358491\n431   20.6200 1.000000000 0.122641509\n432   20.6850 1.000000000 0.117924528\n433   20.8250 1.000000000 0.113207547\n434   20.9300 1.000000000 0.108490566\n435   21.0150 1.000000000 0.103773585\n436   21.0950 1.000000000 0.099056604\n437   21.1300 1.000000000 0.094339623\n438   21.2650 1.000000000 0.089622642\n439   21.4650 1.000000000 0.084905660\n440   21.5850 1.000000000 0.080188679\n441   21.6600 1.000000000 0.075471698\n442   21.7300 1.000000000 0.070754717\n443   21.8800 1.000000000 0.066037736\n444   22.1400 1.000000000 0.061320755\n445   22.6800 1.000000000 0.056603774\n446   23.1500 1.000000000 0.051886792\n447   23.2400 1.000000000 0.047169811\n448   23.2800 1.000000000 0.042452830\n449   23.4000 1.000000000 0.037735849\n450   23.8800 1.000000000 0.033018868\n451   24.4400 1.000000000 0.028301887\n452   24.9250 1.000000000 0.023584906\n453   25.4750 1.000000000 0.018867925\n454   26.4750 1.000000000 0.014150943\n455   27.3200 1.000000000 0.009433962\n456   27.7650 1.000000000 0.004716981\n457       Inf 1.000000000 0.000000000\n\n\n\nPlot the ROC curve\n\nThe code plots sensitivity against specificity (on reversed axis) for all possible values on the cutoff.\n\nr %&gt;% ggroc +\n  ggtitle(\"ROC curve predictor radius_mean\") \n\n\n\n\n\nFind a cutoff value that offers a good balance between sensitivity and specificity.\n\n\nssc = coords(r, \"best\", best.method = \"closest.topleft\")\nssc\n\n  threshold specificity sensitivity\n1     14.15   0.8711485   0.8490566\n\n\n\nRedo the plot of the ROC curve where you also add the optimal cutoff as a red point.\n\n\nr %&gt;% ggroc +\n  ggtitle(\"ROC curve predictor radius_mean\") +\n  geom_point(data=ssc,aes(x=specificity,y =sensitivity),col='red')+\n  annotate(\"text\", x = ssc$specificity-0.2, y = ssc$sensitivity, label = paste0(\"cutoff = \", ssc$threshold))\n\n\n\n\n\nRedo the plot and also add information about the area under the curve (AUC).\n\nThe AUC measure is useful for model comparisons, where a higher value implies a better model. A value of AUC close to 0.5 corresponds to a random guess.\n\nr %&gt;% ggroc +\n  annotate(\"text\", x = 0.3, y = 0.05, label = paste0(\"AUC = \", round(auc(r), 2))) +\n  ggtitle(\"ROC curve predictor radius_mean\") +\n  geom_point(data=ssc,aes(x=specificity,y =sensitivity),col='red')+\n  annotate(\"text\", x = ssc$specificity-0.2, y = ssc$sensitivity, label = paste0(\"cutoff = \", ssc$threshold))"
  },
  {
    "objectID": "ex/risk_classification_ROC.html#compare-classification-models-using-the-roc-curves",
    "href": "ex/risk_classification_ROC.html#compare-classification-models-using-the-roc-curves",
    "title": "Risk classification",
    "section": "Compare classification models using the ROC curves",
    "text": "Compare classification models using the ROC curves\n\nDo the ROC curve analysis for the binary classification model using mean compactness as predictor.\n\n\nr2 = roc(diagnosis ~ compactness_mean, data=df)\n\nSetting levels: control = B, case = M\n\n\nSetting direction: controls &lt; cases\n\ncoords(r2,\"best\", best.method = \"closest.topleft\")\n\n  threshold specificity sensitivity\n1   0.10215   0.7815126   0.8254717\n\n\n\nWhich of the two models has the best performance evaluated by specificity and sensitivity?\nCompare the ROC curves of the models and the area under the curves.\n\n\nlist(radius=r,compactness=r2) %&gt;% ggroc +\n  annotate(\"text\", x = 0.3, y = 0.105, label = paste0(\"AUC radius = \", round(auc(r), 2))) +\n  annotate(\"text\", x = 0.3, y = 0.05, label = paste0(\"AUC compactness = \", round(auc(r2), 2))) +\n  ggtitle(\"ROC curves\")\n\n\n\n\n\n\n\n\n\n\nInformation to add in the report\n\n\n\nAdd the graph with the two ROC curves to the project.\nWhich of the two binary classification models have the best performance according to the AUC measure?\nSuggest three things that could be done to build a better classification model?"
  },
  {
    "objectID": "ex/risk_classification_ROC.html#instructions-for-reporting",
    "href": "ex/risk_classification_ROC.html#instructions-for-reporting",
    "title": "Risk classification",
    "section": "Instructions for reporting",
    "text": "Instructions for reporting\nDownload the template for the report, upload it to the folder named ex in your project in R Studio cloud and open it.\n\n\n\n Download template\n\n\n\nAll code in for the report is in the template. You are to\n\nadd text, such as the answers to questions\n\nRender the report as a html-file and upload it on the assignment for your E4 group on canvas."
  },
  {
    "objectID": "ex/useful_functions.html",
    "href": "ex/useful_functions.html",
    "title": "Introduction to useful functions in Excel",
    "section": "",
    "text": "Work alone or in pairs. If you don’t have Excel on your computer, collaborate with someone that does.\n\n\n\nMicrosoft Excel is a common software in risk analysis. The primary use is to organise and extract data, but it is also possible to build models and perform analysis and simulations in Microsoft Excel.\nThere are several commercial add-on packages designed for specific purposes. One example is @RISK for probabilistic risk analysis in Excel. It includes functions such as fitting distributions to data and performing Monte Carlo simulation.\nWe are not teaching using @RISK at this course, but you are welcome to download a free demo and try it.\nThe purpose with this lecture is to refresh some functions in Excel that might be useful for the course.\nWe will later show how to do this in R\n\n\n\nTo learn\n\nthe basic functions to calculate the average, standard deviation, quantile and size of a sample\nhow to plot a histogram\nhow to plot a probability density function\nhow to sample from a probability distribution\nto illustrate the convergence of a sample statistics to the theoretical values, which is the fundamental behind Monte Carlo simulations\n\n\n\n\n\nThe students explore an excel file with prepared functions\n\n\n\n\n40 minutes\n\n\n\nBe prepared to report back at the end of the exercise."
  },
  {
    "objectID": "ex/useful_functions.html#exercise-overview",
    "href": "ex/useful_functions.html#exercise-overview",
    "title": "Introduction to useful functions in Excel",
    "section": "",
    "text": "Work alone or in pairs. If you don’t have Excel on your computer, collaborate with someone that does.\n\n\n\nMicrosoft Excel is a common software in risk analysis. The primary use is to organise and extract data, but it is also possible to build models and perform analysis and simulations in Microsoft Excel.\nThere are several commercial add-on packages designed for specific purposes. One example is @RISK for probabilistic risk analysis in Excel. It includes functions such as fitting distributions to data and performing Monte Carlo simulation.\nWe are not teaching using @RISK at this course, but you are welcome to download a free demo and try it.\nThe purpose with this lecture is to refresh some functions in Excel that might be useful for the course.\nWe will later show how to do this in R\n\n\n\nTo learn\n\nthe basic functions to calculate the average, standard deviation, quantile and size of a sample\nhow to plot a histogram\nhow to plot a probability density function\nhow to sample from a probability distribution\nto illustrate the convergence of a sample statistics to the theoretical values, which is the fundamental behind Monte Carlo simulations\n\n\n\n\n\nThe students explore an excel file with prepared functions\n\n\n\n\n40 minutes\n\n\n\nBe prepared to report back at the end of the exercise."
  },
  {
    "objectID": "ex/useful_functions.html#preparations",
    "href": "ex/useful_functions.html#preparations",
    "title": "Introduction to useful functions in Excel",
    "section": "Preparations",
    "text": "Preparations\n\nDownload the prepared Excel file and save on your computer.\n\n\n\n\n Download xlsx file\n\n\n\n\nMake sure you have activated the Excel Add-in Analysis ToolPak.\n\nGo to the Data tab.. It is active if you have an icon with Data Analysis in the header.\n\nIf it is not there. Go to File&gt;Options&gt;Add-ins and click Go on Manage Excel Add-ins\n\nTick the box for Analysis ToolPak and Solver Add-in (we will use it later on) and click OK."
  },
  {
    "objectID": "ex/useful_functions.html#descriptive-statistics",
    "href": "ex/useful_functions.html#descriptive-statistics",
    "title": "Introduction to useful functions in Excel",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\nGo to the sheet data sample. This is the first three columns from the breast-cancer data set. We will use this to illustrate the functions for summary statistics.\nGo to the sheet sample summary. Here we show what you get out from running Data&gt;Data Analysis&gt;Descriptive Statistics. Try this!\n\nWe also show examples of functions to derive descriptive statistics from a data sample.\n\n\n\n\n\n\nTip\n\n\n\nClick on the name of the function in the editor to open it’s help text\n\n\n\n\nCalculate the three summary statistics described in the green area of the sheet.\n\n\nThe third quartile in the sample, P75, should be 15.78\nThe 5% quantile (or 5th percentile), P05, should be 9.52\nThe coefficient of variation is the ratio between the sample standard deviation and the sample mean, and should be 25%\n\n\n\n\n\n\n\nTip\n\n\n\nSolutions are found at the end of this document - but try first!"
  },
  {
    "objectID": "ex/useful_functions.html#histogram",
    "href": "ex/useful_functions.html#histogram",
    "title": "Introduction to useful functions in Excel",
    "section": "Histogram",
    "text": "Histogram\n\nGo to the sheet plot a histogram and explore the two ways to create a histogram."
  },
  {
    "objectID": "ex/useful_functions.html#probability-functions",
    "href": "ex/useful_functions.html#probability-functions",
    "title": "Introduction to useful functions in Excel",
    "section": "Probability functions",
    "text": "Probability functions\n\nGo to the sheet probability functions.\n\nHere we listed the functions available in a standard Excel. A density (PDF) and a probability (CDF) is calculated using the function ending with .DIST, but with different arguments. A quantile is calculated with the function ending with .INV. Different distributions are considered by using the name or short name before .DIST or .INV.\n\nCalculate the probability that a normally distributed variable with mean 14 and standard deviation 3.5 is less than 10\n\n\nAnswer should be 0.127\n\n\nFind the 95% quantile in the same distribution\n\n\nAnswer should be 19.8\n\n\nCalculate the probability that an exponentially distributed variable with mean 14 is less than 10\n\n\nAnswer should be 0.51"
  },
  {
    "objectID": "ex/useful_functions.html#plot-probability-distributions",
    "href": "ex/useful_functions.html#plot-probability-distributions",
    "title": "Introduction to useful functions in Excel",
    "section": "Plot probability distributions",
    "text": "Plot probability distributions\nThe general principle to plot a function is to create pairs of x-y values that are connected by a line.\n\nGo to the sheet plot probability distribution and study the plotting of the probability density function for a normal distribution with mean 14 and standard deviation 3.5.\n\n\nColumn A: pp-values are probabilities going from 0.01 to 0.99 - this is a trick to avoid having to create new x-values every time we change the parameters of the distribution.\nColumn B: the x-values are generated by calculating the quantile for each pp-value\nColumn C: the y-values (probability density) is calculated for each x-value\n\n\nSee what happens when you change the values of the parameters mean and standard deviation (yellow cells)\n\n\n\n\n\n\n\nTip\n\n\n\nThe only thing you need to change are the parameters!\nLinking functions to each other will save a lot of time and reduce the risk for errors when you work with excel.\n\n\n\n\n\n\n\n\nExtra\n\n\n\nIf you feel you have the time or do another time:\nCopy the sheet and refine the grid by using pp-values from 0.001 to 0.999."
  },
  {
    "objectID": "ex/useful_functions.html#random-sampling",
    "href": "ex/useful_functions.html#random-sampling",
    "title": "Introduction to useful functions in Excel",
    "section": "Random sampling",
    "text": "Random sampling\n\nGo to the sheet random sampling.\n\nAll sample generators start with a random number between 0 and 1. This is also a sample from a uniform distribution.\n\n\n\n\n\n\nTip\n\n\n\nPress F9 to make a new draw\n\n\n\nType into cell D4 a function that generates a uniform random number in the interval 1 to 6. Hint: check out the help text for RAND\n\nA random draw from a probability distribution can be generated by the inverse method. - Draws pp-values from a uniform distribution between 0 and 1 - Transform them into quantiles of the target distribution\n\nThe inverse method is demonstrated in cell D6 where it generates random draws from a normal distribution\nIn cell D8 we draw from a beta distribution\n\nThe inverse method is used for generating random values from probability distributions in Monte Carlo simulations."
  },
  {
    "objectID": "ex/useful_functions.html#compare-descriptive-statistics-against-theoretical-values",
    "href": "ex/useful_functions.html#compare-descriptive-statistics-against-theoretical-values",
    "title": "Introduction to useful functions in Excel",
    "section": "Compare descriptive statistics against theoretical values",
    "text": "Compare descriptive statistics against theoretical values\nWow - now we can generate data where we know the true value on parameters and all theoretical probabilities and quantiles, and compare with what we get when deriving descriptive statistics from the random sample.\nNow we can explore the importance of large number of random numbers to gett good approximations when doing Monte Carlo simulations.\n\nGo to the final sheet compare\n\nThis sheet generates a random sample from a beta distribution.\nA beta distribution has two parameters \\(\\alpha\\) and \\(\\beta\\)\nThe expected value of a beta distributed variable is \\(\\frac{\\alpha}{\\alpha+\\beta}\\)\n\nCompare the calculated sample average to the theoretical expected value (green cells)\n\nWe can also derive the theoretical quantile, let us say the P95.\n\nCompare the quantile from the sample with the quantile calculated from the inverse probability distribution function (blue cells)\n\n\nWhich of them has the smallest difference? Why do you think it is like that?\n\n\n\n\n\n\n\nExtra\n\n\n\nIf you feel you have the time or do another time:\nExplore what happens with the difference between theoretical and statistical values when you increase sample size from 20 to a high number (close to 1000)?\nTip: Drag the formula in column B27 down to a row with a large number."
  },
  {
    "objectID": "ex/useful_functions.html#solutions",
    "href": "ex/useful_functions.html#solutions",
    "title": "Introduction to useful functions in Excel",
    "section": "Solutions",
    "text": "Solutions\n\nFunctions in Excel\nQUARTILE.INC(‘data sample’!C:C,3)\nPERCENTILE.INC(‘data sample’!C:C,0.05)\nSTDEV.S(‘data sample’!C:C)/AVERAGE(‘data sample’!C:C)*100\nNORM.DIST(10,14,3.5,1)\nNORM.INV(0.95,14,3.5)\nEXPON.DIST(10,1/14,1)\nRAND()*(6-1)+1\n\n\nFunctions in R\nYou can find a reproduction of the calculations, visualisations and simulations using R. Look at them before doing the exercises on Sept 6th.\nUseful functions reproduced using R"
  },
  {
    "objectID": "ex/wisdom_of_crowds.html#quantity-to-be-elicited",
    "href": "ex/wisdom_of_crowds.html#quantity-to-be-elicited",
    "title": "Wisdom of crowds",
    "section": "Quantity to be elicited",
    "text": "Quantity to be elicited\nThe healthy life years for females in Norway 2003\nHealthy life years is defined as the number of remaining years that a person at birth is expected to live without any severe or moderate health problems.\n[Link to definition on eurostat] (https://ec.europa.eu/eurostat/cache/metadata/en/hlth_hlye_h_esms.htm)"
  },
  {
    "objectID": "ex/wisdom_of_crowds.html#procedure-1",
    "href": "ex/wisdom_of_crowds.html#procedure-1",
    "title": "Wisdom of crowds",
    "section": "Procedure (1)",
    "text": "Procedure (1)\n\nWe discussed the quantity and enusured that it was well-defined\nThe experts (students) made their judgements indpendent of each other\nEach student fitted a distribution to their judgements and extracted a 80% probability interval\nThe judgements (five quantiles) and the probability interval was shared with the facilitator (Ullrika)"
  },
  {
    "objectID": "ex/wisdom_of_crowds.html#procedure-2",
    "href": "ex/wisdom_of_crowds.html#procedure-2",
    "title": "Wisdom of crowds",
    "section": "Procedure (2)",
    "text": "Procedure (2)\n\nThe facilitator uploaded the judgements into the SHELF app for multiple experts"
  },
  {
    "objectID": "ex/wisdom_of_crowds.html#procedure-3",
    "href": "ex/wisdom_of_crowds.html#procedure-3",
    "title": "Wisdom of crowds",
    "section": "Procedure (3)",
    "text": "Procedure (3)\n\nJudgements shown by the quartiles"
  },
  {
    "objectID": "ex/wisdom_of_crowds.html#procedure-3-1",
    "href": "ex/wisdom_of_crowds.html#procedure-3-1",
    "title": "Wisdom of crowds",
    "section": "Procedure (3)",
    "text": "Procedure (3)\n\nJudgements shown as “histograms”"
  },
  {
    "objectID": "ex/wisdom_of_crowds.html#procedure-3-2",
    "href": "ex/wisdom_of_crowds.html#procedure-3-2",
    "title": "Wisdom of crowds",
    "section": "Procedure (3)",
    "text": "Procedure (3)\n\nJudgements shown by the best fitting probability distribution"
  },
  {
    "objectID": "ex/wisdom_of_crowds.html#the-aggregated-judgement---linear-pool",
    "href": "ex/wisdom_of_crowds.html#the-aggregated-judgement---linear-pool",
    "title": "Wisdom of crowds",
    "section": "The aggregated judgement - linear pool",
    "text": "The aggregated judgement - linear pool"
  },
  {
    "objectID": "ex/wisdom_of_crowds.html#individual-judgements",
    "href": "ex/wisdom_of_crowds.html#individual-judgements",
    "title": "Wisdom of crowds",
    "section": "Individual judgements",
    "text": "Individual judgements\n\nJudgements shown as 80% probability intervals"
  },
  {
    "objectID": "ex/wisdom_of_crowds.html#with-the-aggregated-judgement",
    "href": "ex/wisdom_of_crowds.html#with-the-aggregated-judgement",
    "title": "Wisdom of crowds",
    "section": "With the aggregated judgement",
    "text": "With the aggregated judgement\n\nAggregation made taking the linear pool"
  },
  {
    "objectID": "ex/wisdom_of_crowds.html#with-the-true-value",
    "href": "ex/wisdom_of_crowds.html#with-the-true-value",
    "title": "Wisdom of crowds",
    "section": "With the true value",
    "text": "With the true value\n\nThe true value taken from eurostat is 64.2 years (red line)"
  },
  {
    "objectID": "ex/wisdom_of_crowds.html#comment",
    "href": "ex/wisdom_of_crowds.html#comment",
    "title": "Wisdom of crowds",
    "section": "Comment",
    "text": "Comment\n\nThe average number of healthy years for females in Norway is likely to be below but close to the life expectancy, and unlikely to be close to 0 or above 100\nSome probability distributions allows for negative values, which is problematic when calculating the probability intervals\nTake home message:\n\nmake sure the experts understand the quantity, e.g. by asking them to justify their judgements\nthe final probability distribution must be chosen with care"
  }
]